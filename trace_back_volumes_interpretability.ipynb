{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d4cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_file = \"./interpretability/interpretability_PDA_mixed5/fold_2_log.csv\"\n",
    "raw_vols_folder = \"./data/raw/CPTAC_PDA_93_surv/cptacpda_93/CPTAC-PDA\"\n",
    "raw_vols_metadata = \"./data/raw/CPTAC_PDA_93_surv/cptacpda_93/metadata.csv\"\n",
    "raw_segs_folder = \"./data/raw/CPTAC_PDA_93_surv/Segmentations/CPTAC-PDA\"\n",
    "raw_segd_metadata = \"./data/raw/CPTAC_PDA_93_surv/Segmentations/metadata.csv\"\n",
    "segs_csv = \"./data/metadata_annotations/Metadata_Report_CPTAC-PDA_2023_07_14.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4323686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --fold_file FOLD_FILE --processed_folder\n",
      "                             PROCESSED_FOLDER --raw_vols_folder\n",
      "                             RAW_VOLS_FOLDER --raw_vols_metadata\n",
      "                             RAW_VOLS_METADATA --raw_segs_folder\n",
      "                             RAW_SEGS_FOLDER --segs_csv SEGS_CSV\n",
      "                             [--workers WORKERS] [--epsilon EPSILON]\n",
      "                             [--out_csv OUT_CSV]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --processed_folder, --raw_vols_folder, --raw_vols_metadata, --raw_segs_folder, --segs_csv\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/pietro.caforio/anaconda3/envs/research_biocv_proj/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Find which raw DICOM volumes produced a given set of pre-processed 224×224×66\n",
    "arrays, by re-running the exact same preprocessing pipeline and comparing the\n",
    "results voxel-wise.\n",
    "\n",
    "For every (patient, index) entry in the fold CSV it logs **all** raw CT series\n",
    "whose pre-processed array matches the reference within ε, and writes the\n",
    "mapping to an output CSV.\n",
    "\n",
    "Assumptions confirmed by the user\n",
    "---------------------------------\n",
    "• Raw volumes live in:  <raw_vols_folder>/<Patient>/<SeriesUID>/…/*.dcm\n",
    "• Pre-processed arrays live in: <processed_folder>/<Patient>/<idx>.npy\n",
    "• Target isotropic volume size = (224, 224, 224)\n",
    "• Final depth  = 66 slices   (central padding/trim if needed)\n",
    "• oversampling = False\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Your existing utility functions (import exactly as in the original script)\n",
    "# --------------------------------------------------------------------------- #\n",
    "sys.path.insert(0, \"../\")  # Change if these utils live elsewhere\n",
    "from util.data_util import (  # noqa: E402\n",
    "    get_occupied_slices,\n",
    "    load_single_volume,\n",
    "    preprocess,\n",
    "    remap_occupied_slices,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Constants\n",
    "# --------------------------------------------------------------------------- #\n",
    "TARGET_SHAPE = [224, 224, 224]\n",
    "FIX_DEPTH = 66\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Logging\n",
    "# --------------------------------------------------------------------------- #\n",
    "def setup_logging(log_dir: str = \"./logs\") -> str:\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    logfile = Path(log_dir) / f\"match_log_{ts}.log\"\n",
    "    logging.basicConfig(\n",
    "        filename=logfile,\n",
    "        level=logging.DEBUG,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "        filemode=\"w\",\n",
    "    )\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    console.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\"))\n",
    "    logging.getLogger().addHandler(console)\n",
    "    logging.info(\"Logging initialised.\")\n",
    "    return str(logfile)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Helper utilities\n",
    "# --------------------------------------------------------------------------- #\n",
    "def central_trim(indices: List[int], target_len: int) -> List[int]:\n",
    "    \"\"\"Trim a sorted list of slice indices to `target_len`, keeping the centre.\"\"\"\n",
    "    if len(indices) <= target_len:\n",
    "        return indices\n",
    "    mid = (indices[0] + indices[-1]) // 2\n",
    "    half = target_len // 2\n",
    "    start = mid - half\n",
    "    end = start + target_len\n",
    "    return list(range(start, end))\n",
    "\n",
    "\n",
    "def enforce_depth(indices: List[int]) -> List[int]:\n",
    "    \"\"\"Pad or trim `indices` so that len(indices) == FIX_DEPTH.\"\"\"\n",
    "    if len(indices) == FIX_DEPTH:\n",
    "        return indices\n",
    "\n",
    "    padded = indices.copy()\n",
    "    if len(indices) < FIX_DEPTH:\n",
    "        left = indices[0] - 1\n",
    "        right = indices[-1] + 1\n",
    "        while len(padded) < FIX_DEPTH:\n",
    "            if left >= 0:\n",
    "                padded.insert(0, left)\n",
    "                left -= 1\n",
    "            if len(padded) < FIX_DEPTH and right < TARGET_SHAPE[2]:\n",
    "                padded.append(right)\n",
    "                right += 1\n",
    "        return padded\n",
    "    else:  # len(indices) > FIX_DEPTH\n",
    "        return central_trim(indices, FIX_DEPTH)\n",
    "\n",
    "\n",
    "def normalise_path(p: str) -> str:\n",
    "    \"\"\"Convert Windows back-slashes to ‘/’ for portability.\"\"\"\n",
    "    return p.replace(\"\\\\\", \"/\") if os.name != \"nt\" else p\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Worker\n",
    "# --------------------------------------------------------------------------- #\n",
    "def worker(task) -> List[Tuple]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    task : dict with keys\n",
    "        patient_id, idx, ref_np_path, seg_df (filtered for this patient),\n",
    "        raw_meta_df, args\n",
    "    Returns\n",
    "    -------\n",
    "    list of (patient, idx, raw_series_uid, raw_folder, delta)\n",
    "        One tuple per successful match.\n",
    "    \"\"\"\n",
    "    patient = task[\"patient_id\"]\n",
    "    idx = task[\"idx\"]\n",
    "    ref_np = task[\"ref_np_path\"]\n",
    "    seg_df = task[\"seg_df\"]\n",
    "    raw_meta = task[\"raw_meta_df\"]\n",
    "    args = task[\"args\"]\n",
    "\n",
    "    out_tuples = []\n",
    "\n",
    "    try:\n",
    "        ref_arr = np.load(ref_np)\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"[{patient} | {idx}] reference array not found: {ref_np}\")\n",
    "        return out_tuples\n",
    "\n",
    "    # =======================================================================\n",
    "    #  For each segmentation object → find its referenced CT series\n",
    "    # =======================================================================\n",
    "    for _, seg_row in seg_df.iterrows():\n",
    "        seg_folder_rel = normalise_path(seg_row[\"File Location\"].split(\".\\\\\")[-1])\n",
    "        seg_path = Path(args.raw_segs_folder) / seg_folder_rel\n",
    "        seg_path = seg_path.resolve()\n",
    "\n",
    "        ct_series_uid = seg_row[\"ReferencedSeriesInstanceUID\"].strip()\n",
    "\n",
    "        # -------------------------------------------------------------------\n",
    "        #  Locate the raw CT folder from metadata\n",
    "        # -------------------------------------------------------------------\n",
    "        ct_meta_row = raw_meta[raw_meta[\"Series UID\"] == ct_series_uid]\n",
    "        if ct_meta_row.empty:\n",
    "            logging.debug(\n",
    "                f\"[{patient} | {idx}] Series UID {ct_series_uid} not in raw metadata\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        ct_folder_rel = normalise_path(ct_meta_row.iloc[0][\"File Location\"])\n",
    "        ct_path = Path(args.raw_vols_folder) / Path(*Path(ct_folder_rel).parts[2:])\n",
    "        ct_path = ct_path.resolve()\n",
    "\n",
    "        if not ct_path.exists():\n",
    "            # Fallback for possible “-NA” suffix stripping\n",
    "            ct_path = Path(str(ct_path).replace(\"-NA\", \"\"))\n",
    "            if not ct_path.exists():\n",
    "                logging.debug(\n",
    "                    f\"[{patient} | {idx}] CT folder missing: {ct_path} (after -NA fix)\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        # -------------------------------------------------------------------\n",
    "        #  Re-run preprocessing (exact replica of original pipeline)\n",
    "        # -------------------------------------------------------------------\n",
    "        vol, dim, dcm_slices, direction = load_single_volume(str(ct_path))\n",
    "        if vol is None:\n",
    "            logging.debug(f\"[{patient} | {idx}] Empty volume at {ct_path}\")\n",
    "            continue\n",
    "\n",
    "        if direction == \"sagittal\":\n",
    "            vol = vol.transpose(1, 0, 2)\n",
    "        elif direction == \"coronal\":\n",
    "            vol = vol.transpose(2, 0, 1)\n",
    "\n",
    "        occupied = get_occupied_slices(str(seg_path / os.listdir(seg_path)[0]),\n",
    "                                       dcm_slices, direction)\n",
    "        if not occupied:\n",
    "            logging.debug(f\"[{patient} | {idx}] No occupied slices for {ct_path}\")\n",
    "            continue\n",
    "\n",
    "        vol, zoom = preprocess(vol, TARGET_SHAPE)\n",
    "        occupied = remap_occupied_slices(occupied, zoom[0])\n",
    "        occupied = enforce_depth(sorted(occupied))\n",
    "\n",
    "        new_arr = vol[:, :, occupied]  # → (224,224,66)\n",
    "        if new_arr.shape != ref_arr.shape:\n",
    "            continue\n",
    "\n",
    "        delta = float(np.abs(new_arr.astype(np.float32) -\n",
    "                             ref_arr.astype(np.float32)).max())\n",
    "\n",
    "        if delta <= args.epsilon:\n",
    "            logging.info(\n",
    "                f\"[{patient} | {idx}] MATCH  \"\n",
    "                f\"{ct_series_uid}  →  Δ={delta:.2e},  path={ct_path}\"\n",
    "            )\n",
    "            out_tuples.append((patient, idx, ct_series_uid, str(ct_path), delta))\n",
    "        else:\n",
    "            logging.debug(\n",
    "                f\"[{patient} | {idx}] no match (Δ={delta:.3e})  series {ct_series_uid}\"\n",
    "            )\n",
    "\n",
    "    return out_tuples\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Main\n",
    "# --------------------------------------------------------------------------- #\n",
    "def main(argv=None):\n",
    "    parser = argparse.ArgumentParser(description=\"Match raw CT volumes to \"\n",
    "                                                 \"pre-processed 224×224×66 arrays\")\n",
    "    parser.add_argument(\"--fold_file\", required=True)\n",
    "    parser.add_argument(\"--processed_folder\", required=True)\n",
    "    parser.add_argument(\"--raw_vols_folder\", required=True)\n",
    "    parser.add_argument(\"--raw_vols_metadata\", required=True)\n",
    "    parser.add_argument(\"--raw_segs_folder\", required=True)\n",
    "    parser.add_argument(\"--segs_csv\", required=True)\n",
    "\n",
    "    parser.add_argument(\"--workers\", type=int, default=4)\n",
    "    parser.add_argument(\"--epsilon\", type=float, default=1e-6)\n",
    "    parser.add_argument(\"--out_csv\", default=\"matched_raw_volumes.csv\")\n",
    "\n",
    "    args = parser.parse_args(argv)\n",
    "    log_file = setup_logging()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #  Load driver tables\n",
    "    # ------------------------------------------------------------------ #\n",
    "    fold_df = pd.read_csv(args.fold_file)\n",
    "    raw_meta_df = pd.read_csv(args.raw_vols_metadata)\n",
    "    segs_df = pd.read_csv(args.segs_csv)\n",
    "\n",
    "    # Ensure we have the columns we need\n",
    "    segs_df = segs_df[segs_df[\"Annotation Type\"] == \"Segmentation\"]\n",
    "\n",
    "    logging.info(f\"Fold rows: {len(fold_df)}\")\n",
    "    logging.info(f\"Segmentation objects: {len(segs_df)}\")\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #  Build job list\n",
    "    # ------------------------------------------------------------------ #\n",
    "    jobs = []\n",
    "    for _, row in fold_df.iterrows():\n",
    "        patient = row[\"patient_id\"]\n",
    "        # index = number before '_embeddings.npy'\n",
    "        m = re.search(r\"(\\d+)_embeddings\\.npy$\", row[\"ct_path\"])\n",
    "        if not m:\n",
    "            logging.warning(f\"Could not parse index from ct_path: {row['ct_path']}\")\n",
    "            continue\n",
    "        idx = int(m.group(1))\n",
    "\n",
    "        ref_np = Path(args.processed_folder) / patient / f\"{idx}.npy\"\n",
    "        patient_segs = segs_df[segs_df[\"PatientID\"] == patient]\n",
    "\n",
    "        if patient_segs.empty:\n",
    "            logging.debug(f\"No segmentation rows for patient {patient}\")\n",
    "            continue\n",
    "\n",
    "        jobs.append(\n",
    "            dict(\n",
    "                patient_id=patient,\n",
    "                idx=idx,\n",
    "                ref_np_path=str(ref_np),\n",
    "                seg_df=patient_segs.copy(),\n",
    "                raw_meta_df=raw_meta_df,\n",
    "                args=args,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #  Parallel processing\n",
    "    # ------------------------------------------------------------------ #\n",
    "    logging.info(f\"Launching pool with {args.workers} workers …\")\n",
    "    results = []\n",
    "    with Pool(args.workers) as pool:\n",
    "        for res in pool.imap_unordered(worker, jobs):\n",
    "            results.extend(res)\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #  Persist matches\n",
    "    # ------------------------------------------------------------------ #\n",
    "    if results:\n",
    "        with open(args.out_csv, \"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow(\n",
    "                [\"patient_id\", \"idx\", \"raw_series_uid\", \"raw_folder\", \"delta\"]\n",
    "            )\n",
    "            w.writerows(results)\n",
    "        logging.info(f\"Wrote {len(results)} matches → {args.out_csv}\")\n",
    "    else:\n",
    "        logging.warning(\"No matches were found!\")\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    logging.info(f\"Done in {time.strftime('%H:%M:%S', time.gmtime(elapsed))}\")\n",
    "    logging.info(f\"Full log: {log_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_biocv_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
