{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9300ed25",
   "metadata": {},
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799cc1d2",
   "metadata": {},
   "source": [
    "Sul fold migliore vado a fare forward pass e calcolarmi i gradienti, i positional embedding, correlazioni, curvatura e path degli input che danno quei risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc296b",
   "metadata": {},
   "source": [
    "Pensiero random (potrebbe essere carino per la tesi e quasi d’obbligo per il paper), lo lascerei comunque come ultimo step se abbiamo tempo: riusciamo sulla fold che va meglio, del dataset che va meglio per il nostro modello tenere traccia in validation di quali sono le coppie di feature (in modo da rintracciare la coppia rad-histo) che portano gradienti maggiori e minori e salvare le rispettive correlazioni (e calcolare il rischio)? Questo significa rintracciare le coppie la cui correlazione ha un’importanza o meno nel calcolo del rischio.\n",
    "Quello che sarebbe interessante far vedere sono gli estremi opposti su due coppie di rad histo con:\n",
    "Alta correlazione, alto gradiente, alto risk score, basso survival, alto grado di tumore: sulla radiologia calcolare tipo regolarità della forma (che ci i aspetta bassa, usando la segmentazione), grandezza tumore (che ci si aspetta alta, usando la segmentazione) e sulla histo il numero di cellule epiteliali cancerogene (che ci si aspetta alta, cosa che si può fare con una rete pretrainata) \n",
    "Alta correlazione, basso gradiente, basso risk score, alto survival, basso grado di tumore: sulla radiologia calcolare tipo regolarità della forma (che ci i aspetta alta), grandezza tumore (che ci si aspetta bassa, usando la segmentazione) e sulla histo il numero di cellule epiteliali cancerogene (che ci si aspetta bassa, cosa che si può fare con una rete pretrainata) \n",
    "Se tu ti occupi di tracciare i gradienti/correlazioni/file (non dovrebbe essere troppo complesso, basta salvarsi le combinazioni di risultati su un file), io mi occuperei di tutto il resto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e07be3",
   "metadata": {},
   "source": [
    "### Define Functions for interpretability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab473306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from itertools import product\n",
    "#from pathlib import Path\n",
    "\n",
    "#import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "#import torchstain\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "#from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class   MultimodalCTWSIDatasetSurv(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for paired CT and WSI data, handling missing modalities\n",
    "        at both dataset and sampling level\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fold: int,\n",
    "        split: str,  # either \"train\" or \"test\"\n",
    "        ct_path: str,\n",
    "        wsi_path: str,\n",
    "        labels_splits_path: str,\n",
    "        missing_modality_prob: float = 0.0,  # Additional random masking probability\n",
    "        missing_modality: str = \"both\",\n",
    "        require_both_modalities: bool = False,  # Whether to only include patients\n",
    "        # with both modalities\n",
    "        pairing_mode: str = None,  # 'all_combinations, 'one_to_one', 'fixed_count'\n",
    "        pairs_per_patient: int = None,  # For fixed_count_mode\n",
    "        allow_repeats: bool = False,  # For fixed_count mode\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # assert split in [\"train\", \"val\", \"overfit\", \"all\"]\n",
    "        assert split in [\"train\", \"test\"]\n",
    "        assert pairing_mode in [\"all_combinations\", \"one_to_one\", \"fixed_count\"]\n",
    "        assert 0 <= missing_modality_prob <= 1\n",
    "        assert missing_modality in [\"ct\", \"wsi\", \"both\"]\n",
    "        self.missing_modality = missing_modality\n",
    "        self.fold = fold\n",
    "        self.split = split\n",
    "        self.ct_path = ct_path\n",
    "        self.wsi_path = wsi_path\n",
    "        self.missing_modality_prob = missing_modality_prob\n",
    "        self.require_both_modalities = require_both_modalities\n",
    "        self.labels_splits_path = labels_splits_path\n",
    "        self.pairing_mode = (\n",
    "            pairing_mode  # 'all_combinations', 'one_to_one', or 'fixed_count'\n",
    "        )\n",
    "        self.pairs_per_patient = pairs_per_patient  # For fixed_count mode\n",
    "        self.allow_repeats = allow_repeats  # For fixed_count mode\n",
    "        labels_splits = pd.read_csv(labels_splits_path, sep=\"\\t\")\n",
    "        labels_splits = labels_splits[labels_splits[f\"fold_{self.fold}\"] == self.split]\n",
    "        self.labels_splits = labels_splits[\n",
    "            [\"case_id\", \"OS_days\", \"OS_event\"]\n",
    "        ].drop_duplicates(\"case_id\")\n",
    "\n",
    "        # Initialize data structures\n",
    "        # Will store CT and WSI paths per patient\n",
    "        self.patient_data = {}\n",
    "        # Will store all valid combinations\n",
    "        self.samples = []\n",
    "        self.modality_stats = {\"ct_only\": 0, \"wsi_only\": 0, \"both\": 0}\n",
    "\n",
    "        # Load split file\n",
    "        self._load_split()\n",
    "\n",
    "    def _get_max_pairs_for_patient(self, ct_scans, wsi_folders, allow_repeats):\n",
    "        \"\"\"\n",
    "        Calculate maximum possible pairs for a patient based on available data and pairing mode.\n",
    "\n",
    "        Args:\n",
    "            ct_scans (list): List of CT scan files\n",
    "            wsi_folders (list): List of WSI folder names\n",
    "            allow_repeats (bool): If True, return all possible combinations count\n",
    "                                If False, return maximum unique pairs count\n",
    "\n",
    "        Returns:\n",
    "            int: Maximum number of possible pairs\n",
    "        \"\"\"\n",
    "        if not ct_scans or not wsi_folders:\n",
    "            return 0\n",
    "\n",
    "        if allow_repeats:\n",
    "            return len(ct_scans) * len(wsi_folders)\n",
    "        else:  # one-to-one\n",
    "            return min(len(ct_scans), len(wsi_folders))\n",
    "\n",
    "    def _get_fixed_pairs(self, ct_scans, wsi_folders, n_pairs, allow_repeats=True):\n",
    "        \"\"\"\n",
    "        Generate fixed number of pairs between CT scans and WSI folders.\n",
    "\n",
    "        Args:\n",
    "            ct_scans (list): List of CT scan files\n",
    "            wsi_folders (list): List of WSI folder names\n",
    "            n_pairs (int): Number of pairs to generate\n",
    "            allow_repeats (bool): If True, allows repeating elements to reach n_pairs\n",
    "                                If False, limits pairs to minimum unique combinations\n",
    "\n",
    "        Returns:\n",
    "            list: List of (ct_scan, wsi_folder) pairs\n",
    "        \"\"\"\n",
    "        max_unique_pairs = min(len(ct_scans), len(wsi_folders))\n",
    "\n",
    "        if not allow_repeats:\n",
    "            n_pairs = min(n_pairs, max_unique_pairs)\n",
    "\n",
    "        # Generate initial unique pairs\n",
    "        shuffled_ct = ct_scans.copy()\n",
    "        shuffled_wsi = wsi_folders.copy()\n",
    "        random.shuffle(shuffled_ct)\n",
    "        random.shuffle(shuffled_wsi)\n",
    "\n",
    "        pairs = list(\n",
    "            zip(shuffled_ct[:max_unique_pairs], shuffled_wsi[:max_unique_pairs])\n",
    "        )\n",
    "\n",
    "        if n_pairs <= len(pairs):\n",
    "            # Downsample if needed\n",
    "            random.shuffle(pairs)\n",
    "            return pairs[:n_pairs]\n",
    "\n",
    "        if not allow_repeats:\n",
    "            return pairs\n",
    "\n",
    "        # Need to generate additional pairs with repeats\n",
    "        while len(pairs) < n_pairs:\n",
    "            ct_scan = random.choice(ct_scans)\n",
    "            wsi_folder = random.choice(wsi_folders)\n",
    "            pairs.append((ct_scan, wsi_folder))\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def _load_split(self):\n",
    "        \"\"\"Load and organize all CT and WSI data for the given split\n",
    "\n",
    "        Supports different pairing modes:\n",
    "        - 'all_combinations': Creates all possible CT-WSI pairs\n",
    "        - 'one_to_one': Creates random 1:1 pairs\n",
    "        - 'fixed_count': Creates fixed number of pairs per patient\n",
    "        \"\"\"\n",
    "\n",
    "        # First pass: count maximum possible pairs per patient\n",
    "        max_pairs_possible = float(\"inf\")\n",
    "        if self.pairing_mode == \"fixed_count\":\n",
    "            for patient_id in self.labels_splits[\"case_id\"].values:\n",
    "\n",
    "                ct_path = os.path.join(self.ct_path, patient_id)\n",
    "                ct_features = []\n",
    "                if os.path.exists(ct_path):\n",
    "                    ct_features = [f for f in os.listdir(ct_path)]\n",
    "\n",
    "                wsi_path = os.path.join(self.wsi_path)\n",
    "                wsi_features = [\n",
    "                    f\n",
    "                    for f in os.listdir(wsi_path)\n",
    "                    if patient_id in f and os.path.isdir(os.path.join(wsi_path, f))\n",
    "                ]\n",
    "\n",
    "                patient_max_pairs = self._get_max_pairs_for_patient(\n",
    "                    ct_features, wsi_features, self.allow_repeats\n",
    "                )\n",
    "\n",
    "                if patient_max_pairs > 0:  # Only update if patient has both modalities\n",
    "\n",
    "                    max_pairs_possible = min(max_pairs_possible, patient_max_pairs)\n",
    "\n",
    "        # Use provided pairs_per_patient or calculated maximum\n",
    "        n_pairs = (\n",
    "            self.pairs_per_patient\n",
    "            if self.pairs_per_patient is not None\n",
    "            else max_pairs_possible\n",
    "        )\n",
    "\n",
    "        # Main loading loop\n",
    "        for row in self.labels_splits[\"case_id\"].values:\n",
    "            patient_id = row.strip()\n",
    "            # Find all CT scans for this patient\n",
    "            ct_path = os.path.join(self.ct_path, patient_id)\n",
    "            ct_features = []\n",
    "            if os.path.exists(ct_path):\n",
    "                ct_features = [f for f in os.listdir(ct_path)]\n",
    "\n",
    "            # Find all WSI .h5 files for this patient\n",
    "            wsi_path = self.wsi_path\n",
    "            wsi_features = [f for f in os.listdir(wsi_path) if patient_id in f]\n",
    "\n",
    "            # Skip patient if we require both modalities and they don't have them\n",
    "            if self.require_both_modalities and (not ct_features or not wsi_features):\n",
    "                continue\n",
    "\n",
    "            # Store available data for this patient\n",
    "            self.patient_data[patient_id] = {\n",
    "                \"ct_features\": ct_features,\n",
    "                \"wsi_features\": wsi_features,\n",
    "            }\n",
    "\n",
    "            # Update modality statistics\n",
    "            if ct_features and wsi_features:\n",
    "                self.modality_stats[\"both\"] += 1\n",
    "            elif ct_features:\n",
    "                self.modality_stats[\"ct_only\"] += 1\n",
    "            else:\n",
    "                self.modality_stats[\"wsi_only\"] += 1\n",
    "\n",
    "            cnt = 0\n",
    "            # Generate samples based on available data and pairing mode\n",
    "            if ct_features and wsi_features:\n",
    "                if self.pairing_mode == \"fixed_count\":\n",
    "                    # Generate fixed number of pairs\n",
    "                    pairs = self._get_fixed_pairs(\n",
    "                        ct_features, wsi_features, n_pairs, self.allow_repeats\n",
    "                    )\n",
    "\n",
    "                    for ct_feature, wsi_feature in pairs:\n",
    "                        cnt += 1\n",
    "                        self.samples.append(\n",
    "                            {\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"ct_path\": os.path.join(\n",
    "                                    self.ct_path, patient_id, ct_feature\n",
    "                                ),\n",
    "                                \"wsi_folder\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                                \"base_modality_mask\": [1, 1],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                elif self.pairing_mode == \"one_to_one\":\n",
    "                    # Original one_to_one logic\n",
    "                    num_pairs = min(len(ct_features), len(wsi_features))\n",
    "                    shuffled_ct = ct_features.copy()\n",
    "                    shuffled_wsi = wsi_features.copy()\n",
    "                    random.shuffle(shuffled_ct)\n",
    "                    random.shuffle(shuffled_wsi)\n",
    "\n",
    "                    for ct_feature, wsi_feature in zip(\n",
    "                        shuffled_ct[:num_pairs], shuffled_wsi[:num_pairs]\n",
    "                    ):\n",
    "                        cnt += 1\n",
    "                        self.samples.append(\n",
    "                            {\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"ct_path\": os.path.join(\n",
    "                                    self.ct_path, patient_id, ct_feature\n",
    "                                ),\n",
    "                                \"wsi_feature\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                                \"base_modality_mask\": [1, 1],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                else:  # 'all_combinations' mode\n",
    "                    for ct_feature, wsi_feature in product(ct_features, wsi_features):\n",
    "                        cnt += 1\n",
    "                        self.samples.append(\n",
    "                            {\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"ct_path\": os.path.join(\n",
    "                                    self.ct_path, patient_id, ct_feature\n",
    "                                ),\n",
    "                                \"wsi_feature\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                                \"base_modality_mask\": [1, 1],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            elif ct_features:\n",
    "                for ct_feature in ct_features:\n",
    "                    cnt += 1\n",
    "                    self.samples.append(\n",
    "                        {\n",
    "                            \"patient_id\": patient_id,\n",
    "                            \"ct_path\": os.path.join(\n",
    "                                self.ct_path, patient_id, ct_feature\n",
    "                            ),\n",
    "                            \"wsi_feature\": None,\n",
    "                            \"base_modality_mask\": [1, 0],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            elif wsi_features:\n",
    "                for wsi_feature in wsi_features:\n",
    "                    cnt += 1\n",
    "                    self.samples.append(\n",
    "                        {\n",
    "                            \"patient_id\": patient_id,\n",
    "                            \"ct_path\": None,\n",
    "                            \"wsi_feature\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                            \"base_modality_mask\": [0, 1],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    def _load_ct_feature(self, ct_path):\n",
    "        \"\"\"Load and standardize CT feature\"\"\"\n",
    "        if ct_path.endswith(\".pt\"):\n",
    "            volume = np.array(torch.load(ct_path, weights_only=True))\n",
    "        elif ct_path.endswith(\".npy\"):\n",
    "            volume = np.load(ct_path)\n",
    "        return volume\n",
    "\n",
    "    def _load_wsi_feature(self, wsi_path):\n",
    "        \"\"\"Load WSI feature\"\"\"\n",
    "        feature = None\n",
    "        with h5py.File(wsi_path, \"r\") as f:\n",
    "            feature = np.array(f[\"features\"][:])\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def _get_empty_ct_feature(self):\n",
    "        \"\"\"Return empty CT feature of correct shape\"\"\"\n",
    "        return np.zeros((66, 1024))\n",
    "\n",
    "    def _get_empty_wsi_feature(self):\n",
    "        \"\"\"Return empty WSI feature of correct shape\"\"\"\n",
    "        return np.zeros((768,))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'patient_id': str,\n",
    "                'ct_feature': numpy array or zeros if missing,\n",
    "                'wsi_feature': tensor or zeros if missing,\n",
    "                'label': int,\n",
    "                'modality_mask': tensor indicating present modalities [CT, WSI],\n",
    "                'base_modality_mask': tensor indicating modalities available in dataset\n",
    "            }\n",
    "        \"\"\"\n",
    "\n",
    "        sample = self.samples[index]\n",
    "        patient_id = sample[\"patient_id\"]\n",
    "        base_mask = sample[\"base_modality_mask\"]\n",
    "        ct_path = sample[\"ct_path\"]\n",
    "        wsi_path = sample[\"wsi_feature\"]\n",
    "        if wsi_path is None: wsi_path = \"\"\n",
    "        if ct_path is None: ct_path = \"\"\n",
    "        # Apply additional random masking only to available modalities\n",
    "        final_mask = base_mask.copy()\n",
    "        if self.missing_modality_prob > 0:\n",
    "            if self.missing_modality == \"both\":\n",
    "                for i in range(2):\n",
    "                    if (\n",
    "                        base_mask[i] == 1\n",
    "                        and random.random() < self.missing_modality_prob\n",
    "                    ):\n",
    "                        final_mask[i] = 0\n",
    "            elif self.missing_modality == \"ct\":\n",
    "                if base_mask[0] == 1 and random.random() < self.missing_modality_prob:\n",
    "                    final_mask[0] = 0\n",
    "            elif self.missing_modality == \"wsi\":\n",
    "                if base_mask[1] == 1 and random.random() < self.missing_modality_prob:\n",
    "                    final_mask[1] = 0\n",
    "\n",
    "            # Ensure at least one modality remains if it was originally available\n",
    "            if sum(final_mask) == 0 and sum(base_mask) > 0:\n",
    "                # Randomly choose one of the originally available modalities\n",
    "                available_indices = [i for i in range(2) if base_mask[i] == 1]\n",
    "                chosen_idx = random.choice(available_indices)\n",
    "                final_mask[chosen_idx] = 1\n",
    "\n",
    "        # Load features based on final mask\n",
    "        ct_feature = (\n",
    "            self._load_ct_feature(sample[\"ct_path\"])\n",
    "            if final_mask[0] and sample[\"ct_path\"]\n",
    "            else self._get_empty_ct_feature()\n",
    "        )\n",
    "\n",
    "        wsi_feature = (\n",
    "            self._load_wsi_feature(sample[\"wsi_feature\"])\n",
    "            if final_mask[1] and sample[\"wsi_feature\"]\n",
    "            else self._get_empty_wsi_feature()\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"patient_id\": patient_id,\n",
    "            \"ct_feature\": torch.from_numpy(ct_feature).float(),\n",
    "            \"wsi_feature\": torch.from_numpy(wsi_feature).float(),\n",
    "            \"survtime\": torch.tensor(\n",
    "                self.labels_splits[self.labels_splits[\"case_id\"] == patient_id][\n",
    "                    \"OS_days\"\n",
    "                ].iloc[0],\n",
    "                dtype=torch.long,\n",
    "            ),\n",
    "            \"censor\": ~torch.tensor(\n",
    "                self.labels_splits[self.labels_splits[\"case_id\"] == patient_id][\n",
    "                    \"OS_event\"\n",
    "                ].iloc[0],\n",
    "                dtype=torch.bool,\n",
    "            ),\n",
    "            \"modality_mask\": torch.tensor(final_mask, dtype=torch.float32),\n",
    "            \"base_modality_mask\": torch.tensor(base_mask, dtype=torch.float32),\n",
    "            \"ct_path\": ct_path,\n",
    "            \"wsi_path\": wsi_path\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def stats(self):\n",
    "        \"\"\"Return dataset statistics\"\"\"\n",
    "        return {\n",
    "            \"total_samples\": len(self.samples),\n",
    "            \"total_patients\": len(self.patient_data),\n",
    "            \"modality_availability\": self.modality_stats,\n",
    "            \"missing_modality_prob\": self.missing_modality_prob,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def move_batch_to_device(batch, device):\n",
    "        \"\"\"Move all elements of the batch to device\"\"\"\n",
    "        batch[\"ct_feature\"] = batch[\"ct_feature\"].to(device)\n",
    "        batch[\"wsi_feature\"] = batch[\"wsi_feature\"].to(device)\n",
    "        batch[\"survtime\"] = batch[\"survtime\"].to(device)\n",
    "        batch[\"censor\"] = batch[\"censor\"].to(device)\n",
    "        batch[\"modality_mask\"] = batch[\"modality_mask\"].to(device)\n",
    "        batch[\"ct_path\"] = batch[\"ct_path\"].to(device)\n",
    "        batch[\"wsi_path\"] = batch[\"wsi_path\"].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eea5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from data.multimodal_features_surv import MultimodalCTWSIDatasetSurv\n",
    "from models.dpe.main_model_nobackbone_surv_new_gcs import MADPENetNoBackbonesSurv\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "\n",
    "class CoxLoss(_WeightedLoss):\n",
    "    def forward(self, hazard_pred: torch.Tensor, survtime: torch.Tensor, censor: torch.Tensor):\n",
    "        censor = censor.float()\n",
    "        n = len(survtime)\n",
    "        # risk‑set matrix\n",
    "        R_mat = survtime.reshape((1, n)) >= survtime.reshape((n, 1))\n",
    "        theta = hazard_pred.reshape(-1)\n",
    "        exp_theta = torch.exp(theta)\n",
    "        # negative log‑partial likelihood\n",
    "        loss = -torch.mean((theta - torch.log(torch.sum(exp_theta * R_mat, dim=1))) * censor)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def compute_grad_and_curvature(fused_features, hazard, survtime, censor):\n",
    "    hazard = hazard.view(-1)\n",
    "    loss = CoxLoss()(hazard, survtime, censor)\n",
    "    # first derivative\n",
    "    grad_f = torch.autograd.grad(loss, fused_features, create_graph=True)[0]\n",
    "    grad_norm = grad_f.flatten(1).norm(p=2, dim=1)\n",
    "\n",
    "    # Hutchinson estimator for Hessian diagonal\n",
    "    random_vec = grad_f.detach().clone().sign()\n",
    "    grad_dot_random = torch.sum(grad_f * random_vec)\n",
    "    hvp = torch.autograd.grad(grad_dot_random, fused_features, retain_graph=False)[0]\n",
    "    curvature = torch.sum(hvp * random_vec, dim=list(range(1, hvp.ndim)))\n",
    "\n",
    "    return grad_norm.detach().cpu().numpy(), curvature.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def evaluate_and_log(folds_dir, ct_path, wsi_path, test_path, output_dir, batch_size=16, n_folds=5):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        print(f\"\\n=== Fold {fold} ===\")\n",
    "        model = MADPENetNoBackbonesSurv(\n",
    "            rad_input_dim=1024, histo_input_dim=768,\n",
    "            inter_dim=256, token_dim=256, dim_hider=256\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        # Load checkpoint\n",
    "        fold_dir = os.path.join(folds_dir, f\"fold_{fold}\")\n",
    "        model_subdir = next(d for d in os.listdir(fold_dir)\n",
    "                            if os.path.isdir(os.path.join(fold_dir, d)))\n",
    "        model_file = next(f for f in os.listdir(os.path.join(fold_dir, model_subdir))\n",
    "                          if f.endswith(\"mixed_missing.pth\"))\n",
    "        checkpoint = torch.load(\n",
    "            os.path.join(fold_dir, model_subdir, model_file), map_location=\"cpu\"\n",
    "        )\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        dataset = MultimodalCTWSIDatasetSurv(\n",
    "            fold=fold,\n",
    "            split=\"test\",\n",
    "            ct_path=ct_path,\n",
    "            wsi_path=wsi_path,\n",
    "            labels_splits_path=test_path,\n",
    "            missing_modality_prob=0.0,\n",
    "            require_both_modalities=True,\n",
    "            pairing_mode=\"one_to_one\",\n",
    "            allow_repeats=True,\n",
    "            pairs_per_patient=None,\n",
    "            missing_modality=\"wsi\"\n",
    "        )\n",
    "\n",
    "        loader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "\n",
    "        # CSV Logger\n",
    "        log_file = os.path.join(output_dir, f\"fold_{fold}_log.csv\")\n",
    "        fold_tensor_dir = os.path.join(output_dir, f\"fold_{fold}\")\n",
    "        os.makedirs(fold_tensor_dir, exist_ok=True)\n",
    "\n",
    "        with open(log_file, \"w\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                \"patient_id\", \"ct_path\", \"wsi_path\",\n",
    "                \"ct_available\", \"wsi_available\",\n",
    "                \"hazard_score\", \"survtime\", \"censor\",\n",
    "                \"grad_norm\", \"curvature\",\n",
    "                \"fused_path\", \"pe_path\", \"hazard_tensor_path\"\n",
    "            ])\n",
    "\n",
    "            sample_counter = 0\n",
    "            for batch in loader:\n",
    "                # ensure floats\n",
    "                for k, v in batch.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        batch[k] = v.float()\n",
    "\n",
    "                with torch.enable_grad():\n",
    "                    for v in batch.values():\n",
    "                        if isinstance(v, torch.Tensor):\n",
    "                            v.requires_grad = True\n",
    "\n",
    "                    outputs = model(\n",
    "                        batch[\"ct_feature\"],\n",
    "                        batch[\"wsi_feature\"],\n",
    "                        modality_flag=batch[\"modality_mask\"],\n",
    "                        output_layers=[\"hazard\", \"fused_features\", \"positional_embeddings\"]\n",
    "                    )\n",
    "                    hazard = outputs[\"hazard\"].squeeze(1)  # [B]\n",
    "                    fused_features = outputs[\"fused_features\"]    # [B, D]\n",
    "                    pos_emb = outputs[\"positional_embeddings\"]    # [B, D]\n",
    "\n",
    "                    grad_norms, curvatures = compute_grad_and_curvature(\n",
    "                        fused_features, hazard,\n",
    "                        batch[\"survtime\"], batch[\"censor\"]\n",
    "                    )\n",
    "\n",
    "                # save per-sample\n",
    "                B = hazard.size(0)\n",
    "                for j in range(B):\n",
    "                    sample_id = f\"sample_{sample_counter:06d}\"\n",
    "                    fused_path = os.path.join(fold_tensor_dir, f\"{sample_id}_fused.npy\")\n",
    "                    pe_path = os.path.join(fold_tensor_dir, f\"{sample_id}_pe.npy\")\n",
    "                    hazard_path = os.path.join(fold_tensor_dir, f\"{sample_id}_hazard.npy\")\n",
    "\n",
    "                    np.save(fused_path, fused_features[j].detach().cpu().numpy())\n",
    "                    np.save(pe_path, pos_emb[j].detach().cpu().numpy())\n",
    "                    np.save(hazard_path, hazard[j].detach().cpu().numpy())\n",
    "\n",
    "                    ct_flag = int(batch[\"modality_mask\"][j, 0].item())\n",
    "                    wsi_flag = int(batch[\"modality_mask\"][j, 1].item())\n",
    "                    if batch[\"censor\"][j].item()==0: continue\n",
    "                    writer.writerow([\n",
    "                        batch[\"patient_id\"][j],\n",
    "                        batch[\"ct_path\"][j],\n",
    "                        batch[\"wsi_path\"][j],\n",
    "                        ct_flag, wsi_flag,\n",
    "                        hazard[j].item(),\n",
    "                        batch[\"survtime\"][j].item(),\n",
    "                        batch[\"censor\"][j].item(),\n",
    "                        grad_norms[j], curvatures[j],\n",
    "                        fused_path, pe_path, hazard_path\n",
    "                    ])\n",
    "\n",
    "                    sample_counter += 1\n",
    "                    \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "def find_topk_gradient_extremes(\n",
    "    csv_path: str,\n",
    "    k: int = 10,\n",
    "    output_dir: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds top-k highest and lowest |gradient norm| samples from a CSV log.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to fold_X_log.csv generated during eval.\n",
    "        k (int): Number of top and bottom samples to retrieve.\n",
    "        output_dir (str): Optional folder to save CSVs. If None, only prints results.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if \"grad_norm\" not in df.columns:\n",
    "        raise ValueError(\"The CSV does not contain 'grad_norm' column.\")\n",
    "\n",
    "    # Compute absolute gradient norms\n",
    "    df[\"abs_grad_norm\"] = df[\"grad_norm\"].abs()\n",
    "\n",
    "    # Sort by absolute gradient norm\n",
    "    df_sorted = df.sort_values(by=\"abs_grad_norm\", ascending=True)\n",
    "\n",
    "    topk_low = df_sorted.head(k).copy()\n",
    "    topk_high = df_sorted.tail(k).copy()[::-1]  # High to low\n",
    "\n",
    "    print(\"\\n🔹 Top-k LOW |Gradient Norm| Samples:\")\n",
    "    print(topk_low[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    print(\"\\n🔸 Top-k HIGH |Gradient Norm| Samples:\")\n",
    "    print(topk_high[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        low_path = os.path.join(output_dir, \"topk_low_gradient.csv\")\n",
    "        high_path = os.path.join(output_dir, \"topk_high_gradient.csv\")\n",
    "        topk_low.to_csv(low_path, index=False)\n",
    "        topk_high.to_csv(high_path, index=False)\n",
    "        print(f\"\\n📁 Saved to:\\n  {low_path}\\n  {high_path}\")\n",
    "\n",
    "    return topk_low, topk_high\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    # Example usage\n",
    "#    for i in range(4):\n",
    "#        csv_input = f\"./interpretability/interpretability_UCEC_mixed50/fold_{i}_log.csv\"\n",
    "#        find_topk_gradient_extremes(\n",
    "#            csv_path=csv_input,\n",
    "#            k=15,\n",
    "#            output_dir=f\"./interpretability/interpretability_UCEC_mixed50/fold_{i}_topk\"\n",
    "#        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246042e8",
   "metadata": {},
   "source": [
    "## SET GLOBAL SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41f464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 0\n",
    "\n",
    "def set_global_seed(seed=SEED):\n",
    "    \"\"\"\n",
    "    Set a global seed for reproducibility across different libraries and random number generators.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed value to be used\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e068e",
   "metadata": {},
   "source": [
    "## CPTAC-PDA train-mixed 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "273a21a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n",
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n",
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4 ===\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-04479   0.015239       0.015239             1              1\n",
      "0  C3L-04479   0.018554       0.018554             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-04479   0.018554       0.018554             1              1\n",
      "1  C3L-04479   0.015239       0.015239             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-02010   0.000000       0.000000             1              1\n",
      "3  C3N-02010   0.000000       0.000000             1              1\n",
      "0  C3L-03356   0.011778       0.011778             1              1\n",
      "1  C3N-01012   0.022394       0.022394             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-01012   0.022394       0.022394             1              1\n",
      "0  C3L-03356   0.011778       0.011778             1              1\n",
      "3  C3N-02010   0.000000       0.000000             1              1\n",
      "2  C3N-02010   0.000000       0.000000             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "6  C3N-00198   0.000003       0.000003             1              1\n",
      "4  C3N-00198   0.000008       0.000008             1              1\n",
      "5  C3N-00198   0.000012       0.000012             1              1\n",
      "2  C3N-00198   0.000931       0.000931             1              1\n",
      "3  C3N-00198   0.001015       0.001015             1              1\n",
      "1  C3L-02109   0.005727       0.005727             1              1\n",
      "0  C3L-02109   0.005733       0.005733             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-02109   0.005733       0.005733             1              1\n",
      "1  C3L-02109   0.005727       0.005727             1              1\n",
      "3  C3N-00198   0.001015       0.001015             1              1\n",
      "2  C3N-00198   0.000931       0.000931             1              1\n",
      "5  C3N-00198   0.000012       0.000012             1              1\n",
      "4  C3N-00198   0.000008       0.000008             1              1\n",
      "6  C3N-00198   0.000003       0.000003             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-03123   0.001971       0.001971             1              1\n",
      "0  C3L-03123   0.004932       0.004932             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-03123   0.004932       0.004932             1              1\n",
      "1  C3L-03123   0.001971       0.001971             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_3_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_4_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_4_topk/topk_high_gradient.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    }
   ],
   "source": [
    "folds_dir = \"./models/ckpts/CPTACPDA_trainmixed50_multival_Titan_MedImSight_redone\"\n",
    "test_path = \"./data/processed/processed_CPTAC_PDA_survival/k=all.tsv\"\n",
    "ct_path = \"../MedImageInsights/embeddings_output_cptacpda_93\"\n",
    "wsi_path = \"../../TitanCPTACPDA/20x_512px_0px_overlap/slide_features_titan\"\n",
    "output_dir = \"./interpretability/interpretability_pda_mixed50\"\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16)\n",
    "for i in range(5):\n",
    "    csv_input = f\"./interpretability/interpretability_pda_mixed50/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=10,\n",
    "        output_dir=f\"./interpretability/interpretability_pda_mixed50/fold_{i}_topk\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953750f",
   "metadata": {},
   "source": [
    "## CPTAC-PDA train-mixed 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0489a9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-04479   0.000053       0.000053             1              1\n",
      "1  C3L-04479   0.137801       0.137801             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-04479   0.137801       0.137801             1              1\n",
      "0  C3L-04479   0.000053       0.000053             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-01012   0.002529       0.002529             1              1\n",
      "2  C3N-02010   0.009107       0.009107             1              1\n",
      "3  C3N-02010   0.009224       0.009224             1              1\n",
      "0  C3L-03356   0.011109       0.011109             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-03356   0.011109       0.011109             1              1\n",
      "3  C3N-02010   0.009224       0.009224             1              1\n",
      "2  C3N-02010   0.009107       0.009107             1              1\n",
      "1  C3N-01012   0.002529       0.002529             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-00198   0.003446       0.003446             1              1\n",
      "3  C3N-00198   0.003934       0.003934             1              1\n",
      "0  C3L-02109   0.007536       0.007536             1              1\n",
      "1  C3L-02109   0.024497       0.024497             1              1\n",
      "5  C3N-00198   0.030822       0.030822             1              1\n",
      "4  C3N-00198   0.034195       0.034195             1              1\n",
      "6  C3N-00198   0.064661       0.064661             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "6  C3N-00198   0.064661       0.064661             1              1\n",
      "4  C3N-00198   0.034195       0.034195             1              1\n",
      "5  C3N-00198   0.030822       0.030822             1              1\n",
      "1  C3L-02109   0.024497       0.024497             1              1\n",
      "0  C3L-02109   0.007536       0.007536             1              1\n",
      "3  C3N-00198   0.003934       0.003934             1              1\n",
      "2  C3N-00198   0.003446       0.003446             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-03123   0.016339       0.016339             1              1\n",
      "1  C3L-03123   0.020274       0.020274             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-03123   0.020274       0.020274             1              1\n",
      "0  C3L-03123   0.016339       0.016339             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_3_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_4_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_4_topk/topk_high_gradient.csv\n"
     ]
    }
   ],
   "source": [
    "folds_dir = \"./models/ckpts/CPTACPDA_trainmixed30_multival_Titan_MedImSight\"\n",
    "test_path = \"./data/processed/processed_CPTAC_PDA_survival/k=all.tsv\"\n",
    "ct_path = \"../MedImageInsights/embeddings_output_cptacpda_93\"\n",
    "wsi_path = \"../../TitanCPTACPDA/20x_512px_0px_overlap/slide_features_titan\"\n",
    "output_dir = \"./interpretability/interpretability_PDA_mixed30\"\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16)\n",
    "# Example usage\n",
    "for i in range(5):\n",
    "    csv_input = f\"./interpretability/interpretability_PDA_mixed30/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_PDA_mixed30/fold_{i}_topk\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f00b78",
   "metadata": {},
   "source": [
    "## CPTAC-PDA Mixed 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b71e2204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-04479   0.002971       0.002971             1              1\n",
      "0  C3L-04479   0.048582       0.048582             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-04479   0.048582       0.048582             1              1\n",
      "1  C3L-04479   0.002971       0.002971             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-03356        0.0            0.0             1              1\n",
      "1  C3N-01012        0.0            0.0             1              1\n",
      "2  C3N-02010        0.0            0.0             1              1\n",
      "3  C3N-02010        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "3  C3N-02010        0.0            0.0             1              1\n",
      "2  C3N-02010        0.0            0.0             1              1\n",
      "1  C3N-01012        0.0            0.0             1              1\n",
      "0  C3L-03356        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-00198   0.000417       0.000417             1              1\n",
      "3  C3N-00198   0.000622       0.000622             1              1\n",
      "5  C3N-00198   0.000835       0.000835             1              1\n",
      "6  C3N-00198   0.003650       0.003650             1              1\n",
      "4  C3N-00198   0.004358       0.004358             1              1\n",
      "0  C3L-02109   0.032183       0.032183             1              1\n",
      "1  C3L-02109   0.032189       0.032189             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-02109   0.032189       0.032189             1              1\n",
      "0  C3L-02109   0.032183       0.032183             1              1\n",
      "4  C3N-00198   0.004358       0.004358             1              1\n",
      "6  C3N-00198   0.003650       0.003650             1              1\n",
      "5  C3N-00198   0.000835       0.000835             1              1\n",
      "3  C3N-00198   0.000622       0.000622             1              1\n",
      "2  C3N-00198   0.000417       0.000417             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-03123        0.0            0.0             1              1\n",
      "1  C3L-03123        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-03123        0.0            0.0             1              1\n",
      "0  C3L-03123        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_3_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_4_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_4_topk/topk_high_gradient.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds_dir = \"./models/ckpts/CPTACPDA_trainmixed15_multival_Titan_MedImSight\"\n",
    "test_path = \"./data/processed/processed_CPTAC_PDA_survival/k=all.tsv\"\n",
    "ct_path = \"../MedImageInsights/embeddings_output_cptacpda_93\"\n",
    "wsi_path = \"../../TitanCPTACPDA/20x_512px_0px_overlap/slide_features_titan\"\n",
    "output_dir = \"./interpretability/interpretability_PDA_mixed15\"\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16)\n",
    "for i in range(5):\n",
    "    csv_input = f\"./interpretability/interpretability_PDA_mixed15/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_PDA_mixed15/fold_{i}_topk\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece50f2e",
   "metadata": {},
   "source": [
    "## CPTAC PDA mixed 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efda33e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-04479   0.008891       0.008891             1              1\n",
      "0  C3L-04479   0.017203       0.017203             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-04479   0.017203       0.017203             1              1\n",
      "1  C3L-04479   0.008891       0.008891             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-01012   0.004764       0.004764             1              1\n",
      "0  C3L-03356   0.010079       0.010079             1              1\n",
      "3  C3N-02010   0.025140       0.025140             1              1\n",
      "2  C3N-02010   0.025422       0.025422             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-02010   0.025422       0.025422             1              1\n",
      "3  C3N-02010   0.025140       0.025140             1              1\n",
      "0  C3L-03356   0.010079       0.010079             1              1\n",
      "1  C3N-01012   0.004764       0.004764             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "4  C3N-00198   0.000409       0.000409             1              1\n",
      "5  C3N-00198   0.003613       0.003613             1              1\n",
      "6  C3N-00198   0.003953       0.003953             1              1\n",
      "2  C3N-00198   0.004511       0.004511             1              1\n",
      "3  C3N-00198   0.005625       0.005625             1              1\n",
      "1  C3L-02109   0.016683       0.016683             1              1\n",
      "0  C3L-02109   0.029697       0.029697             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-02109   0.029697       0.029697             1              1\n",
      "1  C3L-02109   0.016683       0.016683             1              1\n",
      "3  C3N-00198   0.005625       0.005625             1              1\n",
      "2  C3N-00198   0.004511       0.004511             1              1\n",
      "6  C3N-00198   0.003953       0.003953             1              1\n",
      "5  C3N-00198   0.003613       0.003613             1              1\n",
      "4  C3N-00198   0.000409       0.000409             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-03123        0.0            0.0             1              1\n",
      "1  C3L-03123        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-03123        0.0            0.0             1              1\n",
      "0  C3L-03123        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_3_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_4_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_4_topk/topk_high_gradient.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds_dir = \"./models/ckpts/CPTACPDA_trainmixed5_multival_Titan_MedImSight\"\n",
    "test_path = \"./data/processed/processed_CPTAC_PDA_survival/k=all.tsv\"\n",
    "ct_path = \"../MedImageInsights/embeddings_output_cptacpda_93\"\n",
    "wsi_path = \"../../TitanCPTACPDA/20x_512px_0px_overlap/slide_features_titan\"\n",
    "output_dir = \"./interpretability/interpretability_PDA_mixed5\"\n",
    "\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16)\n",
    "for i in range(5):\n",
    "    csv_input = f\"./interpretability/interpretability_PDA_mixed5/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_PDA_mixed5/fold_{i}_topk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41616de2",
   "metadata": {},
   "source": [
    "## CPTAC UCEC 5% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116b94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from itertools import product\n",
    "#from pathlib import Path\n",
    "\n",
    "#import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "#import torchstain\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "#from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class   MultimodalCTWSIDatasetSurv(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for paired CT and WSI data, handling missing modalities\n",
    "        at both dataset and sampling level\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fold: int,\n",
    "        split: str,  # either \"train\" or \"test\"\n",
    "        ct_path: str,\n",
    "        wsi_path: str,\n",
    "        labels_splits_path: str,\n",
    "        missing_modality_prob: float = 0.0,  # Additional random masking probability\n",
    "        missing_modality: str = \"both\",\n",
    "        require_both_modalities: bool = False,  # Whether to only include patients\n",
    "        # with both modalities\n",
    "        pairing_mode: str = None,  # 'all_combinations, 'one_to_one', 'fixed_count'\n",
    "        pairs_per_patient: int = None,  # For fixed_count_mode\n",
    "        allow_repeats: bool = False,  # For fixed_count mode\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # assert split in [\"train\", \"val\", \"overfit\", \"all\"]\n",
    "        assert split in [\"train\", \"test\"]\n",
    "        assert pairing_mode in [\"all_combinations\", \"one_to_one\", \"fixed_count\"]\n",
    "        assert 0 <= missing_modality_prob <= 1\n",
    "        assert missing_modality in [\"ct\", \"wsi\", \"both\"]\n",
    "        self.missing_modality = missing_modality\n",
    "        self.fold = fold\n",
    "        self.split = split\n",
    "        self.ct_path = ct_path\n",
    "        self.wsi_path = wsi_path\n",
    "        self.missing_modality_prob = missing_modality_prob\n",
    "        self.require_both_modalities = require_both_modalities\n",
    "        self.labels_splits_path = labels_splits_path\n",
    "        self.pairing_mode = (\n",
    "            pairing_mode  # 'all_combinations', 'one_to_one', or 'fixed_count'\n",
    "        )\n",
    "        self.pairs_per_patient = pairs_per_patient  # For fixed_count mode\n",
    "        self.allow_repeats = allow_repeats  # For fixed_count mode\n",
    "        labels_splits = pd.read_csv(labels_splits_path, sep=\"\\t\")\n",
    "        labels_splits = labels_splits[labels_splits[f\"fold_{self.fold}\"] == self.split]\n",
    "        self.labels_splits = labels_splits[\n",
    "            [\"case_id\", \"OS_days\", \"OS_event\"]\n",
    "        ].drop_duplicates(\"case_id\")\n",
    "\n",
    "        # Initialize data structures\n",
    "        # Will store CT and WSI paths per patient\n",
    "        self.patient_data = {}\n",
    "        # Will store all valid combinations\n",
    "        self.samples = []\n",
    "        self.modality_stats = {\"ct_only\": 0, \"wsi_only\": 0, \"both\": 0}\n",
    "\n",
    "        # Load split file\n",
    "        self._load_split()\n",
    "\n",
    "    def _get_max_pairs_for_patient(self, ct_scans, wsi_folders, allow_repeats):\n",
    "        \"\"\"\n",
    "        Calculate maximum possible pairs for a patient based on available data and pairing mode.\n",
    "\n",
    "        Args:\n",
    "            ct_scans (list): List of CT scan files\n",
    "            wsi_folders (list): List of WSI folder names\n",
    "            allow_repeats (bool): If True, return all possible combinations count\n",
    "                                If False, return maximum unique pairs count\n",
    "\n",
    "        Returns:\n",
    "            int: Maximum number of possible pairs\n",
    "        \"\"\"\n",
    "        if not ct_scans or not wsi_folders:\n",
    "            return 0\n",
    "\n",
    "        if allow_repeats:\n",
    "            return len(ct_scans) * len(wsi_folders)\n",
    "        else:  # one-to-one\n",
    "            return min(len(ct_scans), len(wsi_folders))\n",
    "\n",
    "    def _get_fixed_pairs(self, ct_scans, wsi_folders, n_pairs, allow_repeats=True):\n",
    "        \"\"\"\n",
    "        Generate fixed number of pairs between CT scans and WSI folders.\n",
    "\n",
    "        Args:\n",
    "            ct_scans (list): List of CT scan files\n",
    "            wsi_folders (list): List of WSI folder names\n",
    "            n_pairs (int): Number of pairs to generate\n",
    "            allow_repeats (bool): If True, allows repeating elements to reach n_pairs\n",
    "                                If False, limits pairs to minimum unique combinations\n",
    "\n",
    "        Returns:\n",
    "            list: List of (ct_scan, wsi_folder) pairs\n",
    "        \"\"\"\n",
    "        max_unique_pairs = min(len(ct_scans), len(wsi_folders))\n",
    "\n",
    "        if not allow_repeats:\n",
    "            n_pairs = min(n_pairs, max_unique_pairs)\n",
    "\n",
    "        # Generate initial unique pairs\n",
    "        shuffled_ct = ct_scans.copy()\n",
    "        shuffled_wsi = wsi_folders.copy()\n",
    "        random.shuffle(shuffled_ct)\n",
    "        random.shuffle(shuffled_wsi)\n",
    "\n",
    "        pairs = list(\n",
    "            zip(shuffled_ct[:max_unique_pairs], shuffled_wsi[:max_unique_pairs])\n",
    "        )\n",
    "\n",
    "        if n_pairs <= len(pairs):\n",
    "            # Downsample if needed\n",
    "            random.shuffle(pairs)\n",
    "            return pairs[:n_pairs]\n",
    "\n",
    "        if not allow_repeats:\n",
    "            return pairs\n",
    "\n",
    "        # Need to generate additional pairs with repeats\n",
    "        while len(pairs) < n_pairs:\n",
    "            ct_scan = random.choice(ct_scans)\n",
    "            wsi_folder = random.choice(wsi_folders)\n",
    "            pairs.append((ct_scan, wsi_folder))\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def _load_split(self):\n",
    "        \"\"\"Load and organize all CT and WSI data for the given split\n",
    "\n",
    "        Supports different pairing modes:\n",
    "        - 'all_combinations': Creates all possible CT-WSI pairs\n",
    "        - 'one_to_one': Creates random 1:1 pairs\n",
    "        - 'fixed_count': Creates fixed number of pairs per patient\n",
    "        \"\"\"\n",
    "\n",
    "        # First pass: count maximum possible pairs per patient\n",
    "        max_pairs_possible = float(\"inf\")\n",
    "        if self.pairing_mode == \"fixed_count\":\n",
    "            for patient_id in self.labels_splits[\"case_id\"].values:\n",
    "\n",
    "                ct_path = os.path.join(self.ct_path, patient_id)\n",
    "                ct_features = []\n",
    "                if os.path.exists(ct_path):\n",
    "                    ct_features = [f for f in os.listdir(ct_path)]\n",
    "\n",
    "                wsi_path = os.path.join(self.wsi_path)\n",
    "                wsi_features = [\n",
    "                    f\n",
    "                    for f in os.listdir(wsi_path)\n",
    "                    if patient_id in f and os.path.isdir(os.path.join(wsi_path, f))\n",
    "                ]\n",
    "\n",
    "                patient_max_pairs = self._get_max_pairs_for_patient(\n",
    "                    ct_features, wsi_features, self.allow_repeats\n",
    "                )\n",
    "\n",
    "                if patient_max_pairs > 0:  # Only update if patient has both modalities\n",
    "\n",
    "                    max_pairs_possible = min(max_pairs_possible, patient_max_pairs)\n",
    "\n",
    "        # Use provided pairs_per_patient or calculated maximum\n",
    "        n_pairs = (\n",
    "            self.pairs_per_patient\n",
    "            if self.pairs_per_patient is not None\n",
    "            else max_pairs_possible\n",
    "        )\n",
    "\n",
    "        # Main loading loop\n",
    "        for row in self.labels_splits[\"case_id\"].values:\n",
    "            patient_id = row.strip()\n",
    "            # Find all CT scans for this patient\n",
    "            ct_path = os.path.join(self.ct_path, patient_id)\n",
    "            ct_features = []\n",
    "            if os.path.exists(ct_path):\n",
    "                ct_features = [f for f in os.listdir(ct_path)]\n",
    "\n",
    "            # Find all WSI .h5 files for this patient\n",
    "            wsi_path = self.wsi_path\n",
    "            wsi_features = [f for f in os.listdir(wsi_path) if patient_id in f]\n",
    "\n",
    "            # Skip patient if we require both modalities and they don't have them\n",
    "            if self.require_both_modalities and (not ct_features or not wsi_features):\n",
    "                continue\n",
    "\n",
    "            # Store available data for this patient\n",
    "            self.patient_data[patient_id] = {\n",
    "                \"ct_features\": ct_features,\n",
    "                \"wsi_features\": wsi_features,\n",
    "            }\n",
    "\n",
    "            # Update modality statistics\n",
    "            if ct_features and wsi_features:\n",
    "                self.modality_stats[\"both\"] += 1\n",
    "            elif ct_features:\n",
    "                self.modality_stats[\"ct_only\"] += 1\n",
    "            else:\n",
    "                self.modality_stats[\"wsi_only\"] += 1\n",
    "\n",
    "            cnt = 0\n",
    "            # Generate samples based on available data and pairing mode\n",
    "            if ct_features and wsi_features:\n",
    "                if self.pairing_mode == \"fixed_count\":\n",
    "                    # Generate fixed number of pairs\n",
    "                    pairs = self._get_fixed_pairs(\n",
    "                        ct_features, wsi_features, n_pairs, self.allow_repeats\n",
    "                    )\n",
    "\n",
    "                    for ct_feature, wsi_feature in pairs:\n",
    "                        cnt += 1\n",
    "                        self.samples.append(\n",
    "                            {\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"ct_path\": os.path.join(\n",
    "                                    self.ct_path, patient_id, ct_feature\n",
    "                                ),\n",
    "                                \"wsi_folder\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                                \"base_modality_mask\": [1, 1],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                elif self.pairing_mode == \"one_to_one\":\n",
    "                    # Original one_to_one logic\n",
    "                    num_pairs = min(len(ct_features), len(wsi_features))\n",
    "                    shuffled_ct = ct_features.copy()\n",
    "                    shuffled_wsi = wsi_features.copy()\n",
    "                    random.shuffle(shuffled_ct)\n",
    "                    random.shuffle(shuffled_wsi)\n",
    "\n",
    "                    for ct_feature, wsi_feature in zip(\n",
    "                        shuffled_ct[:num_pairs], shuffled_wsi[:num_pairs]\n",
    "                    ):\n",
    "                        cnt += 1\n",
    "                        self.samples.append(\n",
    "                            {\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"ct_path\": os.path.join(\n",
    "                                    self.ct_path, patient_id, ct_feature\n",
    "                                ),\n",
    "                                \"wsi_feature\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                                \"base_modality_mask\": [1, 1],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                else:  # 'all_combinations' mode\n",
    "                    for ct_feature, wsi_feature in product(ct_features, wsi_features):\n",
    "                        cnt += 1\n",
    "                        self.samples.append(\n",
    "                            {\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"ct_path\": os.path.join(\n",
    "                                    self.ct_path, patient_id, ct_feature\n",
    "                                ),\n",
    "                                \"wsi_feature\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                                \"base_modality_mask\": [1, 1],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            elif ct_features:\n",
    "                for ct_feature in ct_features:\n",
    "                    cnt += 1\n",
    "                    self.samples.append(\n",
    "                        {\n",
    "                            \"patient_id\": patient_id,\n",
    "                            \"ct_path\": os.path.join(\n",
    "                                self.ct_path, patient_id, ct_feature\n",
    "                            ),\n",
    "                            \"wsi_feature\": None,\n",
    "                            \"base_modality_mask\": [1, 0],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            elif wsi_features:\n",
    "                for wsi_feature in wsi_features:\n",
    "                    cnt += 1\n",
    "                    self.samples.append(\n",
    "                        {\n",
    "                            \"patient_id\": patient_id,\n",
    "                            \"ct_path\": None,\n",
    "                            \"wsi_feature\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                            \"base_modality_mask\": [0, 1],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    def _load_ct_feature(self, ct_path):\n",
    "        \"\"\"Load and standardize CT feature\"\"\"\n",
    "        if ct_path.endswith(\".pt\"):\n",
    "            volume = np.array(torch.load(ct_path, weights_only=True))\n",
    "        elif ct_path.endswith(\".npy\"):\n",
    "            volume = np.load(ct_path)\n",
    "        return volume\n",
    "\n",
    "    def _load_wsi_feature(self, wsi_path):\n",
    "        \"\"\"Load WSI feature\"\"\"\n",
    "        feature = None\n",
    "        with h5py.File(wsi_path, \"r\") as f:\n",
    "            feature = np.array(f[\"features\"][:])\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def _get_empty_ct_feature(self):\n",
    "        \"\"\"Return empty CT feature of correct shape\"\"\"\n",
    "        return np.zeros((131, 1024))\n",
    "\n",
    "    def _get_empty_wsi_feature(self):\n",
    "        \"\"\"Return empty WSI feature of correct shape\"\"\"\n",
    "        return np.zeros((768,))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'patient_id': str,\n",
    "                'ct_feature': numpy array or zeros if missing,\n",
    "                'wsi_feature': tensor or zeros if missing,\n",
    "                'label': int,\n",
    "                'modality_mask': tensor indicating present modalities [CT, WSI],\n",
    "                'base_modality_mask': tensor indicating modalities available in dataset\n",
    "            }\n",
    "        \"\"\"\n",
    "\n",
    "        sample = self.samples[index]\n",
    "        patient_id = sample[\"patient_id\"]\n",
    "        base_mask = sample[\"base_modality_mask\"]\n",
    "        ct_path = sample[\"ct_path\"]\n",
    "        wsi_path = sample[\"wsi_feature\"]\n",
    "        if wsi_path is None: wsi_path = \"\"\n",
    "        if ct_path is None: ct_path = \"\"\n",
    "        # Apply additional random masking only to available modalities\n",
    "        final_mask = base_mask.copy()\n",
    "        if self.missing_modality_prob > 0:\n",
    "            if self.missing_modality == \"both\":\n",
    "                for i in range(2):\n",
    "                    if (\n",
    "                        base_mask[i] == 1\n",
    "                        and random.random() < self.missing_modality_prob\n",
    "                    ):\n",
    "                        final_mask[i] = 0\n",
    "            elif self.missing_modality == \"ct\":\n",
    "                if base_mask[0] == 1 and random.random() < self.missing_modality_prob:\n",
    "                    final_mask[0] = 0\n",
    "            elif self.missing_modality == \"wsi\":\n",
    "                if base_mask[1] == 1 and random.random() < self.missing_modality_prob:\n",
    "                    final_mask[1] = 0\n",
    "\n",
    "            # Ensure at least one modality remains if it was originally available\n",
    "            if sum(final_mask) == 0 and sum(base_mask) > 0:\n",
    "                # Randomly choose one of the originally available modalities\n",
    "                available_indices = [i for i in range(2) if base_mask[i] == 1]\n",
    "                chosen_idx = random.choice(available_indices)\n",
    "                final_mask[chosen_idx] = 1\n",
    "\n",
    "        # Load features based on final mask\n",
    "        ct_feature = (\n",
    "            self._load_ct_feature(sample[\"ct_path\"])\n",
    "            if final_mask[0] and sample[\"ct_path\"]\n",
    "            else self._get_empty_ct_feature()\n",
    "        )\n",
    "\n",
    "        wsi_feature = (\n",
    "            self._load_wsi_feature(sample[\"wsi_feature\"])\n",
    "            if final_mask[1] and sample[\"wsi_feature\"]\n",
    "            else self._get_empty_wsi_feature()\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"patient_id\": patient_id,\n",
    "            \"ct_feature\": torch.from_numpy(ct_feature).float(),\n",
    "            \"wsi_feature\": torch.from_numpy(wsi_feature).float(),\n",
    "            \"survtime\": torch.tensor(\n",
    "                self.labels_splits[self.labels_splits[\"case_id\"] == patient_id][\n",
    "                    \"OS_days\"\n",
    "                ].iloc[0],\n",
    "                dtype=torch.long,\n",
    "            ),\n",
    "            \"censor\": ~torch.tensor(\n",
    "                self.labels_splits[self.labels_splits[\"case_id\"] == patient_id][\n",
    "                    \"OS_event\"\n",
    "                ].iloc[0],\n",
    "                dtype=torch.bool,\n",
    "            ),\n",
    "            \"modality_mask\": torch.tensor(final_mask, dtype=torch.float32),\n",
    "            \"base_modality_mask\": torch.tensor(base_mask, dtype=torch.float32),\n",
    "            \"ct_path\": ct_path,\n",
    "            \"wsi_path\": wsi_path\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def stats(self):\n",
    "        \"\"\"Return dataset statistics\"\"\"\n",
    "        return {\n",
    "            \"total_samples\": len(self.samples),\n",
    "            \"total_patients\": len(self.patient_data),\n",
    "            \"modality_availability\": self.modality_stats,\n",
    "            \"missing_modality_prob\": self.missing_modality_prob,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def move_batch_to_device(batch, device):\n",
    "        \"\"\"Move all elements of the batch to device\"\"\"\n",
    "        batch[\"ct_feature\"] = batch[\"ct_feature\"].to(device)\n",
    "        batch[\"wsi_feature\"] = batch[\"wsi_feature\"].to(device)\n",
    "        batch[\"survtime\"] = batch[\"survtime\"].to(device)\n",
    "        batch[\"censor\"] = batch[\"censor\"].to(device)\n",
    "        batch[\"modality_mask\"] = batch[\"modality_mask\"].to(device)\n",
    "        batch[\"ct_path\"] = batch[\"ct_path\"].to(device)\n",
    "        batch[\"wsi_path\"] = batch[\"wsi_path\"].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a09b2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-02631   0.025622       0.025622             1              1\n",
      "0  C3N-02631   0.025737       0.025737             1              1\n",
      "2  C3N-02631   0.059709       0.059709             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-02631   0.059709       0.059709             1              1\n",
      "0  C3N-02631   0.025737       0.025737             1              1\n",
      "1  C3N-02631   0.025622       0.025622             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02678   0.248485       0.248485             1              1\n",
      "1  C3N-02678   0.248485       0.248485             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-02678   0.248485       0.248485             1              1\n",
      "0  C3N-02678   0.248485       0.248485             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-01877        0.0            0.0             1              1\n",
      "1  C3N-01877        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-01877        0.0            0.0             1              1\n",
      "0  C3N-01877        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02639        0.0            0.0             1              1\n",
      "1  C3N-02639        0.0            0.0             1              1\n",
      "2  C3N-01003        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-01003        0.0            0.0             1              1\n",
      "1  C3N-02639        0.0            0.0             1              1\n",
      "0  C3N-02639        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_3_topk/topk_high_gradient.csv\n"
     ]
    }
   ],
   "source": [
    "folds_dir = \"./models/ckpts/CPTAC_UCEC_titan_medimsight_trainmixed5\"\n",
    "ct_path= \"../MedImageInsights/embeddings_cptacucec\"\n",
    "wsi_path= \"../../trident_processed_UCEC_titan/20x_512px_0px_overlap/slide_features_titan\"\n",
    "test_path =  \"./data/processed/processed_CPTACUCEC_survival/k=all.tsv\"\n",
    "output_dir = \"./interpretability/interpretability_UCEC_mixed5\"\n",
    "\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16, n_folds=4)\n",
    "for i in range(4):\n",
    "    csv_input = f\"./interpretability/interpretability_UCEC_mixed5/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_UCEC_mixed5/fold_{i}_topk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51956999",
   "metadata": {},
   "source": [
    "## CPTAC UCEC 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f34bc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-02631   0.005976       0.005976             1              1\n",
      "0  C3N-02631   0.058858       0.058858             1              1\n",
      "2  C3N-02631   0.097936       0.097936             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-02631   0.097936       0.097936             1              1\n",
      "0  C3N-02631   0.058858       0.058858             1              1\n",
      "1  C3N-02631   0.005976       0.005976             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-02678   0.008589       0.008589             1              1\n",
      "0  C3N-02678   0.008673       0.008673             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02678   0.008673       0.008673             1              1\n",
      "1  C3N-02678   0.008589       0.008589             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-01877   0.033299       0.033299             1              1\n",
      "0  C3N-01877   0.037402       0.037402             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-01877   0.037402       0.037402             1              1\n",
      "1  C3N-01877   0.033299       0.033299             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-01003   0.000654       0.000654             1              1\n",
      "1  C3N-02639   0.083787       0.083787             1              1\n",
      "0  C3N-02639   0.085376       0.085376             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02639   0.085376       0.085376             1              1\n",
      "1  C3N-02639   0.083787       0.083787             1              1\n",
      "2  C3N-01003   0.000654       0.000654             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_3_topk/topk_high_gradient.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "folds_dir = \"./models/ckpts/CPTAC_UCEC_titan_medimsight_trainmixed15\"\n",
    "ct_path= \"../MedImageInsights/embeddings_cptacucec\"\n",
    "wsi_path= \"../../trident_processed_UCEC_titan/20x_512px_0px_overlap/slide_features_titan\"\n",
    "test_path =  \"./data/processed/processed_CPTACUCEC_survival/k=all.tsv\"\n",
    "output_dir = \"./interpretability/interpretability_UCEC_mixed15\"\n",
    "\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16, n_folds=4)\n",
    "for i in range(4):\n",
    "    csv_input = f\"./interpretability/interpretability_UCEC_mixed15/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_UCEC_mixed15/fold_{i}_topk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c024d",
   "metadata": {},
   "source": [
    "## CPTAC UCEC 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50e3a607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02631   0.041842       0.041842             1              1\n",
      "1  C3N-02631   0.046953       0.046953             1              1\n",
      "2  C3N-02631   0.128688       0.128688             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-02631   0.128688       0.128688             1              1\n",
      "1  C3N-02631   0.046953       0.046953             1              1\n",
      "0  C3N-02631   0.041842       0.041842             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02678   0.010207       0.010207             1              1\n",
      "1  C3N-02678   0.011398       0.011398             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-02678   0.011398       0.011398             1              1\n",
      "0  C3N-02678   0.010207       0.010207             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-01877   0.002767       0.002767             1              1\n",
      "1  C3N-01877   0.003491       0.003491             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-01877   0.003491       0.003491             1              1\n",
      "0  C3N-01877   0.002767       0.002767             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02639        0.0            0.0             1              1\n",
      "1  C3N-02639        0.0            0.0             1              1\n",
      "2  C3N-01003        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-01003        0.0            0.0             1              1\n",
      "1  C3N-02639        0.0            0.0             1              1\n",
      "0  C3N-02639        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_3_topk/topk_high_gradient.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "folds_dir = \"./models/ckpts/CPTAC_UCEC_titan_medimsight_trainmixed30\"\n",
    "ct_path= \"../MedImageInsights/embeddings_cptacucec\"\n",
    "wsi_path= \"../../trident_processed_UCEC_titan/20x_512px_0px_overlap/slide_features_titan\"\n",
    "test_path =  \"./data/processed/processed_CPTACUCEC_survival/k=all.tsv\"\n",
    "output_dir = \"./interpretability/interpretability_UCEC_mixed30\"\n",
    "\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16, n_folds=4)\n",
    "for i in range(4):\n",
    "    csv_input = f\"./interpretability/interpretability_UCEC_mixed30/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_UCEC_mixed30/fold_{i}_topk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf4e94",
   "metadata": {},
   "source": [
    "## CPTAC UCEC 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5363406a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_and_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/processed/processed_CPTACUCEC_survival/k=all.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./interpretability/interpretability_UCEC_mixed50\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mevaluate_and_log\u001b[49m(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, n_folds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m      9\u001b[0m     csv_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./interpretability/interpretability_UCEC_mixed50/fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_log.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_and_log' is not defined"
     ]
    }
   ],
   "source": [
    "folds_dir = \"./models/ckpts/CPTACUCEC_trainmixed50_multival_Titan_MedImSight_fullepochs\"\n",
    "ct_path = \"../MedImageInsights/embeddings_cptacucec\"\n",
    "wsi_path = \"../../trident_processed_UCEC_titan/20x_512px_0px_overlap/slide_features_titan\"\n",
    "test_path = \"./data/processed/processed_CPTACUCEC_survival/k=all.tsv\"\n",
    "output_dir = \"./interpretability/interpretability_UCEC_mixed50\"\n",
    "\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16, n_folds=4)\n",
    "for i in range(4):\n",
    "    csv_input = f\"./interpretability/interpretability_UCEC_mixed50/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_UCEC_mixed50/fold_{i}_topk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e82612",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a76b90",
   "metadata": {},
   "source": [
    "## CPTAC PDA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from data.multimodal_features_surv import MultimodalCTWSIDatasetSurv\n",
    "from models.dpe.main_model_nobackbone_surv_new_gcs import MADPENetNoBackbonesSurv\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "def evaluate_and_log(folds_dir, ct_path, wsi_path, test_path, output_dir, batch_size=16, n_folds=5):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        print(f\"\\n=== Fold {fold} ===\")\n",
    "        model = MADPENetNoBackbonesSurv(\n",
    "            rad_input_dim=1024, histo_input_dim=768,\n",
    "            inter_dim=256, token_dim=256, dim_hider=256\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        # Load checkpoint\n",
    "        fold_dir = os.path.join(folds_dir, f\"fold_{fold}\")\n",
    "        model_subdir = next(d for d in os.listdir(fold_dir)\n",
    "                            if os.path.isdir(os.path.join(fold_dir, d)))\n",
    "        model_file = next(f for f in os.listdir(os.path.join(fold_dir, model_subdir))\n",
    "                          if f.endswith(\"mixed_missing.pth\"))\n",
    "        checkpoint = torch.load(\n",
    "            os.path.join(fold_dir, model_subdir, model_file), map_location=\"cpu\"\n",
    "        )\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        dataset = MultimodalCTWSIDatasetSurv(\n",
    "            fold=0,\n",
    "            split=\"train\",\n",
    "            ct_path=ct_path,\n",
    "            wsi_path=wsi_path,\n",
    "            labels_splits_path=test_path,\n",
    "            missing_modality_prob=0.0,\n",
    "            require_both_modalities=False,\n",
    "            pairing_mode=\"one_to_one\",\n",
    "            allow_repeats=True,\n",
    "            pairs_per_patient=None,\n",
    "            missing_modality=\"ct\"\n",
    "        )\n",
    "        print(dataset.modality_stats)\n",
    "        loader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "\n",
    "        # CSV Logger\n",
    "        log_file = os.path.join(output_dir, f\"fold_{fold}_log.csv\")\n",
    "        fold_tensor_dir = os.path.join(output_dir, f\"fold_{fold}\")\n",
    "        os.makedirs(fold_tensor_dir, exist_ok=True)\n",
    "\n",
    "        with open(log_file, \"w\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                \"patient_id\", \"ct_path\", \"wsi_path\",\n",
    "                \"ct_available\", \"wsi_available\",\n",
    "                \"hazard_score\", \"survtime\", \"censor\",\n",
    "                \"grad_norm\", \"curvature\",\n",
    "                \"fused_path\", \"pe_path\", \"hazard_tensor_path\"\n",
    "            ])\n",
    "\n",
    "            sample_counter = 0\n",
    "            for batch in loader:\n",
    "                \n",
    "                # ensure floats\n",
    "                for k, v in batch.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        batch[k] = v.float()\n",
    "\n",
    "                with torch.enable_grad():\n",
    "                    for v in batch.values():\n",
    "                        if isinstance(v, torch.Tensor):\n",
    "                            v.requires_grad = True\n",
    "\n",
    "                    outputs = model(\n",
    "                        batch[\"ct_feature\"],\n",
    "                        batch[\"wsi_feature\"],\n",
    "                        modality_flag=batch[\"modality_mask\"],\n",
    "                        output_layers=[\"hazard\", \"fused_features\", \"positional_embeddings\"]\n",
    "                    )\n",
    "                    hazard = outputs[\"hazard\"].squeeze(1)  # [B]\n",
    "                    \n",
    "                    fused_features = outputs[\"fused_features\"]    # [B, D]\n",
    "                    pos_emb = outputs[\"positional_embeddings\"]    # [B, D]\n",
    "\n",
    "                    grad_norms, curvatures = compute_grad_and_curvature(\n",
    "                        fused_features, hazard,\n",
    "                        batch[\"survtime\"], batch[\"censor\"]\n",
    "                    )\n",
    "\n",
    "                # save per-sample\n",
    "                B = hazard.size(0)\n",
    "                for j in range(B):\n",
    "                    sample_id = f\"sample_{sample_counter:06d}\"\n",
    "                    fused_path = os.path.join(fold_tensor_dir, f\"{sample_id}_fused.npy\")\n",
    "                    pe_path = os.path.join(fold_tensor_dir, f\"{sample_id}_pe.npy\")\n",
    "                    hazard_path = os.path.join(fold_tensor_dir, f\"{sample_id}_hazard.npy\")\n",
    "\n",
    "                    np.save(fused_path, fused_features[j].detach().cpu().numpy())\n",
    "                    np.save(pe_path, pos_emb[j].detach().cpu().numpy())\n",
    "                    np.save(hazard_path, hazard[j].detach().cpu().numpy())\n",
    "\n",
    "                    ct_flag = int(batch[\"modality_mask\"][j, 0].item())\n",
    "                    wsi_flag = int(batch[\"modality_mask\"][j, 1].item())\n",
    "                    if batch[\"censor\"][j].item()==0: continue\n",
    "                    writer.writerow([\n",
    "                        batch[\"patient_id\"][j],\n",
    "                        batch[\"ct_path\"][j],\n",
    "                        batch[\"wsi_path\"][j],\n",
    "                        ct_flag, wsi_flag,\n",
    "                        hazard[j].item(),\n",
    "                        batch[\"survtime\"][j].item(),\n",
    "                        batch[\"censor\"][j].item(),\n",
    "                        grad_norms[j], curvatures[j],\n",
    "                        fused_path, pe_path, hazard_path\n",
    "                    ])\n",
    "\n",
    "                    sample_counter += 1\n",
    "                    \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "def find_topk_gradient_extremes(\n",
    "    csv_path: str,\n",
    "    k: int = 10,\n",
    "    output_dir: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds top-k highest and lowest |gradient norm| samples from a CSV log.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to fold_X_log.csv generated during eval.\n",
    "        k (int): Number of top and bottom samples to retrieve.\n",
    "        output_dir (str): Optional folder to save CSVs. If None, only prints results.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if \"grad_norm\" not in df.columns:\n",
    "        raise ValueError(\"The CSV does not contain 'grad_norm' column.\")\n",
    "\n",
    "    # Compute absolute gradient norms\n",
    "    df[\"abs_grad_norm\"] = df[\"grad_norm\"].abs()\n",
    "\n",
    "    # Sort by absolute gradient norm\n",
    "    df_sorted = df.sort_values(by=\"abs_grad_norm\", ascending=True)\n",
    "\n",
    "    topk_low = df_sorted.head(k).copy()\n",
    "    topk_high = df_sorted.tail(k).copy()[::-1]  # High to low\n",
    "\n",
    "    print(\"\\n🔹 Top-k LOW |Gradient Norm| Samples:\")\n",
    "    print(topk_low[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    print(\"\\n🔸 Top-k HIGH |Gradient Norm| Samples:\")\n",
    "    print(topk_high[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        low_path = os.path.join(output_dir, \"topk_low_gradient.csv\")\n",
    "        high_path = os.path.join(output_dir, \"topk_high_gradient.csv\")\n",
    "        topk_low.to_csv(low_path, index=False)\n",
    "        topk_high.to_csv(high_path, index=False)\n",
    "        print(f\"\\n📁 Saved to:\\n  {low_path}\\n  {high_path}\")\n",
    "\n",
    "    return topk_low, topk_high\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 0\n",
    "\n",
    "def set_global_seed(seed=SEED):\n",
    "    \"\"\"\n",
    "    Set a global seed for reproducibility across different libraries and random number generators.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed value to be used\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "871f7286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n",
      "{'ct_only': 1, 'wsi_only': 86, 'both': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3800379/1792767783.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3800379/769493430.py:388: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [131, 1024] at entry 0 and [66, 1024] at entry 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m folds_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/ckpts/CPTACPDA_trainmixed50_multival_Titan_MedImSight_new\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/processed/processed_CPTAC_PDA_test/k=all.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mevaluate_and_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolds_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mct_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwsi_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     10\u001b[0m     csv_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./interpretability/interpretability_PDA_mixed50TEST/fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_log.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[29], line 58\u001b[0m, in \u001b[0;36mevaluate_and_log\u001b[0;34m(folds_dir, ct_path, wsi_path, test_path, output_dir, batch_size, n_folds)\u001b[0m\n\u001b[1;32m     49\u001b[0m writer\u001b[38;5;241m.\u001b[39mwriterow([\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mct_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwsi_path\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mct_available\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwsi_available\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpe_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhazard_tensor_path\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m ])\n\u001b[1;32m     57\u001b[0m sample_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     59\u001b[0m     \n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# ensure floats\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/anaconda3/envs/research_biocv_proj/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/research_biocv_proj/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/research_biocv_proj/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/research_biocv_proj/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/research_biocv_proj/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:171\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m--> 171\u001b[0m         {\n\u001b[1;32m    172\u001b[0m             key: collate(\n\u001b[1;32m    173\u001b[0m                 [d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map\n\u001b[1;32m    174\u001b[0m             )\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    176\u001b[0m         }\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/research_biocv_proj/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:172\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[1;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    171\u001b[0m         {\n\u001b[0;32m--> 172\u001b[0m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[1;32m    176\u001b[0m         }\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/research_biocv_proj/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/anaconda3/envs/research_biocv_proj/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [131, 1024] at entry 0 and [66, 1024] at entry 7"
     ]
    }
   ],
   "source": [
    "ct_path=\"../MedImageInsights/embeddings_output_cptacpda_93\"\n",
    "wsi_path=\"../../TitanCPTACPDA/20x_512px_0px_overlap/slide_features_titan\"\n",
    "output_dir = \"./interpretability/interpretability_PDA_mixed50TEST\"\n",
    "folds_dir = \"./models/ckpts/CPTACPDA_trainmixed50_multival_Titan_MedImSight_new\"\n",
    "test_path = \"./data/processed/processed_CPTAC_PDA_test/k=all.tsv\"\n",
    "\n",
    "\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16, n_folds=5)\n",
    "for i in range(5):\n",
    "    csv_input = f\"./interpretability/interpretability_PDA_mixed50TEST/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_PDA_mixed50TEST/fold_{i}_topk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b735d",
   "metadata": {},
   "source": [
    "## CPTAC UCEC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "391c9b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from data.multimodal_features_surv import MultimodalCTWSIDatasetSurv\n",
    "from models.dpe.main_model_nobackbone_surv_new_gcs import MADPENetNoBackbonesSurv\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "def evaluate_and_log(folds_dir, ct_path, wsi_path, test_path, output_dir, batch_size=16, n_folds=5):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        print(f\"\\n=== Fold {fold} ===\")\n",
    "        model = MADPENetNoBackbonesSurv(\n",
    "            rad_input_dim=1024, histo_input_dim=768,\n",
    "            inter_dim=256, token_dim=256, dim_hider=256\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        # Load checkpoint\n",
    "        fold_dir = os.path.join(folds_dir, f\"fold_{fold}\")\n",
    "        model_subdir = next(d for d in os.listdir(fold_dir)\n",
    "                            if os.path.isdir(os.path.join(fold_dir, d)))\n",
    "        model_file = next(f for f in os.listdir(os.path.join(fold_dir, model_subdir))\n",
    "                          if f.endswith(\"mixed_missing.pth\"))\n",
    "        checkpoint = torch.load(\n",
    "            os.path.join(fold_dir, model_subdir, model_file), map_location=\"cpu\"\n",
    "        )\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        dataset = MultimodalCTWSIDatasetSurv(\n",
    "            fold=0,\n",
    "            split=\"train\",\n",
    "            ct_path=ct_path,\n",
    "            wsi_path=wsi_path,\n",
    "            labels_splits_path=test_path,\n",
    "            missing_modality_prob=0.0,\n",
    "            require_both_modalities=False,\n",
    "            pairing_mode=\"one_to_one\",\n",
    "            allow_repeats=True,\n",
    "            pairs_per_patient=None,\n",
    "            missing_modality=\"ct\"\n",
    "        )\n",
    "        print(dataset.modality_stats)\n",
    "        loader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "\n",
    "        # CSV Logger\n",
    "        log_file = os.path.join(output_dir, f\"fold_{fold}_log.csv\")\n",
    "        fold_tensor_dir = os.path.join(output_dir, f\"fold_{fold}\")\n",
    "        os.makedirs(fold_tensor_dir, exist_ok=True)\n",
    "\n",
    "        with open(log_file, \"w\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                \"patient_id\", \"ct_path\", \"wsi_path\",\n",
    "                \"ct_available\", \"wsi_available\",\n",
    "                \"hazard_score\", \"survtime\", \"censor\",\n",
    "                \"grad_norm\", \"curvature\",\n",
    "                \"fused_path\", \"pe_path\", \"hazard_tensor_path\"\n",
    "            ])\n",
    "\n",
    "            sample_counter = 0\n",
    "            for batch in loader:\n",
    "                \n",
    "                # ensure floats\n",
    "                for k, v in batch.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        batch[k] = v.float()\n",
    "\n",
    "                with torch.enable_grad():\n",
    "                    for v in batch.values():\n",
    "                        if isinstance(v, torch.Tensor):\n",
    "                            v.requires_grad = True\n",
    "\n",
    "                    outputs = model(\n",
    "                        batch[\"ct_feature\"],\n",
    "                        batch[\"wsi_feature\"],\n",
    "                        modality_flag=batch[\"modality_mask\"],\n",
    "                        output_layers=[\"hazard\", \"fused_features\", \"positional_embeddings\"]\n",
    "                    )\n",
    "                    hazard = outputs[\"hazard\"].squeeze(1)  # [B]\n",
    "                    \n",
    "                    fused_features = outputs[\"fused_features\"]    # [B, D]\n",
    "                    pos_emb = outputs[\"positional_embeddings\"]    # [B, D]\n",
    "\n",
    "                    grad_norms, curvatures = compute_grad_and_curvature(\n",
    "                        fused_features, hazard,\n",
    "                        batch[\"survtime\"], batch[\"censor\"]\n",
    "                    )\n",
    "\n",
    "                # save per-sample\n",
    "                B = hazard.size(0)\n",
    "                for j in range(B):\n",
    "                    sample_id = f\"sample_{sample_counter:06d}\"\n",
    "                    fused_path = os.path.join(fold_tensor_dir, f\"{sample_id}_fused.npy\")\n",
    "                    pe_path = os.path.join(fold_tensor_dir, f\"{sample_id}_pe.npy\")\n",
    "                    hazard_path = os.path.join(fold_tensor_dir, f\"{sample_id}_hazard.npy\")\n",
    "\n",
    "                    np.save(fused_path, fused_features[j].detach().cpu().numpy())\n",
    "                    np.save(pe_path, pos_emb[j].detach().cpu().numpy())\n",
    "                    np.save(hazard_path, hazard[j].detach().cpu().numpy())\n",
    "\n",
    "                    ct_flag = int(batch[\"modality_mask\"][j, 0].item())\n",
    "                    wsi_flag = int(batch[\"modality_mask\"][j, 1].item())\n",
    "                    #if batch[\"censor\"][j].item()==0: continue\n",
    "                    writer.writerow([\n",
    "                        batch[\"patient_id\"][j],\n",
    "                        batch[\"ct_path\"][j],\n",
    "                        batch[\"wsi_path\"][j],\n",
    "                        ct_flag, wsi_flag,\n",
    "                        hazard[j].item(),\n",
    "                        batch[\"survtime\"][j].item(),\n",
    "                        batch[\"censor\"][j].item(),\n",
    "                        grad_norms[j], curvatures[j],\n",
    "                        fused_path, pe_path, hazard_path\n",
    "                    ])\n",
    "\n",
    "                    sample_counter += 1\n",
    "                    \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "def find_topk_gradient_extremes(\n",
    "    csv_path: str,\n",
    "    k: int = 10,\n",
    "    output_dir: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds top-k highest and lowest |gradient norm| samples from a CSV log.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to fold_X_log.csv generated during eval.\n",
    "        k (int): Number of top and bottom samples to retrieve.\n",
    "        output_dir (str): Optional folder to save CSVs. If None, only prints results.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if \"grad_norm\" not in df.columns:\n",
    "        raise ValueError(\"The CSV does not contain 'grad_norm' column.\")\n",
    "\n",
    "    # Compute absolute gradient norms\n",
    "    df[\"abs_grad_norm\"] = df[\"grad_norm\"].abs()\n",
    "\n",
    "    # Sort by absolute gradient norm\n",
    "    df_sorted = df.sort_values(by=\"abs_grad_norm\", ascending=True)\n",
    "\n",
    "    topk_low = df_sorted.head(k).copy()\n",
    "    topk_high = df_sorted.tail(k).copy()[::-1]  # High to low\n",
    "\n",
    "    print(\"\\n🔹 Top-k LOW |Gradient Norm| Samples:\")\n",
    "    print(topk_low[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    print(\"\\n🔸 Top-k HIGH |Gradient Norm| Samples:\")\n",
    "    print(topk_high[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        low_path = os.path.join(output_dir, \"topk_low_gradient.csv\")\n",
    "        high_path = os.path.join(output_dir, \"topk_high_gradient.csv\")\n",
    "        topk_low.to_csv(low_path, index=False)\n",
    "        topk_high.to_csv(high_path, index=False)\n",
    "        print(f\"\\n📁 Saved to:\\n  {low_path}\\n  {high_path}\")\n",
    "\n",
    "    return topk_low, topk_high\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 0\n",
    "\n",
    "def set_global_seed(seed=SEED):\n",
    "    \"\"\"\n",
    "    Set a global seed for reproducibility across different libraries and random number generators.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed value to be used\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5077d2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n",
      "{'ct_only': 0, 'wsi_only': 196, 'both': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3800671/2227639788.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3800671/769493430.py:388: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "{'ct_only': 0, 'wsi_only': 196, 'both': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3800671/2227639788.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3800671/769493430.py:388: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n",
      "{'ct_only': 0, 'wsi_only': 196, 'both': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3800671/2227639788.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3800671/769493430.py:388: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n",
      "{'ct_only': 0, 'wsi_only': 196, 'both': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3800671/2227639788.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3800671/769493430.py:388: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0    C3L-02125        0.0            0.0             0              1\n",
      "473  C3L-00136        0.0            0.0             0              1\n",
      "474  C3L-00136        0.0            0.0             0              1\n",
      "475  C3L-00136        0.0            0.0             0              1\n",
      "476  C3L-00136        0.0            0.0             0              1\n",
      "477  C3N-01510        0.0            0.0             0              1\n",
      "478  C3N-01510        0.0            0.0             0              1\n",
      "479  C3N-01510        0.0            0.0             0              1\n",
      "480  C3N-01510        0.0            0.0             0              1\n",
      "472  C3L-00157        0.0            0.0             0              1\n",
      "481  C3N-01510        0.0            0.0             0              1\n",
      "483  C3N-01537        0.0            0.0             0              1\n",
      "484  C3N-01537        0.0            0.0             0              1\n",
      "485  C3N-01537        0.0            0.0             0              1\n",
      "486  C3L-01311        0.0            0.0             0              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "717  C3N-00850        0.0            0.0             0              1\n",
      "224  C3L-02411        0.0            0.0             0              1\n",
      "244  C3L-00898        0.0            0.0             0              1\n",
      "243  C3L-00898        0.0            0.0             0              1\n",
      "242  C3L-00898        0.0            0.0             0              1\n",
      "241  C3L-00898        0.0            0.0             0              1\n",
      "240  C3L-00898        0.0            0.0             0              1\n",
      "239  C3L-00898        0.0            0.0             0              1\n",
      "238  C3L-05849        0.0            0.0             0              1\n",
      "237  C3L-05849        0.0            0.0             0              1\n",
      "236  C3L-05849        0.0            0.0             0              1\n",
      "235  C3L-05849        0.0            0.0             0              1\n",
      "234  C3L-02468        0.0            0.0             0              1\n",
      "233  C3L-02468        0.0            0.0             0              1\n",
      "232  C3L-02468        0.0            0.0             0              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed50TEST/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed50TEST/fold_0_topk/topk_high_gradient.csv\n"
     ]
    }
   ],
   "source": [
    "folds_dir = \"./models/ckpts/CPTACUCEC_trainmixed50_multival_Titan_MedImSight_fullepochs\"\n",
    "ct_path = \"../MedImageInsights/embeddings_cptacucec\"\n",
    "wsi_path = \"../../trident_processed_UCEC_titan/20x_512px_0px_overlap/slide_features_titan\"\n",
    "test_path = \"./data/processed/processed_CPTAC_UCEC_test/k=all.tsv\"\n",
    "output_dir = \"./interpretability/interpretability_UCEC_mixed50TEST\"\n",
    "\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16, n_folds=4)\n",
    "for i in range(1):\n",
    "    csv_input = f\"./interpretability/interpretability_UCEC_mixed50TEST/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_UCEC_mixed50TEST/fold_{i}_topk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7bd4ef12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3606097/1337002563.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(fold_dir, model_subdir, model_file), map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0    C3L-04473        0.0            0.0             0              1\n",
      "177  C3L-01662        0.0            0.0             0              1\n",
      "178  C3L-01662        0.0            0.0             0              1\n",
      "179  C3L-01662        0.0            0.0             0              1\n",
      "180  C3L-01662        0.0            0.0             0              1\n",
      "181  C3L-01160        0.0            0.0             0              1\n",
      "182  C3L-01160        0.0            0.0             0              1\n",
      "183  C3L-01160        0.0            0.0             0              1\n",
      "184  C3N-03884        0.0            0.0             0              1\n",
      "185  C3N-03884        0.0            0.0             0              1\n",
      "186  C3N-03884        0.0            0.0             0              1\n",
      "187  C3L-01031        0.0            0.0             0              1\n",
      "188  C3L-01031        0.0            0.0             0              1\n",
      "189  C3L-01031        0.0            0.0             0              1\n",
      "190  C3L-04027        0.0            0.0             0              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "    patient_id     grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "136  C3N-02768  2.288209e-08   2.288209e-08             0              1\n",
      "264  C3N-03853  2.288209e-08   2.288209e-08             0              1\n",
      "208  C3L-02809  2.288209e-08   2.288209e-08             0              1\n",
      "63   C3N-01719  2.288209e-08   2.288209e-08             0              1\n",
      "62   C3N-01719  2.288209e-08   2.288209e-08             0              1\n",
      "220  C3N-03190  2.288209e-08   2.288209e-08             0              1\n",
      "55   C3N-04284  2.288209e-08   2.288209e-08             0              1\n",
      "92   C3N-03439  2.288209e-08   2.288209e-08             0              1\n",
      "35   C3N-03069  2.288209e-08   2.288209e-08             0              1\n",
      "248  C3L-03388  2.288209e-08   2.288209e-08             0              1\n",
      "166  C3L-00102  2.288209e-08   2.288209e-08             0              1\n",
      "162  C3L-01052  2.288209e-08   2.288209e-08             0              1\n",
      "257  C3N-03211  2.288209e-08   2.288209e-08             0              1\n",
      "118  C3L-02604  2.288209e-08   2.288209e-08             0              1\n",
      "105  C3L-02701  2.288209e-08   2.288209e-08             0              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_testmixed50/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_testmixed50/fold_0_topk/topk_high_gradient.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from data.multimodal_features_surv import MultimodalCTWSIDatasetSurv\n",
    "from models.dpe.main_model_nobackbone_surv_new_gcs import MADPENetNoBackbonesSurv\n",
    "\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "class CoxLoss(_WeightedLoss):\n",
    "    def forward(self, hazard_pred: torch.Tensor, survtime: torch.Tensor, censor: torch.Tensor):\n",
    "        censor = censor.float()\n",
    "        n = len(survtime)\n",
    "        R_mat = survtime.reshape((1, n)) >= survtime.reshape((n, 1))\n",
    "        theta = hazard_pred.reshape(-1)\n",
    "        exp_theta = torch.exp(theta)\n",
    "        loss = -torch.mean((theta - torch.log(torch.sum(exp_theta * R_mat, dim=1))) * censor)\n",
    "        return loss\n",
    "\n",
    "\n",
    "cox_loss_fn = CoxLoss()\n",
    "\n",
    "def compute_grad_and_curvature(fused_features, hazard, survtime, censor):\n",
    "    hazard = hazard.view(-1)\n",
    "    loss = cox_loss_fn(hazard, survtime, censor)\n",
    "    grad_f = torch.autograd.grad(loss, fused_features, create_graph=True)[0]\n",
    "    grad_norm = grad_f.flatten(1).norm(p=2, dim=1)\n",
    "\n",
    "    random_vec = grad_f.detach().clone().sign()\n",
    "    grad_dot_random = torch.sum(grad_f * random_vec)\n",
    "    hvp = torch.autograd.grad(grad_dot_random, fused_features, retain_graph=False)[0]\n",
    "    curvature = torch.sum(hvp * random_vec, dim=list(range(1, hvp.ndim)))\n",
    "    \n",
    "    #print(\"grad_f_att:\", grad_f)\n",
    "    #print(\"grad_norm:\", grad_f.flatten(1).norm(p=2, dim=1))\n",
    "\n",
    "    \n",
    "    return grad_norm.detach().cpu().numpy(), curvature.detach().cpu().numpy()   \n",
    "\n",
    "def evaluate_and_log():\n",
    "    folds_dir = \"./models/ckpts/CPTACPDA_trainmixed50_multival_Titan_MedImSight\"\n",
    "    test_path = \"./data/processed/processed_CPTAC_PDA_test/k=all.tsv\"\n",
    "    ct_path = \"../MedImageInsights/embeddings_output_cptacpda_93\"\n",
    "    wsi_path = \"../../TitanCPTACPDA/20x_512px_0px_overlap/slide_features_titan\"\n",
    "    output_dir = \"./interpretability/interpretability_PDA_testmixed50\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for fold in range(1):\n",
    "        print(f\"\\n=== Fold {fold} ===\")\n",
    "        model = MADPENetNoBackbonesSurv(\n",
    "            rad_input_dim=1024, histo_input_dim=768,\n",
    "            inter_dim=256, token_dim=256, dim_hider=256\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        # Load checkpoint\n",
    "        fold_dir = os.path.join(folds_dir, f\"fold_{fold}\")\n",
    "        model_subdir = next(d for d in os.listdir(fold_dir) if os.path.isdir(os.path.join(fold_dir, d)))\n",
    "        model_file = next(f for f in os.listdir(os.path.join(fold_dir, model_subdir)) if f.endswith(\"mixed_missing.pth\"))\n",
    "        checkpoint = torch.load(os.path.join(fold_dir, model_subdir, model_file), map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        dataset = MultimodalCTWSIDatasetSurv(\n",
    "            fold=fold,\n",
    "            split=\"train\",\n",
    "            ct_path=ct_path,\n",
    "            wsi_path=wsi_path,\n",
    "            labels_splits_path=test_path,\n",
    "            missing_modality_prob=0.0,\n",
    "            require_both_modalities=False,\n",
    "            pairing_mode=\"one_to_one\",\n",
    "            allow_repeats=True,\n",
    "            pairs_per_patient=None,\n",
    "            missing_modality=\"wsi\"\n",
    "        )\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "        # CSV Logger\n",
    "        log_file = os.path.join(output_dir, f\"fold_{fold}_log.csv\")\n",
    "        fold_tensor_dir = os.path.join(output_dir, f\"fold_{fold}\")\n",
    "        os.makedirs(fold_tensor_dir, exist_ok=True)\n",
    "\n",
    "        with open(log_file, \"w\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                \"patient_id\", \"ct_path\", \"wsi_path\",\n",
    "                \"ct_available\", \"wsi_available\",\n",
    "                \"hazard_score\", \"survtime\", \"censor\",\n",
    "                \"grad_norm\", \"curvature\",\n",
    "                \"fused_path\", \"pe_path\", \"hazard_tensor_path\"\n",
    "            ])\n",
    "\n",
    "            for i, batch in enumerate(loader):\n",
    "                for k in batch:\n",
    "                    if isinstance(batch[k], torch.Tensor):\n",
    "                        batch[k] = batch[k].float()\n",
    "\n",
    "                with torch.enable_grad():\n",
    "                    for k in batch:\n",
    "                        if isinstance(batch[k], torch.Tensor):\n",
    "                            batch[k].requires_grad = True\n",
    "\n",
    "                    outputs = model(\n",
    "                        batch[\"ct_feature\"],\n",
    "                        batch[\"wsi_feature\"],\n",
    "                        modality_flag=batch[\"modality_mask\"],\n",
    "                        output_layers=[\"hazard\", \"fused_features\", \"positional_embeddings\"]\n",
    "                    )\n",
    "                    fused_features = outputs[\"fused_features\"]\n",
    "                    fused_features.requires_grad_(True)\n",
    "\n",
    "                    # Recompute hazard *outside* the model to ensure it is explicitly tied to fused_features\n",
    "                    hazard = model.hazard_net(fused_features)\n",
    "\n",
    "                    #fused_features = outputs[\"fused_features\"]\n",
    "                    pos_emb = outputs[\"positional_embeddings\"].squeeze()\n",
    "                    #fused_features.requires_grad_(True)\n",
    "                    grad_norm, curvature = compute_grad_and_curvature(\n",
    "                        fused_features, hazard, batch[\"survtime\"], batch[\"censor\"]\n",
    "                    )\n",
    "\n",
    "                # Save tensor files\n",
    "                sample_id = f\"sample_{i:04d}\"\n",
    "                fused_path = os.path.join(fold_tensor_dir, f\"{sample_id}_fused.npy\")\n",
    "                pe_path = os.path.join(fold_tensor_dir, f\"{sample_id}_pe.npy\")\n",
    "                hazard_path = os.path.join(fold_tensor_dir, f\"{sample_id}_hazard.npy\")\n",
    "\n",
    "                np.save(fused_path, fused_features.detach().cpu().numpy())\n",
    "                np.save(pe_path, pos_emb.detach().cpu().numpy())\n",
    "                np.save(hazard_path, hazard.detach().cpu().numpy())\n",
    "\n",
    "                ct_flag = int(batch[\"modality_mask\"][0, 0].item())\n",
    "                wsi_flag = int(batch[\"modality_mask\"][0, 1].item())\n",
    "\n",
    "                writer.writerow([\n",
    "                    batch[\"patient_id\"][0],\n",
    "                    dataset.samples[i][\"ct_path\"],\n",
    "                    dataset.samples[i][\"wsi_feature\"],\n",
    "                    ct_flag, wsi_flag,\n",
    "                    hazard.item(),\n",
    "                    batch[\"survtime\"].item(),\n",
    "                    batch[\"censor\"].item(),\n",
    "                    grad_norm[0],\n",
    "                    curvature[0],\n",
    "                    fused_path,\n",
    "                    pe_path,\n",
    "                    hazard_path\n",
    "                ])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_and_log()\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "def find_topk_gradient_extremes(\n",
    "    csv_path: str,\n",
    "    k: int = 10,\n",
    "    output_dir: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds top-k highest and lowest |gradient norm| samples from a CSV log.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to fold_X_log.csv generated during eval.\n",
    "        k (int): Number of top and bottom samples to retrieve.\n",
    "        output_dir (str): Optional folder to save CSVs. If None, only prints results.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if \"grad_norm\" not in df.columns:\n",
    "        raise ValueError(\"The CSV does not contain 'grad_norm' column.\")\n",
    "\n",
    "    # Compute absolute gradient norms\n",
    "    df[\"abs_grad_norm\"] = df[\"grad_norm\"].abs()\n",
    "\n",
    "    # Sort by absolute gradient norm\n",
    "    df_sorted = df.sort_values(by=\"abs_grad_norm\", ascending=True)\n",
    "\n",
    "    topk_low = df_sorted.head(k).copy()\n",
    "    topk_high = df_sorted.tail(k).copy()[::-1]  # High to low\n",
    "\n",
    "    print(\"\\n🔹 Top-k LOW |Gradient Norm| Samples:\")\n",
    "    print(topk_low[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    print(\"\\n🔸 Top-k HIGH |Gradient Norm| Samples:\")\n",
    "    print(topk_high[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        low_path = os.path.join(output_dir, \"topk_low_gradient.csv\")\n",
    "        high_path = os.path.join(output_dir, \"topk_high_gradient.csv\")\n",
    "        topk_low.to_csv(low_path, index=False)\n",
    "        topk_high.to_csv(high_path, index=False)\n",
    "        print(f\"\\n📁 Saved to:\\n  {low_path}\\n  {high_path}\")\n",
    "\n",
    "    return topk_low, topk_high\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    for i in range(1):\n",
    "        csv_input = f\"./interpretability/interpretability_PDA_testmixed50/fold_{i}_log.csv\"\n",
    "        find_topk_gradient_extremes(\n",
    "            csv_path=csv_input,\n",
    "            k=15,\n",
    "            output_dir=f\"./interpretability/interpretability_PDA_testmixed50/fold_{i}_topk\"\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_biocv_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
