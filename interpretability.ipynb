{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9300ed25",
   "metadata": {},
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799cc1d2",
   "metadata": {},
   "source": [
    "Sul fold migliore vado a fare forward pass e calcolarmi i gradienti, i positional embedding, correlazioni, curvatura e path degli input che danno quei risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc296b",
   "metadata": {},
   "source": [
    "Pensiero random (potrebbe essere carino per la tesi e quasi d’obbligo per il paper), lo lascerei comunque come ultimo step se abbiamo tempo: riusciamo sulla fold che va meglio, del dataset che va meglio per il nostro modello tenere traccia in validation di quali sono le coppie di feature (in modo da rintracciare la coppia rad-histo) che portano gradienti maggiori e minori e salvare le rispettive correlazioni (e calcolare il rischio)? Questo significa rintracciare le coppie la cui correlazione ha un’importanza o meno nel calcolo del rischio.\n",
    "Quello che sarebbe interessante far vedere sono gli estremi opposti su due coppie di rad histo con:\n",
    "Alta correlazione, alto gradiente, alto risk score, basso survival, alto grado di tumore: sulla radiologia calcolare tipo regolarità della forma (che ci i aspetta bassa, usando la segmentazione), grandezza tumore (che ci si aspetta alta, usando la segmentazione) e sulla histo il numero di cellule epiteliali cancerogene (che ci si aspetta alta, cosa che si può fare con una rete pretrainata) \n",
    "Alta correlazione, basso gradiente, basso risk score, alto survival, basso grado di tumore: sulla radiologia calcolare tipo regolarità della forma (che ci i aspetta alta), grandezza tumore (che ci si aspetta bassa, usando la segmentazione) e sulla histo il numero di cellule epiteliali cancerogene (che ci si aspetta bassa, cosa che si può fare con una rete pretrainata) \n",
    "Se tu ti occupi di tracciare i gradienti/correlazioni/file (non dovrebbe essere troppo complesso, basta salvarsi le combinazioni di risultati su un file), io mi occuperei di tutto il resto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e07be3",
   "metadata": {},
   "source": [
    "### Define Functions for interpretability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab473306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from itertools import product\n",
    "#from pathlib import Path\n",
    "\n",
    "#import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "#import torchstain\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "#from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class   MultimodalCTWSIDatasetSurv(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for paired CT and WSI data, handling missing modalities\n",
    "        at both dataset and sampling level\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fold: int,\n",
    "        split: str,  # either \"train\" or \"test\"\n",
    "        ct_path: str,\n",
    "        wsi_path: str,\n",
    "        labels_splits_path: str,\n",
    "        missing_modality_prob: float = 0.0,  # Additional random masking probability\n",
    "        missing_modality: str = \"both\",\n",
    "        require_both_modalities: bool = False,  # Whether to only include patients\n",
    "        # with both modalities\n",
    "        pairing_mode: str = None,  # 'all_combinations, 'one_to_one', 'fixed_count'\n",
    "        pairs_per_patient: int = None,  # For fixed_count_mode\n",
    "        allow_repeats: bool = False,  # For fixed_count mode\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # assert split in [\"train\", \"val\", \"overfit\", \"all\"]\n",
    "        assert split in [\"train\", \"test\"]\n",
    "        assert pairing_mode in [\"all_combinations\", \"one_to_one\", \"fixed_count\"]\n",
    "        assert 0 <= missing_modality_prob <= 1\n",
    "        assert missing_modality in [\"ct\", \"wsi\", \"both\"]\n",
    "        self.missing_modality = missing_modality\n",
    "        self.fold = fold\n",
    "        self.split = split\n",
    "        self.ct_path = ct_path\n",
    "        self.wsi_path = wsi_path\n",
    "        self.missing_modality_prob = missing_modality_prob\n",
    "        self.require_both_modalities = require_both_modalities\n",
    "        self.labels_splits_path = labels_splits_path\n",
    "        self.pairing_mode = (\n",
    "            pairing_mode  # 'all_combinations', 'one_to_one', or 'fixed_count'\n",
    "        )\n",
    "        self.pairs_per_patient = pairs_per_patient  # For fixed_count mode\n",
    "        self.allow_repeats = allow_repeats  # For fixed_count mode\n",
    "        labels_splits = pd.read_csv(labels_splits_path, sep=\"\\t\")\n",
    "        labels_splits = labels_splits[labels_splits[f\"fold_{self.fold}\"] == self.split]\n",
    "        self.labels_splits = labels_splits[\n",
    "            [\"case_id\", \"OS_days\", \"OS_event\"]\n",
    "        ].drop_duplicates(\"case_id\")\n",
    "\n",
    "        # Initialize data structures\n",
    "        # Will store CT and WSI paths per patient\n",
    "        self.patient_data = {}\n",
    "        # Will store all valid combinations\n",
    "        self.samples = []\n",
    "        self.modality_stats = {\"ct_only\": 0, \"wsi_only\": 0, \"both\": 0}\n",
    "\n",
    "        # Load split file\n",
    "        self._load_split()\n",
    "\n",
    "    def _get_max_pairs_for_patient(self, ct_scans, wsi_folders, allow_repeats):\n",
    "        \"\"\"\n",
    "        Calculate maximum possible pairs for a patient based on available data and pairing mode.\n",
    "\n",
    "        Args:\n",
    "            ct_scans (list): List of CT scan files\n",
    "            wsi_folders (list): List of WSI folder names\n",
    "            allow_repeats (bool): If True, return all possible combinations count\n",
    "                                If False, return maximum unique pairs count\n",
    "\n",
    "        Returns:\n",
    "            int: Maximum number of possible pairs\n",
    "        \"\"\"\n",
    "        if not ct_scans or not wsi_folders:\n",
    "            return 0\n",
    "\n",
    "        if allow_repeats:\n",
    "            return len(ct_scans) * len(wsi_folders)\n",
    "        else:  # one-to-one\n",
    "            return min(len(ct_scans), len(wsi_folders))\n",
    "\n",
    "    def _get_fixed_pairs(self, ct_scans, wsi_folders, n_pairs, allow_repeats=True):\n",
    "        \"\"\"\n",
    "        Generate fixed number of pairs between CT scans and WSI folders.\n",
    "\n",
    "        Args:\n",
    "            ct_scans (list): List of CT scan files\n",
    "            wsi_folders (list): List of WSI folder names\n",
    "            n_pairs (int): Number of pairs to generate\n",
    "            allow_repeats (bool): If True, allows repeating elements to reach n_pairs\n",
    "                                If False, limits pairs to minimum unique combinations\n",
    "\n",
    "        Returns:\n",
    "            list: List of (ct_scan, wsi_folder) pairs\n",
    "        \"\"\"\n",
    "        max_unique_pairs = min(len(ct_scans), len(wsi_folders))\n",
    "\n",
    "        if not allow_repeats:\n",
    "            n_pairs = min(n_pairs, max_unique_pairs)\n",
    "\n",
    "        # Generate initial unique pairs\n",
    "        shuffled_ct = ct_scans.copy()\n",
    "        shuffled_wsi = wsi_folders.copy()\n",
    "        random.shuffle(shuffled_ct)\n",
    "        random.shuffle(shuffled_wsi)\n",
    "\n",
    "        pairs = list(\n",
    "            zip(shuffled_ct[:max_unique_pairs], shuffled_wsi[:max_unique_pairs])\n",
    "        )\n",
    "\n",
    "        if n_pairs <= len(pairs):\n",
    "            # Downsample if needed\n",
    "            random.shuffle(pairs)\n",
    "            return pairs[:n_pairs]\n",
    "\n",
    "        if not allow_repeats:\n",
    "            return pairs\n",
    "\n",
    "        # Need to generate additional pairs with repeats\n",
    "        while len(pairs) < n_pairs:\n",
    "            ct_scan = random.choice(ct_scans)\n",
    "            wsi_folder = random.choice(wsi_folders)\n",
    "            pairs.append((ct_scan, wsi_folder))\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def _load_split(self):\n",
    "        \"\"\"Load and organize all CT and WSI data for the given split\n",
    "\n",
    "        Supports different pairing modes:\n",
    "        - 'all_combinations': Creates all possible CT-WSI pairs\n",
    "        - 'one_to_one': Creates random 1:1 pairs\n",
    "        - 'fixed_count': Creates fixed number of pairs per patient\n",
    "        \"\"\"\n",
    "\n",
    "        # First pass: count maximum possible pairs per patient\n",
    "        max_pairs_possible = float(\"inf\")\n",
    "        if self.pairing_mode == \"fixed_count\":\n",
    "            for patient_id in self.labels_splits[\"case_id\"].values:\n",
    "\n",
    "                ct_path = os.path.join(self.ct_path, patient_id)\n",
    "                ct_features = []\n",
    "                if os.path.exists(ct_path):\n",
    "                    ct_features = [f for f in os.listdir(ct_path)]\n",
    "\n",
    "                wsi_path = os.path.join(self.wsi_path)\n",
    "                wsi_features = [\n",
    "                    f\n",
    "                    for f in os.listdir(wsi_path)\n",
    "                    if patient_id in f and os.path.isdir(os.path.join(wsi_path, f))\n",
    "                ]\n",
    "\n",
    "                patient_max_pairs = self._get_max_pairs_for_patient(\n",
    "                    ct_features, wsi_features, self.allow_repeats\n",
    "                )\n",
    "\n",
    "                if patient_max_pairs > 0:  # Only update if patient has both modalities\n",
    "\n",
    "                    max_pairs_possible = min(max_pairs_possible, patient_max_pairs)\n",
    "\n",
    "        # Use provided pairs_per_patient or calculated maximum\n",
    "        n_pairs = (\n",
    "            self.pairs_per_patient\n",
    "            if self.pairs_per_patient is not None\n",
    "            else max_pairs_possible\n",
    "        )\n",
    "\n",
    "        # Main loading loop\n",
    "        for row in self.labels_splits[\"case_id\"].values:\n",
    "            patient_id = row.strip()\n",
    "            # Find all CT scans for this patient\n",
    "            ct_path = os.path.join(self.ct_path, patient_id)\n",
    "            ct_features = []\n",
    "            if os.path.exists(ct_path):\n",
    "                ct_features = [f for f in os.listdir(ct_path)]\n",
    "\n",
    "            # Find all WSI .h5 files for this patient\n",
    "            wsi_path = self.wsi_path\n",
    "            wsi_features = [f for f in os.listdir(wsi_path) if patient_id in f]\n",
    "\n",
    "            # Skip patient if we require both modalities and they don't have them\n",
    "            if self.require_both_modalities and (not ct_features or not wsi_features):\n",
    "                continue\n",
    "\n",
    "            # Store available data for this patient\n",
    "            self.patient_data[patient_id] = {\n",
    "                \"ct_features\": ct_features,\n",
    "                \"wsi_features\": wsi_features,\n",
    "            }\n",
    "\n",
    "            # Update modality statistics\n",
    "            if ct_features and wsi_features:\n",
    "                self.modality_stats[\"both\"] += 1\n",
    "            elif ct_features:\n",
    "                self.modality_stats[\"ct_only\"] += 1\n",
    "            else:\n",
    "                self.modality_stats[\"wsi_only\"] += 1\n",
    "\n",
    "            cnt = 0\n",
    "            # Generate samples based on available data and pairing mode\n",
    "            if ct_features and wsi_features:\n",
    "                if self.pairing_mode == \"fixed_count\":\n",
    "                    # Generate fixed number of pairs\n",
    "                    pairs = self._get_fixed_pairs(\n",
    "                        ct_features, wsi_features, n_pairs, self.allow_repeats\n",
    "                    )\n",
    "\n",
    "                    for ct_feature, wsi_feature in pairs:\n",
    "                        cnt += 1\n",
    "                        self.samples.append(\n",
    "                            {\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"ct_path\": os.path.join(\n",
    "                                    self.ct_path, patient_id, ct_feature\n",
    "                                ),\n",
    "                                \"wsi_folder\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                                \"base_modality_mask\": [1, 1],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                elif self.pairing_mode == \"one_to_one\":\n",
    "                    # Original one_to_one logic\n",
    "                    num_pairs = min(len(ct_features), len(wsi_features))\n",
    "                    shuffled_ct = ct_features.copy()\n",
    "                    shuffled_wsi = wsi_features.copy()\n",
    "                    random.shuffle(shuffled_ct)\n",
    "                    random.shuffle(shuffled_wsi)\n",
    "\n",
    "                    for ct_feature, wsi_feature in zip(\n",
    "                        shuffled_ct[:num_pairs], shuffled_wsi[:num_pairs]\n",
    "                    ):\n",
    "                        cnt += 1\n",
    "                        self.samples.append(\n",
    "                            {\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"ct_path\": os.path.join(\n",
    "                                    self.ct_path, patient_id, ct_feature\n",
    "                                ),\n",
    "                                \"wsi_feature\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                                \"base_modality_mask\": [1, 1],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                else:  # 'all_combinations' mode\n",
    "                    for ct_feature, wsi_feature in product(ct_features, wsi_features):\n",
    "                        cnt += 1\n",
    "                        self.samples.append(\n",
    "                            {\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"ct_path\": os.path.join(\n",
    "                                    self.ct_path, patient_id, ct_feature\n",
    "                                ),\n",
    "                                \"wsi_feature\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                                \"base_modality_mask\": [1, 1],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            elif ct_features:\n",
    "                for ct_feature in ct_features:\n",
    "                    cnt += 1\n",
    "                    self.samples.append(\n",
    "                        {\n",
    "                            \"patient_id\": patient_id,\n",
    "                            \"ct_path\": os.path.join(\n",
    "                                self.ct_path, patient_id, ct_feature\n",
    "                            ),\n",
    "                            \"wsi_feature\": None,\n",
    "                            \"base_modality_mask\": [1, 0],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            elif wsi_features:\n",
    "                for wsi_feature in wsi_features:\n",
    "                    cnt += 1\n",
    "                    self.samples.append(\n",
    "                        {\n",
    "                            \"patient_id\": patient_id,\n",
    "                            \"ct_path\": None,\n",
    "                            \"wsi_feature\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                            \"base_modality_mask\": [0, 1],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    def _load_ct_feature(self, ct_path):\n",
    "        \"\"\"Load and standardize CT feature\"\"\"\n",
    "        if ct_path.endswith(\".pt\"):\n",
    "            volume = np.array(torch.load(ct_path, weights_only=True))\n",
    "        elif ct_path.endswith(\".npy\"):\n",
    "            volume = np.load(ct_path)\n",
    "        return volume\n",
    "\n",
    "    def _load_wsi_feature(self, wsi_path):\n",
    "        \"\"\"Load WSI feature\"\"\"\n",
    "        feature = None\n",
    "        with h5py.File(wsi_path, \"r\") as f:\n",
    "            feature = np.array(f[\"features\"][:])\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def _get_empty_ct_feature(self):\n",
    "        \"\"\"Return empty CT feature of correct shape\"\"\"\n",
    "        return np.zeros((66, 1024))\n",
    "\n",
    "    def _get_empty_wsi_feature(self):\n",
    "        \"\"\"Return empty WSI feature of correct shape\"\"\"\n",
    "        return np.zeros((768,))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'patient_id': str,\n",
    "                'ct_feature': numpy array or zeros if missing,\n",
    "                'wsi_feature': tensor or zeros if missing,\n",
    "                'label': int,\n",
    "                'modality_mask': tensor indicating present modalities [CT, WSI],\n",
    "                'base_modality_mask': tensor indicating modalities available in dataset\n",
    "            }\n",
    "        \"\"\"\n",
    "\n",
    "        sample = self.samples[index]\n",
    "        patient_id = sample[\"patient_id\"]\n",
    "        base_mask = sample[\"base_modality_mask\"]\n",
    "        ct_path = sample[\"ct_path\"]\n",
    "        wsi_path = sample[\"wsi_feature\"]\n",
    "        if wsi_path is None: wsi_path = \"\"\n",
    "        if ct_path is None: ct_path = \"\"\n",
    "        # Apply additional random masking only to available modalities\n",
    "        final_mask = base_mask.copy()\n",
    "        if self.missing_modality_prob > 0:\n",
    "            if self.missing_modality == \"both\":\n",
    "                for i in range(2):\n",
    "                    if (\n",
    "                        base_mask[i] == 1\n",
    "                        and random.random() < self.missing_modality_prob\n",
    "                    ):\n",
    "                        final_mask[i] = 0\n",
    "            elif self.missing_modality == \"ct\":\n",
    "                if base_mask[0] == 1 and random.random() < self.missing_modality_prob:\n",
    "                    final_mask[0] = 0\n",
    "            elif self.missing_modality == \"wsi\":\n",
    "                if base_mask[1] == 1 and random.random() < self.missing_modality_prob:\n",
    "                    final_mask[1] = 0\n",
    "\n",
    "            # Ensure at least one modality remains if it was originally available\n",
    "            if sum(final_mask) == 0 and sum(base_mask) > 0:\n",
    "                # Randomly choose one of the originally available modalities\n",
    "                available_indices = [i for i in range(2) if base_mask[i] == 1]\n",
    "                chosen_idx = random.choice(available_indices)\n",
    "                final_mask[chosen_idx] = 1\n",
    "\n",
    "        # Load features based on final mask\n",
    "        ct_feature = (\n",
    "            self._load_ct_feature(sample[\"ct_path\"])\n",
    "            if final_mask[0] and sample[\"ct_path\"]\n",
    "            else self._get_empty_ct_feature()\n",
    "        )\n",
    "\n",
    "        wsi_feature = (\n",
    "            self._load_wsi_feature(sample[\"wsi_feature\"])\n",
    "            if final_mask[1] and sample[\"wsi_feature\"]\n",
    "            else self._get_empty_wsi_feature()\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"patient_id\": patient_id,\n",
    "            \"ct_feature\": torch.from_numpy(ct_feature).float(),\n",
    "            \"wsi_feature\": torch.from_numpy(wsi_feature).float(),\n",
    "            \"survtime\": torch.tensor(\n",
    "                self.labels_splits[self.labels_splits[\"case_id\"] == patient_id][\n",
    "                    \"OS_days\"\n",
    "                ].iloc[0],\n",
    "                dtype=torch.long,\n",
    "            ),\n",
    "            \"censor\": ~torch.tensor(\n",
    "                self.labels_splits[self.labels_splits[\"case_id\"] == patient_id][\n",
    "                    \"OS_event\"\n",
    "                ].iloc[0],\n",
    "                dtype=torch.bool,\n",
    "            ),\n",
    "            \"modality_mask\": torch.tensor(final_mask, dtype=torch.float32),\n",
    "            \"base_modality_mask\": torch.tensor(base_mask, dtype=torch.float32),\n",
    "            \"ct_path\": ct_path,\n",
    "            \"wsi_path\": wsi_path\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def stats(self):\n",
    "        \"\"\"Return dataset statistics\"\"\"\n",
    "        return {\n",
    "            \"total_samples\": len(self.samples),\n",
    "            \"total_patients\": len(self.patient_data),\n",
    "            \"modality_availability\": self.modality_stats,\n",
    "            \"missing_modality_prob\": self.missing_modality_prob,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def move_batch_to_device(batch, device):\n",
    "        \"\"\"Move all elements of the batch to device\"\"\"\n",
    "        batch[\"ct_feature\"] = batch[\"ct_feature\"].to(device)\n",
    "        batch[\"wsi_feature\"] = batch[\"wsi_feature\"].to(device)\n",
    "        batch[\"survtime\"] = batch[\"survtime\"].to(device)\n",
    "        batch[\"censor\"] = batch[\"censor\"].to(device)\n",
    "        batch[\"modality_mask\"] = batch[\"modality_mask\"].to(device)\n",
    "        batch[\"ct_path\"] = batch[\"ct_path\"].to(device)\n",
    "        batch[\"wsi_path\"] = batch[\"wsi_path\"].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eea5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from data.multimodal_features_surv import MultimodalCTWSIDatasetSurv\n",
    "from models.dpe.main_model_nobackbone_surv_new_gcs import MADPENetNoBackbonesSurv\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "\n",
    "class CoxLoss(_WeightedLoss):\n",
    "    def forward(self, hazard_pred: torch.Tensor, survtime: torch.Tensor, censor: torch.Tensor):\n",
    "        censor = censor.float()\n",
    "        n = len(survtime)\n",
    "        # risk‑set matrix\n",
    "        R_mat = survtime.reshape((1, n)) >= survtime.reshape((n, 1))\n",
    "        theta = hazard_pred.reshape(-1)\n",
    "        exp_theta = torch.exp(theta)\n",
    "        # negative log‑partial likelihood\n",
    "        loss = -torch.mean((theta - torch.log(torch.sum(exp_theta * R_mat, dim=1))) * censor)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def compute_grad_and_curvature(fused_features, hazard, survtime, censor):\n",
    "    hazard = hazard.view(-1)\n",
    "    loss = CoxLoss()(hazard, survtime, censor)\n",
    "    # first derivative\n",
    "    grad_f = torch.autograd.grad(loss, fused_features, create_graph=True)[0]\n",
    "    grad_norm = grad_f.flatten(1).norm(p=2, dim=1)\n",
    "\n",
    "    # Hutchinson estimator for Hessian diagonal\n",
    "    random_vec = grad_f.detach().clone().sign()\n",
    "    grad_dot_random = torch.sum(grad_f * random_vec)\n",
    "    hvp = torch.autograd.grad(grad_dot_random, fused_features, retain_graph=False)[0]\n",
    "    curvature = torch.sum(hvp * random_vec, dim=list(range(1, hvp.ndim)))\n",
    "\n",
    "    return grad_norm.detach().cpu().numpy(), curvature.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def evaluate_and_log(folds_dir, ct_path, wsi_path, test_path, output_dir, batch_size=16, n_folds=5):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        print(f\"\\n=== Fold {fold} ===\")\n",
    "        model = MADPENetNoBackbonesSurv(\n",
    "            rad_input_dim=1024, histo_input_dim=768,\n",
    "            inter_dim=256, token_dim=256, dim_hider=256\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        # Load checkpoint\n",
    "        fold_dir = os.path.join(folds_dir, f\"fold_{fold}\")\n",
    "        model_subdir = next(d for d in os.listdir(fold_dir)\n",
    "                            if os.path.isdir(os.path.join(fold_dir, d)))\n",
    "        model_file = next(f for f in os.listdir(os.path.join(fold_dir, model_subdir))\n",
    "                          if f.endswith(\"mixed_missing.pth\"))\n",
    "        checkpoint = torch.load(\n",
    "            os.path.join(fold_dir, model_subdir, model_file), map_location=\"cpu\"\n",
    "        )\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        dataset = MultimodalCTWSIDatasetSurv(\n",
    "            fold=fold,\n",
    "            split=\"test\",\n",
    "            ct_path=ct_path,\n",
    "            wsi_path=wsi_path,\n",
    "            labels_splits_path=test_path,\n",
    "            missing_modality_prob=0.0,\n",
    "            require_both_modalities=True,\n",
    "            pairing_mode=\"one_to_one\",\n",
    "            allow_repeats=True,\n",
    "            pairs_per_patient=None,\n",
    "            missing_modality=\"wsi\"\n",
    "        )\n",
    "\n",
    "        loader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "\n",
    "        # CSV Logger\n",
    "        log_file = os.path.join(output_dir, f\"fold_{fold}_log.csv\")\n",
    "        fold_tensor_dir = os.path.join(output_dir, f\"fold_{fold}\")\n",
    "        os.makedirs(fold_tensor_dir, exist_ok=True)\n",
    "\n",
    "        with open(log_file, \"w\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                \"patient_id\", \"ct_path\", \"wsi_path\",\n",
    "                \"ct_available\", \"wsi_available\",\n",
    "                \"hazard_score\", \"survtime\", \"censor\",\n",
    "                \"grad_norm\", \"curvature\",\n",
    "                \"fused_path\", \"pe_path\", \"hazard_tensor_path\"\n",
    "            ])\n",
    "\n",
    "            sample_counter = 0\n",
    "            for batch in loader:\n",
    "                # ensure floats\n",
    "                for k, v in batch.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        batch[k] = v.float()\n",
    "\n",
    "                with torch.enable_grad():\n",
    "                    for v in batch.values():\n",
    "                        if isinstance(v, torch.Tensor):\n",
    "                            v.requires_grad = True\n",
    "\n",
    "                    outputs = model(\n",
    "                        batch[\"ct_feature\"],\n",
    "                        batch[\"wsi_feature\"],\n",
    "                        modality_flag=batch[\"modality_mask\"],\n",
    "                        output_layers=[\"hazard\", \"fused_features\", \"positional_embeddings\"]\n",
    "                    )\n",
    "                    hazard = outputs[\"hazard\"].squeeze(1)  # [B]\n",
    "                    fused_features = outputs[\"fused_features\"]    # [B, D]\n",
    "                    pos_emb = outputs[\"positional_embeddings\"]    # [B, D]\n",
    "\n",
    "                    grad_norms, curvatures = compute_grad_and_curvature(\n",
    "                        fused_features, hazard,\n",
    "                        batch[\"survtime\"], batch[\"censor\"]\n",
    "                    )\n",
    "\n",
    "                # save per-sample\n",
    "                B = hazard.size(0)\n",
    "                for j in range(B):\n",
    "                    sample_id = f\"sample_{sample_counter:06d}\"\n",
    "                    fused_path = os.path.join(fold_tensor_dir, f\"{sample_id}_fused.npy\")\n",
    "                    pe_path = os.path.join(fold_tensor_dir, f\"{sample_id}_pe.npy\")\n",
    "                    hazard_path = os.path.join(fold_tensor_dir, f\"{sample_id}_hazard.npy\")\n",
    "\n",
    "                    np.save(fused_path, fused_features[j].detach().cpu().numpy())\n",
    "                    np.save(pe_path, pos_emb[j].detach().cpu().numpy())\n",
    "                    np.save(hazard_path, hazard[j].detach().cpu().numpy())\n",
    "\n",
    "                    ct_flag = int(batch[\"modality_mask\"][j, 0].item())\n",
    "                    wsi_flag = int(batch[\"modality_mask\"][j, 1].item())\n",
    "                    if batch[\"censor\"][j].item()==0: continue\n",
    "                    writer.writerow([\n",
    "                        batch[\"patient_id\"][j],\n",
    "                        batch[\"ct_path\"][j],\n",
    "                        batch[\"wsi_path\"][j],\n",
    "                        ct_flag, wsi_flag,\n",
    "                        hazard[j].item(),\n",
    "                        batch[\"survtime\"][j].item(),\n",
    "                        batch[\"censor\"][j].item(),\n",
    "                        grad_norms[j], curvatures[j],\n",
    "                        fused_path, pe_path, hazard_path\n",
    "                    ])\n",
    "\n",
    "                    sample_counter += 1\n",
    "                    \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "def find_topk_gradient_extremes(\n",
    "    csv_path: str,\n",
    "    k: int = 10,\n",
    "    output_dir: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds top-k highest and lowest |gradient norm| samples from a CSV log.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to fold_X_log.csv generated during eval.\n",
    "        k (int): Number of top and bottom samples to retrieve.\n",
    "        output_dir (str): Optional folder to save CSVs. If None, only prints results.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if \"grad_norm\" not in df.columns:\n",
    "        raise ValueError(\"The CSV does not contain 'grad_norm' column.\")\n",
    "\n",
    "    # Compute absolute gradient norms\n",
    "    df[\"abs_grad_norm\"] = df[\"grad_norm\"].abs()\n",
    "\n",
    "    # Sort by absolute gradient norm\n",
    "    df_sorted = df.sort_values(by=\"abs_grad_norm\", ascending=True)\n",
    "\n",
    "    topk_low = df_sorted.head(k).copy()\n",
    "    topk_high = df_sorted.tail(k).copy()[::-1]  # High to low\n",
    "\n",
    "    print(\"\\n🔹 Top-k LOW |Gradient Norm| Samples:\")\n",
    "    print(topk_low[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    print(\"\\n🔸 Top-k HIGH |Gradient Norm| Samples:\")\n",
    "    print(topk_high[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        low_path = os.path.join(output_dir, \"topk_low_gradient.csv\")\n",
    "        high_path = os.path.join(output_dir, \"topk_high_gradient.csv\")\n",
    "        topk_low.to_csv(low_path, index=False)\n",
    "        topk_high.to_csv(high_path, index=False)\n",
    "        print(f\"\\n📁 Saved to:\\n  {low_path}\\n  {high_path}\")\n",
    "\n",
    "    return topk_low, topk_high\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    # Example usage\n",
    "#    for i in range(4):\n",
    "#        csv_input = f\"./interpretability/interpretability_UCEC_mixed50/fold_{i}_log.csv\"\n",
    "#        find_topk_gradient_extremes(\n",
    "#            csv_path=csv_input,\n",
    "#            k=15,\n",
    "#            output_dir=f\"./interpretability/interpretability_UCEC_mixed50/fold_{i}_topk\"\n",
    "#        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246042e8",
   "metadata": {},
   "source": [
    "## SET GLOBAL SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c41f464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 0\n",
    "\n",
    "def set_global_seed(seed=SEED):\n",
    "    \"\"\"\n",
    "    Set a global seed for reproducibility across different libraries and random number generators.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed value to be used\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e068e",
   "metadata": {},
   "source": [
    "## CPTAC-PDA train-mixed 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "273a21a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n",
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n",
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4 ===\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-04479   0.015239       0.015239             1              1\n",
      "0  C3L-04479   0.018554       0.018554             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-04479   0.018554       0.018554             1              1\n",
      "1  C3L-04479   0.015239       0.015239             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-02010   0.000000       0.000000             1              1\n",
      "3  C3N-02010   0.000000       0.000000             1              1\n",
      "0  C3L-03356   0.011778       0.011778             1              1\n",
      "1  C3N-01012   0.022394       0.022394             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-01012   0.022394       0.022394             1              1\n",
      "0  C3L-03356   0.011778       0.011778             1              1\n",
      "3  C3N-02010   0.000000       0.000000             1              1\n",
      "2  C3N-02010   0.000000       0.000000             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "6  C3N-00198   0.000003       0.000003             1              1\n",
      "4  C3N-00198   0.000008       0.000008             1              1\n",
      "5  C3N-00198   0.000012       0.000012             1              1\n",
      "2  C3N-00198   0.000931       0.000931             1              1\n",
      "3  C3N-00198   0.001015       0.001015             1              1\n",
      "1  C3L-02109   0.005727       0.005727             1              1\n",
      "0  C3L-02109   0.005733       0.005733             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-02109   0.005733       0.005733             1              1\n",
      "1  C3L-02109   0.005727       0.005727             1              1\n",
      "3  C3N-00198   0.001015       0.001015             1              1\n",
      "2  C3N-00198   0.000931       0.000931             1              1\n",
      "5  C3N-00198   0.000012       0.000012             1              1\n",
      "4  C3N-00198   0.000008       0.000008             1              1\n",
      "6  C3N-00198   0.000003       0.000003             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-03123   0.001971       0.001971             1              1\n",
      "0  C3L-03123   0.004932       0.004932             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-03123   0.004932       0.004932             1              1\n",
      "1  C3L-03123   0.001971       0.001971             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_3_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_4_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_pda_mixed50/fold_4_topk/topk_high_gradient.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    }
   ],
   "source": [
    "folds_dir = \"./models/ckpts/CPTACPDA_trainmixed50_multival_Titan_MedImSight_redone\"\n",
    "test_path = \"./data/processed/processed_CPTAC_PDA_survival/k=all.tsv\"\n",
    "ct_path = \"../MedImageInsights/embeddings_output_cptacpda_93\"\n",
    "wsi_path = \"../../TitanCPTACPDA/20x_512px_0px_overlap/slide_features_titan\"\n",
    "output_dir = \"./interpretability/interpretability_pda_mixed50\"\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16)\n",
    "for i in range(5):\n",
    "    csv_input = f\"./interpretability/interpretability_pda_mixed50/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=10,\n",
    "        output_dir=f\"./interpretability/interpretability_pda_mixed50/fold_{i}_topk\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953750f",
   "metadata": {},
   "source": [
    "## CPTAC-PDA train-mixed 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0489a9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-04479   0.000053       0.000053             1              1\n",
      "1  C3L-04479   0.137801       0.137801             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-04479   0.137801       0.137801             1              1\n",
      "0  C3L-04479   0.000053       0.000053             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-01012   0.002529       0.002529             1              1\n",
      "2  C3N-02010   0.009107       0.009107             1              1\n",
      "3  C3N-02010   0.009224       0.009224             1              1\n",
      "0  C3L-03356   0.011109       0.011109             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-03356   0.011109       0.011109             1              1\n",
      "3  C3N-02010   0.009224       0.009224             1              1\n",
      "2  C3N-02010   0.009107       0.009107             1              1\n",
      "1  C3N-01012   0.002529       0.002529             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-00198   0.003446       0.003446             1              1\n",
      "3  C3N-00198   0.003934       0.003934             1              1\n",
      "0  C3L-02109   0.007536       0.007536             1              1\n",
      "1  C3L-02109   0.024497       0.024497             1              1\n",
      "5  C3N-00198   0.030822       0.030822             1              1\n",
      "4  C3N-00198   0.034195       0.034195             1              1\n",
      "6  C3N-00198   0.064661       0.064661             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "6  C3N-00198   0.064661       0.064661             1              1\n",
      "4  C3N-00198   0.034195       0.034195             1              1\n",
      "5  C3N-00198   0.030822       0.030822             1              1\n",
      "1  C3L-02109   0.024497       0.024497             1              1\n",
      "0  C3L-02109   0.007536       0.007536             1              1\n",
      "3  C3N-00198   0.003934       0.003934             1              1\n",
      "2  C3N-00198   0.003446       0.003446             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-03123   0.016339       0.016339             1              1\n",
      "1  C3L-03123   0.020274       0.020274             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-03123   0.020274       0.020274             1              1\n",
      "0  C3L-03123   0.016339       0.016339             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_3_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_4_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed30/fold_4_topk/topk_high_gradient.csv\n"
     ]
    }
   ],
   "source": [
    "folds_dir = \"./models/ckpts/CPTACPDA_trainmixed30_multival_Titan_MedImSight\"\n",
    "test_path = \"./data/processed/processed_CPTAC_PDA_survival/k=all.tsv\"\n",
    "ct_path = \"../MedImageInsights/embeddings_output_cptacpda_93\"\n",
    "wsi_path = \"../../TitanCPTACPDA/20x_512px_0px_overlap/slide_features_titan\"\n",
    "output_dir = \"./interpretability/interpretability_PDA_mixed30\"\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16)\n",
    "# Example usage\n",
    "for i in range(5):\n",
    "    csv_input = f\"./interpretability/interpretability_PDA_mixed30/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_PDA_mixed30/fold_{i}_topk\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f00b78",
   "metadata": {},
   "source": [
    "## CPTAC-PDA Mixed 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b71e2204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-04479   0.002971       0.002971             1              1\n",
      "0  C3L-04479   0.048582       0.048582             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-04479   0.048582       0.048582             1              1\n",
      "1  C3L-04479   0.002971       0.002971             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-03356        0.0            0.0             1              1\n",
      "1  C3N-01012        0.0            0.0             1              1\n",
      "2  C3N-02010        0.0            0.0             1              1\n",
      "3  C3N-02010        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "3  C3N-02010        0.0            0.0             1              1\n",
      "2  C3N-02010        0.0            0.0             1              1\n",
      "1  C3N-01012        0.0            0.0             1              1\n",
      "0  C3L-03356        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-00198   0.000417       0.000417             1              1\n",
      "3  C3N-00198   0.000622       0.000622             1              1\n",
      "5  C3N-00198   0.000835       0.000835             1              1\n",
      "6  C3N-00198   0.003650       0.003650             1              1\n",
      "4  C3N-00198   0.004358       0.004358             1              1\n",
      "0  C3L-02109   0.032183       0.032183             1              1\n",
      "1  C3L-02109   0.032189       0.032189             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-02109   0.032189       0.032189             1              1\n",
      "0  C3L-02109   0.032183       0.032183             1              1\n",
      "4  C3N-00198   0.004358       0.004358             1              1\n",
      "6  C3N-00198   0.003650       0.003650             1              1\n",
      "5  C3N-00198   0.000835       0.000835             1              1\n",
      "3  C3N-00198   0.000622       0.000622             1              1\n",
      "2  C3N-00198   0.000417       0.000417             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-03123        0.0            0.0             1              1\n",
      "1  C3L-03123        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-03123        0.0            0.0             1              1\n",
      "0  C3L-03123        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_3_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_4_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed15/fold_4_topk/topk_high_gradient.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds_dir = \"./models/ckpts/CPTACPDA_trainmixed15_multival_Titan_MedImSight\"\n",
    "test_path = \"./data/processed/processed_CPTAC_PDA_survival/k=all.tsv\"\n",
    "ct_path = \"../MedImageInsights/embeddings_output_cptacpda_93\"\n",
    "wsi_path = \"../../TitanCPTACPDA/20x_512px_0px_overlap/slide_features_titan\"\n",
    "output_dir = \"./interpretability/interpretability_PDA_mixed15\"\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16)\n",
    "for i in range(5):\n",
    "    csv_input = f\"./interpretability/interpretability_PDA_mixed15/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_PDA_mixed15/fold_{i}_topk\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece50f2e",
   "metadata": {},
   "source": [
    "## CPTAC PDA mixed 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efda33e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-04479   0.008891       0.008891             1              1\n",
      "0  C3L-04479   0.017203       0.017203             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-04479   0.017203       0.017203             1              1\n",
      "1  C3L-04479   0.008891       0.008891             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-01012   0.004764       0.004764             1              1\n",
      "0  C3L-03356   0.010079       0.010079             1              1\n",
      "3  C3N-02010   0.025140       0.025140             1              1\n",
      "2  C3N-02010   0.025422       0.025422             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-02010   0.025422       0.025422             1              1\n",
      "3  C3N-02010   0.025140       0.025140             1              1\n",
      "0  C3L-03356   0.010079       0.010079             1              1\n",
      "1  C3N-01012   0.004764       0.004764             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "4  C3N-00198   0.000409       0.000409             1              1\n",
      "5  C3N-00198   0.003613       0.003613             1              1\n",
      "6  C3N-00198   0.003953       0.003953             1              1\n",
      "2  C3N-00198   0.004511       0.004511             1              1\n",
      "3  C3N-00198   0.005625       0.005625             1              1\n",
      "1  C3L-02109   0.016683       0.016683             1              1\n",
      "0  C3L-02109   0.029697       0.029697             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-02109   0.029697       0.029697             1              1\n",
      "1  C3L-02109   0.016683       0.016683             1              1\n",
      "3  C3N-00198   0.005625       0.005625             1              1\n",
      "2  C3N-00198   0.004511       0.004511             1              1\n",
      "6  C3N-00198   0.003953       0.003953             1              1\n",
      "5  C3N-00198   0.003613       0.003613             1              1\n",
      "4  C3N-00198   0.000409       0.000409             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3L-03123        0.0            0.0             1              1\n",
      "1  C3L-03123        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3L-03123        0.0            0.0             1              1\n",
      "0  C3L-03123        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_3_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "Empty DataFrame\n",
      "Columns: [patient_id, grad_norm, abs_grad_norm, ct_available, wsi_available]\n",
      "Index: []\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_4_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed5/fold_4_topk/topk_high_gradient.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds_dir = \"./models/ckpts/CPTACPDA_trainmixed5_multival_Titan_MedImSight\"\n",
    "test_path = \"./data/processed/processed_CPTAC_PDA_survival/k=all.tsv\"\n",
    "ct_path = \"../MedImageInsights/embeddings_output_cptacpda_93\"\n",
    "wsi_path = \"../../TitanCPTACPDA/20x_512px_0px_overlap/slide_features_titan\"\n",
    "output_dir = \"./interpretability/interpretability_PDA_mixed5\"\n",
    "\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16)\n",
    "for i in range(5):\n",
    "    csv_input = f\"./interpretability/interpretability_PDA_mixed5/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_PDA_mixed5/fold_{i}_topk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41616de2",
   "metadata": {},
   "source": [
    "## CPTAC UCEC 5% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "116b94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from itertools import product\n",
    "#from pathlib import Path\n",
    "\n",
    "#import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "#import torchstain\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "#from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class   MultimodalCTWSIDatasetSurv(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for paired CT and WSI data, handling missing modalities\n",
    "        at both dataset and sampling level\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fold: int,\n",
    "        split: str,  # either \"train\" or \"test\"\n",
    "        ct_path: str,\n",
    "        wsi_path: str,\n",
    "        labels_splits_path: str,\n",
    "        missing_modality_prob: float = 0.0,  # Additional random masking probability\n",
    "        missing_modality: str = \"both\",\n",
    "        require_both_modalities: bool = False,  # Whether to only include patients\n",
    "        # with both modalities\n",
    "        pairing_mode: str = None,  # 'all_combinations, 'one_to_one', 'fixed_count'\n",
    "        pairs_per_patient: int = None,  # For fixed_count_mode\n",
    "        allow_repeats: bool = False,  # For fixed_count mode\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # assert split in [\"train\", \"val\", \"overfit\", \"all\"]\n",
    "        assert split in [\"train\", \"test\"]\n",
    "        assert pairing_mode in [\"all_combinations\", \"one_to_one\", \"fixed_count\"]\n",
    "        assert 0 <= missing_modality_prob <= 1\n",
    "        assert missing_modality in [\"ct\", \"wsi\", \"both\"]\n",
    "        self.missing_modality = missing_modality\n",
    "        self.fold = fold\n",
    "        self.split = split\n",
    "        self.ct_path = ct_path\n",
    "        self.wsi_path = wsi_path\n",
    "        self.missing_modality_prob = missing_modality_prob\n",
    "        self.require_both_modalities = require_both_modalities\n",
    "        self.labels_splits_path = labels_splits_path\n",
    "        self.pairing_mode = (\n",
    "            pairing_mode  # 'all_combinations', 'one_to_one', or 'fixed_count'\n",
    "        )\n",
    "        self.pairs_per_patient = pairs_per_patient  # For fixed_count mode\n",
    "        self.allow_repeats = allow_repeats  # For fixed_count mode\n",
    "        labels_splits = pd.read_csv(labels_splits_path, sep=\"\\t\")\n",
    "        labels_splits = labels_splits[labels_splits[f\"fold_{self.fold}\"] == self.split]\n",
    "        self.labels_splits = labels_splits[\n",
    "            [\"case_id\", \"OS_days\", \"OS_event\"]\n",
    "        ].drop_duplicates(\"case_id\")\n",
    "\n",
    "        # Initialize data structures\n",
    "        # Will store CT and WSI paths per patient\n",
    "        self.patient_data = {}\n",
    "        # Will store all valid combinations\n",
    "        self.samples = []\n",
    "        self.modality_stats = {\"ct_only\": 0, \"wsi_only\": 0, \"both\": 0}\n",
    "\n",
    "        # Load split file\n",
    "        self._load_split()\n",
    "\n",
    "    def _get_max_pairs_for_patient(self, ct_scans, wsi_folders, allow_repeats):\n",
    "        \"\"\"\n",
    "        Calculate maximum possible pairs for a patient based on available data and pairing mode.\n",
    "\n",
    "        Args:\n",
    "            ct_scans (list): List of CT scan files\n",
    "            wsi_folders (list): List of WSI folder names\n",
    "            allow_repeats (bool): If True, return all possible combinations count\n",
    "                                If False, return maximum unique pairs count\n",
    "\n",
    "        Returns:\n",
    "            int: Maximum number of possible pairs\n",
    "        \"\"\"\n",
    "        if not ct_scans or not wsi_folders:\n",
    "            return 0\n",
    "\n",
    "        if allow_repeats:\n",
    "            return len(ct_scans) * len(wsi_folders)\n",
    "        else:  # one-to-one\n",
    "            return min(len(ct_scans), len(wsi_folders))\n",
    "\n",
    "    def _get_fixed_pairs(self, ct_scans, wsi_folders, n_pairs, allow_repeats=True):\n",
    "        \"\"\"\n",
    "        Generate fixed number of pairs between CT scans and WSI folders.\n",
    "\n",
    "        Args:\n",
    "            ct_scans (list): List of CT scan files\n",
    "            wsi_folders (list): List of WSI folder names\n",
    "            n_pairs (int): Number of pairs to generate\n",
    "            allow_repeats (bool): If True, allows repeating elements to reach n_pairs\n",
    "                                If False, limits pairs to minimum unique combinations\n",
    "\n",
    "        Returns:\n",
    "            list: List of (ct_scan, wsi_folder) pairs\n",
    "        \"\"\"\n",
    "        max_unique_pairs = min(len(ct_scans), len(wsi_folders))\n",
    "\n",
    "        if not allow_repeats:\n",
    "            n_pairs = min(n_pairs, max_unique_pairs)\n",
    "\n",
    "        # Generate initial unique pairs\n",
    "        shuffled_ct = ct_scans.copy()\n",
    "        shuffled_wsi = wsi_folders.copy()\n",
    "        random.shuffle(shuffled_ct)\n",
    "        random.shuffle(shuffled_wsi)\n",
    "\n",
    "        pairs = list(\n",
    "            zip(shuffled_ct[:max_unique_pairs], shuffled_wsi[:max_unique_pairs])\n",
    "        )\n",
    "\n",
    "        if n_pairs <= len(pairs):\n",
    "            # Downsample if needed\n",
    "            random.shuffle(pairs)\n",
    "            return pairs[:n_pairs]\n",
    "\n",
    "        if not allow_repeats:\n",
    "            return pairs\n",
    "\n",
    "        # Need to generate additional pairs with repeats\n",
    "        while len(pairs) < n_pairs:\n",
    "            ct_scan = random.choice(ct_scans)\n",
    "            wsi_folder = random.choice(wsi_folders)\n",
    "            pairs.append((ct_scan, wsi_folder))\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def _load_split(self):\n",
    "        \"\"\"Load and organize all CT and WSI data for the given split\n",
    "\n",
    "        Supports different pairing modes:\n",
    "        - 'all_combinations': Creates all possible CT-WSI pairs\n",
    "        - 'one_to_one': Creates random 1:1 pairs\n",
    "        - 'fixed_count': Creates fixed number of pairs per patient\n",
    "        \"\"\"\n",
    "\n",
    "        # First pass: count maximum possible pairs per patient\n",
    "        max_pairs_possible = float(\"inf\")\n",
    "        if self.pairing_mode == \"fixed_count\":\n",
    "            for patient_id in self.labels_splits[\"case_id\"].values:\n",
    "\n",
    "                ct_path = os.path.join(self.ct_path, patient_id)\n",
    "                ct_features = []\n",
    "                if os.path.exists(ct_path):\n",
    "                    ct_features = [f for f in os.listdir(ct_path)]\n",
    "\n",
    "                wsi_path = os.path.join(self.wsi_path)\n",
    "                wsi_features = [\n",
    "                    f\n",
    "                    for f in os.listdir(wsi_path)\n",
    "                    if patient_id in f and os.path.isdir(os.path.join(wsi_path, f))\n",
    "                ]\n",
    "\n",
    "                patient_max_pairs = self._get_max_pairs_for_patient(\n",
    "                    ct_features, wsi_features, self.allow_repeats\n",
    "                )\n",
    "\n",
    "                if patient_max_pairs > 0:  # Only update if patient has both modalities\n",
    "\n",
    "                    max_pairs_possible = min(max_pairs_possible, patient_max_pairs)\n",
    "\n",
    "        # Use provided pairs_per_patient or calculated maximum\n",
    "        n_pairs = (\n",
    "            self.pairs_per_patient\n",
    "            if self.pairs_per_patient is not None\n",
    "            else max_pairs_possible\n",
    "        )\n",
    "\n",
    "        # Main loading loop\n",
    "        for row in self.labels_splits[\"case_id\"].values:\n",
    "            patient_id = row.strip()\n",
    "            # Find all CT scans for this patient\n",
    "            ct_path = os.path.join(self.ct_path, patient_id)\n",
    "            ct_features = []\n",
    "            if os.path.exists(ct_path):\n",
    "                ct_features = [f for f in os.listdir(ct_path)]\n",
    "\n",
    "            # Find all WSI .h5 files for this patient\n",
    "            wsi_path = self.wsi_path\n",
    "            wsi_features = [f for f in os.listdir(wsi_path) if patient_id in f]\n",
    "\n",
    "            # Skip patient if we require both modalities and they don't have them\n",
    "            if self.require_both_modalities and (not ct_features or not wsi_features):\n",
    "                continue\n",
    "\n",
    "            # Store available data for this patient\n",
    "            self.patient_data[patient_id] = {\n",
    "                \"ct_features\": ct_features,\n",
    "                \"wsi_features\": wsi_features,\n",
    "            }\n",
    "\n",
    "            # Update modality statistics\n",
    "            if ct_features and wsi_features:\n",
    "                self.modality_stats[\"both\"] += 1\n",
    "            elif ct_features:\n",
    "                self.modality_stats[\"ct_only\"] += 1\n",
    "            else:\n",
    "                self.modality_stats[\"wsi_only\"] += 1\n",
    "\n",
    "            cnt = 0\n",
    "            # Generate samples based on available data and pairing mode\n",
    "            if ct_features and wsi_features:\n",
    "                if self.pairing_mode == \"fixed_count\":\n",
    "                    # Generate fixed number of pairs\n",
    "                    pairs = self._get_fixed_pairs(\n",
    "                        ct_features, wsi_features, n_pairs, self.allow_repeats\n",
    "                    )\n",
    "\n",
    "                    for ct_feature, wsi_feature in pairs:\n",
    "                        cnt += 1\n",
    "                        self.samples.append(\n",
    "                            {\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"ct_path\": os.path.join(\n",
    "                                    self.ct_path, patient_id, ct_feature\n",
    "                                ),\n",
    "                                \"wsi_folder\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                                \"base_modality_mask\": [1, 1],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                elif self.pairing_mode == \"one_to_one\":\n",
    "                    # Original one_to_one logic\n",
    "                    num_pairs = min(len(ct_features), len(wsi_features))\n",
    "                    shuffled_ct = ct_features.copy()\n",
    "                    shuffled_wsi = wsi_features.copy()\n",
    "                    random.shuffle(shuffled_ct)\n",
    "                    random.shuffle(shuffled_wsi)\n",
    "\n",
    "                    for ct_feature, wsi_feature in zip(\n",
    "                        shuffled_ct[:num_pairs], shuffled_wsi[:num_pairs]\n",
    "                    ):\n",
    "                        cnt += 1\n",
    "                        self.samples.append(\n",
    "                            {\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"ct_path\": os.path.join(\n",
    "                                    self.ct_path, patient_id, ct_feature\n",
    "                                ),\n",
    "                                \"wsi_feature\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                                \"base_modality_mask\": [1, 1],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                else:  # 'all_combinations' mode\n",
    "                    for ct_feature, wsi_feature in product(ct_features, wsi_features):\n",
    "                        cnt += 1\n",
    "                        self.samples.append(\n",
    "                            {\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"ct_path\": os.path.join(\n",
    "                                    self.ct_path, patient_id, ct_feature\n",
    "                                ),\n",
    "                                \"wsi_feature\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                                \"base_modality_mask\": [1, 1],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            elif ct_features:\n",
    "                for ct_feature in ct_features:\n",
    "                    cnt += 1\n",
    "                    self.samples.append(\n",
    "                        {\n",
    "                            \"patient_id\": patient_id,\n",
    "                            \"ct_path\": os.path.join(\n",
    "                                self.ct_path, patient_id, ct_feature\n",
    "                            ),\n",
    "                            \"wsi_feature\": None,\n",
    "                            \"base_modality_mask\": [1, 0],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            elif wsi_features:\n",
    "                for wsi_feature in wsi_features:\n",
    "                    cnt += 1\n",
    "                    self.samples.append(\n",
    "                        {\n",
    "                            \"patient_id\": patient_id,\n",
    "                            \"ct_path\": None,\n",
    "                            \"wsi_feature\": os.path.join(self.wsi_path, wsi_feature),\n",
    "                            \"base_modality_mask\": [0, 1],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    def _load_ct_feature(self, ct_path):\n",
    "        \"\"\"Load and standardize CT feature\"\"\"\n",
    "        if ct_path.endswith(\".pt\"):\n",
    "            volume = np.array(torch.load(ct_path, weights_only=True))\n",
    "        elif ct_path.endswith(\".npy\"):\n",
    "            volume = np.load(ct_path)\n",
    "        return volume\n",
    "\n",
    "    def _load_wsi_feature(self, wsi_path):\n",
    "        \"\"\"Load WSI feature\"\"\"\n",
    "        feature = None\n",
    "        with h5py.File(wsi_path, \"r\") as f:\n",
    "            feature = np.array(f[\"features\"][:])\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def _get_empty_ct_feature(self):\n",
    "        \"\"\"Return empty CT feature of correct shape\"\"\"\n",
    "        return np.zeros((131, 1024))\n",
    "\n",
    "    def _get_empty_wsi_feature(self):\n",
    "        \"\"\"Return empty WSI feature of correct shape\"\"\"\n",
    "        return np.zeros((768,))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            dict: {\n",
    "                'patient_id': str,\n",
    "                'ct_feature': numpy array or zeros if missing,\n",
    "                'wsi_feature': tensor or zeros if missing,\n",
    "                'label': int,\n",
    "                'modality_mask': tensor indicating present modalities [CT, WSI],\n",
    "                'base_modality_mask': tensor indicating modalities available in dataset\n",
    "            }\n",
    "        \"\"\"\n",
    "\n",
    "        sample = self.samples[index]\n",
    "        patient_id = sample[\"patient_id\"]\n",
    "        base_mask = sample[\"base_modality_mask\"]\n",
    "        ct_path = sample[\"ct_path\"]\n",
    "        wsi_path = sample[\"wsi_feature\"]\n",
    "\n",
    "        # Apply additional random masking only to available modalities\n",
    "        final_mask = base_mask.copy()\n",
    "        if self.missing_modality_prob > 0:\n",
    "            if self.missing_modality == \"both\":\n",
    "                for i in range(2):\n",
    "                    if (\n",
    "                        base_mask[i] == 1\n",
    "                        and random.random() < self.missing_modality_prob\n",
    "                    ):\n",
    "                        final_mask[i] = 0\n",
    "            elif self.missing_modality == \"ct\":\n",
    "                if base_mask[0] == 1 and random.random() < self.missing_modality_prob:\n",
    "                    final_mask[0] = 0\n",
    "            elif self.missing_modality == \"wsi\":\n",
    "                if base_mask[1] == 1 and random.random() < self.missing_modality_prob:\n",
    "                    final_mask[1] = 0\n",
    "\n",
    "            # Ensure at least one modality remains if it was originally available\n",
    "            if sum(final_mask) == 0 and sum(base_mask) > 0:\n",
    "                # Randomly choose one of the originally available modalities\n",
    "                available_indices = [i for i in range(2) if base_mask[i] == 1]\n",
    "                chosen_idx = random.choice(available_indices)\n",
    "                final_mask[chosen_idx] = 1\n",
    "\n",
    "        # Load features based on final mask\n",
    "        ct_feature = (\n",
    "            self._load_ct_feature(sample[\"ct_path\"])\n",
    "            if final_mask[0] and sample[\"ct_path\"]\n",
    "            else self._get_empty_ct_feature()\n",
    "        )\n",
    "\n",
    "        wsi_feature = (\n",
    "            self._load_wsi_feature(sample[\"wsi_feature\"])\n",
    "            if final_mask[1] and sample[\"wsi_feature\"]\n",
    "            else self._get_empty_wsi_feature()\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"patient_id\": patient_id,\n",
    "            \"ct_feature\": torch.from_numpy(ct_feature).float(),\n",
    "            \"wsi_feature\": torch.from_numpy(wsi_feature).float(),\n",
    "            \"survtime\": torch.tensor(\n",
    "                self.labels_splits[self.labels_splits[\"case_id\"] == patient_id][\n",
    "                    \"OS_days\"\n",
    "                ].iloc[0],\n",
    "                dtype=torch.long,\n",
    "            ),\n",
    "            \"censor\": ~torch.tensor(\n",
    "                self.labels_splits[self.labels_splits[\"case_id\"] == patient_id][\n",
    "                    \"OS_event\"\n",
    "                ].iloc[0],\n",
    "                dtype=torch.bool,\n",
    "            ),\n",
    "            \"modality_mask\": torch.tensor(final_mask, dtype=torch.float32),\n",
    "            \"base_modality_mask\": torch.tensor(base_mask, dtype=torch.float32),\n",
    "            \"ct_path\": ct_path,\n",
    "            \"wsi_path\": wsi_path\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def stats(self):\n",
    "        \"\"\"Return dataset statistics\"\"\"\n",
    "        return {\n",
    "            \"total_samples\": len(self.samples),\n",
    "            \"total_patients\": len(self.patient_data),\n",
    "            \"modality_availability\": self.modality_stats,\n",
    "            \"missing_modality_prob\": self.missing_modality_prob,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def move_batch_to_device(batch, device):\n",
    "        \"\"\"Move all elements of the batch to device\"\"\"\n",
    "        batch[\"ct_feature\"] = batch[\"ct_feature\"].to(device)\n",
    "        batch[\"wsi_feature\"] = batch[\"wsi_feature\"].to(device)\n",
    "        batch[\"survtime\"] = batch[\"survtime\"].to(device)\n",
    "        batch[\"censor\"] = batch[\"censor\"].to(device)\n",
    "        batch[\"modality_mask\"] = batch[\"modality_mask\"].to(device)\n",
    "        batch[\"ct_path\"] = batch[\"ct_path\"].to(device)\n",
    "        batch[\"wsi_path\"] = batch[\"wsi_path\"].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a09b2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-02631   0.025622       0.025622             1              1\n",
      "0  C3N-02631   0.025737       0.025737             1              1\n",
      "2  C3N-02631   0.059709       0.059709             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-02631   0.059709       0.059709             1              1\n",
      "0  C3N-02631   0.025737       0.025737             1              1\n",
      "1  C3N-02631   0.025622       0.025622             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02678   0.248485       0.248485             1              1\n",
      "1  C3N-02678   0.248485       0.248485             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-02678   0.248485       0.248485             1              1\n",
      "0  C3N-02678   0.248485       0.248485             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-01877        0.0            0.0             1              1\n",
      "1  C3N-01877        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-01877        0.0            0.0             1              1\n",
      "0  C3N-01877        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02639        0.0            0.0             1              1\n",
      "1  C3N-02639        0.0            0.0             1              1\n",
      "2  C3N-01003        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-01003        0.0            0.0             1              1\n",
      "1  C3N-02639        0.0            0.0             1              1\n",
      "0  C3N-02639        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed5/fold_3_topk/topk_high_gradient.csv\n"
     ]
    }
   ],
   "source": [
    "folds_dir = \"./models/ckpts/CPTAC_UCEC_titan_medimsight_trainmixed5\"\n",
    "ct_path= \"../MedImageInsights/embeddings_cptacucec\"\n",
    "wsi_path= \"../../trident_processed_UCEC_titan/20x_512px_0px_overlap/slide_features_titan\"\n",
    "test_path =  \"./data/processed/processed_CPTACUCEC_survival/k=all.tsv\"\n",
    "output_dir = \"./interpretability/interpretability_UCEC_mixed5\"\n",
    "\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16, n_folds=4)\n",
    "for i in range(4):\n",
    "    csv_input = f\"./interpretability/interpretability_UCEC_mixed5/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_UCEC_mixed5/fold_{i}_topk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51956999",
   "metadata": {},
   "source": [
    "## CPTAC UCEC 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f34bc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-02631   0.005976       0.005976             1              1\n",
      "0  C3N-02631   0.058858       0.058858             1              1\n",
      "2  C3N-02631   0.097936       0.097936             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-02631   0.097936       0.097936             1              1\n",
      "0  C3N-02631   0.058858       0.058858             1              1\n",
      "1  C3N-02631   0.005976       0.005976             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-02678   0.008589       0.008589             1              1\n",
      "0  C3N-02678   0.008673       0.008673             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02678   0.008673       0.008673             1              1\n",
      "1  C3N-02678   0.008589       0.008589             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-01877   0.033299       0.033299             1              1\n",
      "0  C3N-01877   0.037402       0.037402             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-01877   0.037402       0.037402             1              1\n",
      "1  C3N-01877   0.033299       0.033299             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-01003   0.000654       0.000654             1              1\n",
      "1  C3N-02639   0.083787       0.083787             1              1\n",
      "0  C3N-02639   0.085376       0.085376             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02639   0.085376       0.085376             1              1\n",
      "1  C3N-02639   0.083787       0.083787             1              1\n",
      "2  C3N-01003   0.000654       0.000654             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed15/fold_3_topk/topk_high_gradient.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "folds_dir = \"./models/ckpts/CPTAC_UCEC_titan_medimsight_trainmixed15\"\n",
    "ct_path= \"../MedImageInsights/embeddings_cptacucec\"\n",
    "wsi_path= \"../../trident_processed_UCEC_titan/20x_512px_0px_overlap/slide_features_titan\"\n",
    "test_path =  \"./data/processed/processed_CPTACUCEC_survival/k=all.tsv\"\n",
    "output_dir = \"./interpretability/interpretability_UCEC_mixed15\"\n",
    "\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16, n_folds=4)\n",
    "for i in range(4):\n",
    "    csv_input = f\"./interpretability/interpretability_UCEC_mixed15/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_UCEC_mixed15/fold_{i}_topk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c024d",
   "metadata": {},
   "source": [
    "## CPTAC UCEC 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50e3a607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02631   0.041842       0.041842             1              1\n",
      "1  C3N-02631   0.046953       0.046953             1              1\n",
      "2  C3N-02631   0.128688       0.128688             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-02631   0.128688       0.128688             1              1\n",
      "1  C3N-02631   0.046953       0.046953             1              1\n",
      "0  C3N-02631   0.041842       0.041842             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02678   0.010207       0.010207             1              1\n",
      "1  C3N-02678   0.011398       0.011398             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-02678   0.011398       0.011398             1              1\n",
      "0  C3N-02678   0.010207       0.010207             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-01877   0.002767       0.002767             1              1\n",
      "1  C3N-01877   0.003491       0.003491             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-01877   0.003491       0.003491             1              1\n",
      "0  C3N-01877   0.002767       0.002767             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02639        0.0            0.0             1              1\n",
      "1  C3N-02639        0.0            0.0             1              1\n",
      "2  C3N-01003        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-01003        0.0            0.0             1              1\n",
      "1  C3N-02639        0.0            0.0             1              1\n",
      "0  C3N-02639        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed30/fold_3_topk/topk_high_gradient.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "folds_dir = \"./models/ckpts/CPTAC_UCEC_titan_medimsight_trainmixed30\"\n",
    "ct_path= \"../MedImageInsights/embeddings_cptacucec\"\n",
    "wsi_path= \"../../trident_processed_UCEC_titan/20x_512px_0px_overlap/slide_features_titan\"\n",
    "test_path =  \"./data/processed/processed_CPTACUCEC_survival/k=all.tsv\"\n",
    "output_dir = \"./interpretability/interpretability_UCEC_mixed30\"\n",
    "\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16, n_folds=4)\n",
    "for i in range(4):\n",
    "    csv_input = f\"./interpretability/interpretability_UCEC_mixed30/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_UCEC_mixed30/fold_{i}_topk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf4e94",
   "metadata": {},
   "source": [
    "## CPTAC UCEC 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5363406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n",
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-02631   0.028295       0.028295             1              1\n",
      "0  C3N-02631   0.028456       0.028456             1              1\n",
      "2  C3N-02631   0.066085       0.066085             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-02631   0.066085       0.066085             1              1\n",
      "0  C3N-02631   0.028456       0.028456             1              1\n",
      "1  C3N-02631   0.028295       0.028295             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed50/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed50/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02678        0.0            0.0             1              1\n",
      "1  C3N-02678        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-02678        0.0            0.0             1              1\n",
      "0  C3N-02678        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed50/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed50/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "1  C3N-01877   0.008887       0.008887             1              1\n",
      "0  C3N-01877   0.008931       0.008931             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-01877   0.008931       0.008931             1              1\n",
      "1  C3N-01877   0.008887       0.008887             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed50/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed50/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0  C3N-02639        0.0            0.0             1              1\n",
      "1  C3N-02639        0.0            0.0             1              1\n",
      "2  C3N-01003        0.0            0.0             1              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "  patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "2  C3N-01003        0.0            0.0             1              1\n",
      "1  C3N-02639        0.0            0.0             1              1\n",
      "0  C3N-02639        0.0            0.0             1              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_UCEC_mixed50/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_UCEC_mixed50/fold_3_topk/topk_high_gradient.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/3262844422.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2649313966.py:387: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "folds_dir = \"./models/ckpts/CPTACUCEC_trainmixed50_multival_Titan_MedImSight_fullepochs\"\n",
    "ct_path = \"../MedImageInsights/embeddings_cptacucec\"\n",
    "wsi_path = \"../../trident_processed_UCEC_titan/20x_512px_0px_overlap/slide_features_titan\"\n",
    "test_path = \"./data/processed/processed_CPTACUCEC_survival/k=all.tsv\"\n",
    "output_dir = \"./interpretability/interpretability_UCEC_mixed50\"\n",
    "\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16, n_folds=4)\n",
    "for i in range(4):\n",
    "    csv_input = f\"./interpretability/interpretability_UCEC_mixed50/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_UCEC_mixed50/fold_{i}_topk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e82612",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a76b90",
   "metadata": {},
   "source": [
    "## CPTAC PDA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bf3b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_and_log(folds_dir, ct_path, wsi_path, test_path, output_dir, batch_size=16, n_folds=5):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        print(f\"\\n=== Fold {fold} ===\")\n",
    "        model = MADPENetNoBackbonesSurv(\n",
    "            rad_input_dim=1024, histo_input_dim=768,\n",
    "            inter_dim=256, token_dim=256, dim_hider=256\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        # Load checkpoint\n",
    "        fold_dir = os.path.join(folds_dir, f\"fold_{fold}\")\n",
    "        model_subdir = next(d for d in os.listdir(fold_dir)\n",
    "                            if os.path.isdir(os.path.join(fold_dir, d)))\n",
    "        model_file = next(f for f in os.listdir(os.path.join(fold_dir, model_subdir))\n",
    "                          if f.endswith(\"mixed_missing.pth\"))\n",
    "        checkpoint = torch.load(\n",
    "            os.path.join(fold_dir, model_subdir, model_file), map_location=\"cpu\"\n",
    "        )\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        dataset = MultimodalCTWSIDatasetSurv(\n",
    "            fold=0,\n",
    "            split=\"train\",\n",
    "            ct_path=ct_path,\n",
    "            wsi_path=wsi_path,\n",
    "            labels_splits_path=test_path,\n",
    "            missing_modality_prob=0.0,\n",
    "            require_both_modalities=False,\n",
    "            pairing_mode=\"one_to_one\",\n",
    "            allow_repeats=True,\n",
    "            pairs_per_patient=None,\n",
    "            missing_modality=\"wsi\"\n",
    "        )\n",
    "\n",
    "        loader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0)\n",
    "\n",
    "        # CSV Logger\n",
    "        log_file = os.path.join(output_dir, f\"fold_{fold}_log.csv\")\n",
    "        fold_tensor_dir = os.path.join(output_dir, f\"fold_{fold}\")\n",
    "        os.makedirs(fold_tensor_dir, exist_ok=True)\n",
    "\n",
    "        with open(log_file, \"w\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                \"patient_id\", \"ct_path\", \"wsi_path\",\n",
    "                \"ct_available\", \"wsi_available\",\n",
    "                \"hazard_score\", \"survtime\", \"censor\",\n",
    "                \"grad_norm\", \"curvature\",\n",
    "                \"fused_path\", \"pe_path\", \"hazard_tensor_path\"\n",
    "            ])\n",
    "\n",
    "            sample_counter = 0\n",
    "            for batch in loader:\n",
    "                # ensure floats\n",
    "                for k, v in batch.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        batch[k] = v.float()\n",
    "\n",
    "                with torch.enable_grad():\n",
    "                    for v in batch.values():\n",
    "                        if isinstance(v, torch.Tensor):\n",
    "                            v.requires_grad = True\n",
    "\n",
    "                    outputs = model(\n",
    "                        batch[\"ct_feature\"],\n",
    "                        batch[\"wsi_feature\"],\n",
    "                        modality_flag=batch[\"modality_mask\"],\n",
    "                        output_layers=[\"hazard\", \"fused_features\", \"positional_embeddings\"]\n",
    "                    )\n",
    "                    hazard = outputs[\"hazard\"].squeeze(1)  # [B]\n",
    "                    fused_features = outputs[\"fused_features\"]    # [B, D]\n",
    "                    pos_emb = outputs[\"positional_embeddings\"]    # [B, D]\n",
    "\n",
    "                    grad_norms, curvatures = compute_grad_and_curvature(\n",
    "                        fused_features, hazard,\n",
    "                        batch[\"survtime\"], batch[\"censor\"]\n",
    "                    )\n",
    "\n",
    "                # save per-sample\n",
    "                B = hazard.size(0)\n",
    "                for j in range(B):\n",
    "                    sample_id = f\"sample_{sample_counter:06d}\"\n",
    "                    fused_path = os.path.join(fold_tensor_dir, f\"{sample_id}_fused.npy\")\n",
    "                    pe_path = os.path.join(fold_tensor_dir, f\"{sample_id}_pe.npy\")\n",
    "                    hazard_path = os.path.join(fold_tensor_dir, f\"{sample_id}_hazard.npy\")\n",
    "\n",
    "                    np.save(fused_path, fused_features[j].detach().cpu().numpy())\n",
    "                    np.save(pe_path, pos_emb[j].detach().cpu().numpy())\n",
    "                    np.save(hazard_path, hazard[j].detach().cpu().numpy())\n",
    "\n",
    "                    ct_flag = int(batch[\"modality_mask\"][j, 0].item())\n",
    "                    wsi_flag = int(batch[\"modality_mask\"][j, 1].item())\n",
    "                    if batch[\"censor\"][j].item()==0: continue\n",
    "                    writer.writerow([\n",
    "                        batch[\"patient_id\"][j],\n",
    "                        batch[\"ct_path\"][j],\n",
    "                        batch[\"wsi_path\"][j],\n",
    "                        ct_flag, wsi_flag,\n",
    "                        hazard[j].item(),\n",
    "                        batch[\"survtime\"][j].item(),\n",
    "                        batch[\"censor\"][j].item(),\n",
    "                        grad_norms[j], curvatures[j],\n",
    "                        fused_path, pe_path, hazard_path\n",
    "                    ])\n",
    "\n",
    "                    sample_counter += 1\n",
    "                    \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "def find_topk_gradient_extremes(\n",
    "    csv_path: str,\n",
    "    k: int = 10,\n",
    "    output_dir: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds top-k highest and lowest |gradient norm| samples from a CSV log.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to fold_X_log.csv generated during eval.\n",
    "        k (int): Number of top and bottom samples to retrieve.\n",
    "        output_dir (str): Optional folder to save CSVs. If None, only prints results.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if \"grad_norm\" not in df.columns:\n",
    "        raise ValueError(\"The CSV does not contain 'grad_norm' column.\")\n",
    "\n",
    "    # Compute absolute gradient norms\n",
    "    df[\"abs_grad_norm\"] = df[\"grad_norm\"].abs()\n",
    "\n",
    "    # Sort by absolute gradient norm\n",
    "    df_sorted = df.sort_values(by=\"abs_grad_norm\", ascending=True)\n",
    "\n",
    "    topk_low = df_sorted.head(k).copy()\n",
    "    topk_high = df_sorted.tail(k).copy()[::-1]  # High to low\n",
    "\n",
    "    print(\"\\n🔹 Top-k LOW |Gradient Norm| Samples:\")\n",
    "    print(topk_low[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    print(\"\\n🔸 Top-k HIGH |Gradient Norm| Samples:\")\n",
    "    print(topk_high[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        low_path = os.path.join(output_dir, \"topk_low_gradient.csv\")\n",
    "        high_path = os.path.join(output_dir, \"topk_high_gradient.csv\")\n",
    "        topk_low.to_csv(low_path, index=False)\n",
    "        topk_high.to_csv(high_path, index=False)\n",
    "        print(f\"\\n📁 Saved to:\\n  {low_path}\\n  {high_path}\")\n",
    "\n",
    "    return topk_low, topk_high\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "SEED = 0\n",
    "\n",
    "def set_global_seed(seed=SEED):\n",
    "    \"\"\"\n",
    "    Set a global seed for reproducibility across different libraries and random number generators.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed value to be used\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "871f7286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2231045823.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2377206491.py:388: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2231045823.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2377206491.py:388: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2231045823.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2377206491.py:388: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2231045823.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2377206491.py:388: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3656817/2231045823.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n",
      "/tmp/ipykernel_3656817/2377206491.py:388: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  \"censor\": ~torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "99   C3N-02768   0.000245       0.000245             0              1\n",
      "98   C3N-02768   0.000246       0.000246             0              1\n",
      "97   C3N-02768   0.000253       0.000253             0              1\n",
      "161  C3N-03190   0.000846       0.000846             0              1\n",
      "162  C3N-03190   0.000853       0.000853             0              1\n",
      "163  C3N-03190   0.000855       0.000855             0              1\n",
      "191  C3N-01379   0.001462       0.001462             0              1\n",
      "192  C3N-01379   0.001470       0.001470             0              1\n",
      "193  C3N-01379   0.001484       0.001484             0              1\n",
      "108  C3N-01168   0.005592       0.005592             0              1\n",
      "109  C3N-01168   0.005593       0.005593             0              1\n",
      "146  C3L-01158   0.005970       0.005970             0              1\n",
      "166  C3N-03086   0.005977       0.005977             0              1\n",
      "165  C3N-03086   0.005978       0.005978             0              1\n",
      "147  C3L-01158   0.005982       0.005982             0              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "14   C3N-01716   0.091586       0.091586             1              0\n",
      "15   C3N-01716   0.091155       0.091155             1              0\n",
      "106  C3L-01637   0.070939       0.070939             0              1\n",
      "107  C3L-01637   0.070926       0.070926             0              1\n",
      "134  C3N-00516   0.069489       0.069489             0              1\n",
      "133  C3N-00516   0.069488       0.069488             0              1\n",
      "95   C3L-01054   0.067221       0.067221             0              1\n",
      "96   C3L-01054   0.067219       0.067219             0              1\n",
      "94   C3L-01054   0.067197       0.067197             0              1\n",
      "75   C3N-02940   0.063956       0.063956             0              1\n",
      "76   C3N-02940   0.063956       0.063956             0              1\n",
      "74   C3N-02940   0.063917       0.063917             0              1\n",
      "58   C3N-03666   0.063618       0.063618             0              1\n",
      "135  C3N-00516   0.062237       0.062237             0              1\n",
      "190  C3N-01900   0.061396       0.061396             0              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed50TEST/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed50TEST/fold_0_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0    C3L-04473        0.0            0.0             0              1\n",
      "126  C3L-00102        0.0            0.0             0              1\n",
      "127  C3L-00102        0.0            0.0             0              1\n",
      "128  C3N-04282        0.0            0.0             0              1\n",
      "129  C3N-04282        0.0            0.0             0              1\n",
      "130  C3N-04119        0.0            0.0             0              1\n",
      "131  C3N-04119        0.0            0.0             0              1\n",
      "132  C3N-04119        0.0            0.0             0              1\n",
      "133  C3N-00516        0.0            0.0             0              1\n",
      "134  C3N-00516        0.0            0.0             0              1\n",
      "135  C3N-00516        0.0            0.0             0              1\n",
      "136  C3L-01662        0.0            0.0             0              1\n",
      "137  C3L-01662        0.0            0.0             0              1\n",
      "138  C3L-01662        0.0            0.0             0              1\n",
      "139  C3L-01662        0.0            0.0             0              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "198  C3N-03778        0.0            0.0             0              1\n",
      "73   C3N-04126        0.0            0.0             0              1\n",
      "71   C3N-04126        0.0            0.0             0              1\n",
      "70   C3L-02701        0.0            0.0             0              1\n",
      "69   C3L-02701        0.0            0.0             0              1\n",
      "68   C3L-02701        0.0            0.0             0              1\n",
      "67   C3L-02701        0.0            0.0             0              1\n",
      "66   C3L-02701        0.0            0.0             0              1\n",
      "65   C3L-02701        0.0            0.0             0              1\n",
      "64   C3L-02701        0.0            0.0             0              1\n",
      "63   C3L-02701        0.0            0.0             0              1\n",
      "62   C3L-04072        0.0            0.0             0              1\n",
      "61   C3L-04072        0.0            0.0             0              1\n",
      "60   C3N-03439        0.0            0.0             0              1\n",
      "59   C3N-03439        0.0            0.0             0              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed50TEST/fold_1_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed50TEST/fold_1_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0    C3L-04473        0.0            0.0             0              1\n",
      "126  C3L-00102        0.0            0.0             0              1\n",
      "127  C3L-00102        0.0            0.0             0              1\n",
      "128  C3N-04282        0.0            0.0             0              1\n",
      "129  C3N-04282        0.0            0.0             0              1\n",
      "130  C3N-04119        0.0            0.0             0              1\n",
      "131  C3N-04119        0.0            0.0             0              1\n",
      "132  C3N-04119        0.0            0.0             0              1\n",
      "133  C3N-00516        0.0            0.0             0              1\n",
      "134  C3N-00516        0.0            0.0             0              1\n",
      "135  C3N-00516        0.0            0.0             0              1\n",
      "136  C3L-01662        0.0            0.0             0              1\n",
      "137  C3L-01662        0.0            0.0             0              1\n",
      "138  C3L-01662        0.0            0.0             0              1\n",
      "139  C3L-01662        0.0            0.0             0              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "13   C3N-01716   0.064586       0.064586             1              0\n",
      "15   C3N-01716   0.015250       0.015250             1              0\n",
      "198  C3N-03778   0.000000       0.000000             0              1\n",
      "74   C3N-02940   0.000000       0.000000             0              1\n",
      "72   C3N-04126   0.000000       0.000000             0              1\n",
      "71   C3N-04126   0.000000       0.000000             0              1\n",
      "70   C3L-02701   0.000000       0.000000             0              1\n",
      "69   C3L-02701   0.000000       0.000000             0              1\n",
      "68   C3L-02701   0.000000       0.000000             0              1\n",
      "67   C3L-02701   0.000000       0.000000             0              1\n",
      "66   C3L-02701   0.000000       0.000000             0              1\n",
      "65   C3L-02701   0.000000       0.000000             0              1\n",
      "64   C3L-02701   0.000000       0.000000             0              1\n",
      "63   C3L-02701   0.000000       0.000000             0              1\n",
      "62   C3L-04072   0.000000       0.000000             0              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed50TEST/fold_2_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed50TEST/fold_2_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "97   C3N-02768   0.000017       0.000017             0              1\n",
      "99   C3N-02768   0.000017       0.000017             0              1\n",
      "98   C3N-02768   0.000017       0.000017             0              1\n",
      "163  C3N-03190   0.000059       0.000059             0              1\n",
      "162  C3N-03190   0.000059       0.000059             0              1\n",
      "161  C3N-03190   0.000059       0.000059             0              1\n",
      "193  C3N-01379   0.000101       0.000101             0              1\n",
      "192  C3N-01379   0.000101       0.000101             0              1\n",
      "191  C3N-01379   0.000101       0.000101             0              1\n",
      "109  C3N-01168   0.000385       0.000385             0              1\n",
      "108  C3N-01168   0.000385       0.000385             0              1\n",
      "164  C3N-03086   0.000411       0.000411             0              1\n",
      "148  C3L-01158   0.000411       0.000411             0              1\n",
      "147  C3L-01158   0.000411       0.000411             0              1\n",
      "165  C3N-03086   0.000411       0.000411             0              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "15   C3N-01716   0.009583       0.009583             1              0\n",
      "13   C3N-01716   0.009579       0.009579             1              0\n",
      "14   C3N-01716   0.009576       0.009576             1              0\n",
      "107  C3L-01637   0.004880       0.004880             0              1\n",
      "106  C3L-01637   0.004879       0.004879             0              1\n",
      "134  C3N-00516   0.004780       0.004780             0              1\n",
      "133  C3N-00516   0.004780       0.004780             0              1\n",
      "94   C3L-01054   0.004624       0.004624             0              1\n",
      "96   C3L-01054   0.004624       0.004624             0              1\n",
      "95   C3L-01054   0.004624       0.004624             0              1\n",
      "74   C3N-02940   0.004399       0.004399             0              1\n",
      "76   C3N-02940   0.004399       0.004399             0              1\n",
      "75   C3N-02940   0.004399       0.004399             0              1\n",
      "58   C3N-03666   0.004374       0.004374             0              1\n",
      "135  C3N-00516   0.004281       0.004281             0              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed50TEST/fold_3_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed50TEST/fold_3_topk/topk_high_gradient.csv\n",
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "99   C3N-02768   0.000078       0.000078             0              1\n",
      "97   C3N-02768   0.000079       0.000079             0              1\n",
      "98   C3N-02768   0.000079       0.000079             0              1\n",
      "163  C3N-03190   0.000267       0.000267             0              1\n",
      "162  C3N-03190   0.000268       0.000268             0              1\n",
      "161  C3N-03190   0.000268       0.000268             0              1\n",
      "193  C3N-01379   0.000461       0.000461             0              1\n",
      "192  C3N-01379   0.000463       0.000463             0              1\n",
      "191  C3N-01379   0.000463       0.000463             0              1\n",
      "108  C3N-01168   0.001755       0.001755             0              1\n",
      "109  C3N-01168   0.001755       0.001755             0              1\n",
      "164  C3N-03086   0.001871       0.001871             0              1\n",
      "147  C3L-01158   0.001873       0.001873             0              1\n",
      "148  C3L-01158   0.001873       0.001873             0              1\n",
      "165  C3N-03086   0.001874       0.001874             0              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "107  C3L-01637   0.022225       0.022225             0              1\n",
      "106  C3L-01637   0.022224       0.022224             0              1\n",
      "133  C3N-00516   0.021881       0.021881             0              1\n",
      "134  C3N-00516   0.021770       0.021770             0              1\n",
      "94   C3L-01054   0.021065       0.021065             0              1\n",
      "96   C3L-01054   0.021058       0.021058             0              1\n",
      "95   C3L-01054   0.021058       0.021058             0              1\n",
      "74   C3N-02940   0.020042       0.020042             0              1\n",
      "76   C3N-02940   0.020033       0.020033             0              1\n",
      "75   C3N-02940   0.020032       0.020032             0              1\n",
      "58   C3N-03666   0.019919       0.019919             0              1\n",
      "135  C3N-00516   0.019498       0.019498             0              1\n",
      "189  C3N-01900   0.019239       0.019239             0              1\n",
      "188  C3N-01900   0.019232       0.019232             0              1\n",
      "190  C3N-01900   0.019228       0.019228             0              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_mixed50TEST/fold_4_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_mixed50TEST/fold_4_topk/topk_high_gradient.csv\n"
     ]
    }
   ],
   "source": [
    "ct_path=\"../MedImageInsights/embeddings_output_cptacpda_93\"\n",
    "wsi_path=\"../../TitanCPTACPDA/20x_512px_0px_overlap/slide_features_titan\"\n",
    "output_dir = \"./interpretability/interpretability_PDA_mixed50TEST\"\n",
    "folds_dir = \"./models/ckpts/CPTACPDA_trainmixed50_multival_Titan_MedImSight_new\"\n",
    "test_path = \"./data/processed/processed_CPTAC_PDA_test/k=all.tsv\"\n",
    "\n",
    "\n",
    "evaluate_and_log(folds_dir,ct_path,wsi_path,test_path,output_dir,batch_size=16, n_folds=5)\n",
    "for i in range(5):\n",
    "    csv_input = f\"./interpretability/interpretability_PDA_mixed50TEST/fold_{i}_log.csv\"\n",
    "    find_topk_gradient_extremes(\n",
    "        csv_path=csv_input,\n",
    "        k=15,\n",
    "        output_dir=f\"./interpretability/interpretability_PDA_mixed50TEST/fold_{i}_topk\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b735d",
   "metadata": {},
   "source": [
    "## CPTAC UCEC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7bd4ef12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3606097/1337002563.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(fold_dir, model_subdir, model_file), map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Top-k LOW |Gradient Norm| Samples:\n",
      "    patient_id  grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "0    C3L-04473        0.0            0.0             0              1\n",
      "177  C3L-01662        0.0            0.0             0              1\n",
      "178  C3L-01662        0.0            0.0             0              1\n",
      "179  C3L-01662        0.0            0.0             0              1\n",
      "180  C3L-01662        0.0            0.0             0              1\n",
      "181  C3L-01160        0.0            0.0             0              1\n",
      "182  C3L-01160        0.0            0.0             0              1\n",
      "183  C3L-01160        0.0            0.0             0              1\n",
      "184  C3N-03884        0.0            0.0             0              1\n",
      "185  C3N-03884        0.0            0.0             0              1\n",
      "186  C3N-03884        0.0            0.0             0              1\n",
      "187  C3L-01031        0.0            0.0             0              1\n",
      "188  C3L-01031        0.0            0.0             0              1\n",
      "189  C3L-01031        0.0            0.0             0              1\n",
      "190  C3L-04027        0.0            0.0             0              1\n",
      "\n",
      "🔸 Top-k HIGH |Gradient Norm| Samples:\n",
      "    patient_id     grad_norm  abs_grad_norm  ct_available  wsi_available\n",
      "136  C3N-02768  2.288209e-08   2.288209e-08             0              1\n",
      "264  C3N-03853  2.288209e-08   2.288209e-08             0              1\n",
      "208  C3L-02809  2.288209e-08   2.288209e-08             0              1\n",
      "63   C3N-01719  2.288209e-08   2.288209e-08             0              1\n",
      "62   C3N-01719  2.288209e-08   2.288209e-08             0              1\n",
      "220  C3N-03190  2.288209e-08   2.288209e-08             0              1\n",
      "55   C3N-04284  2.288209e-08   2.288209e-08             0              1\n",
      "92   C3N-03439  2.288209e-08   2.288209e-08             0              1\n",
      "35   C3N-03069  2.288209e-08   2.288209e-08             0              1\n",
      "248  C3L-03388  2.288209e-08   2.288209e-08             0              1\n",
      "166  C3L-00102  2.288209e-08   2.288209e-08             0              1\n",
      "162  C3L-01052  2.288209e-08   2.288209e-08             0              1\n",
      "257  C3N-03211  2.288209e-08   2.288209e-08             0              1\n",
      "118  C3L-02604  2.288209e-08   2.288209e-08             0              1\n",
      "105  C3L-02701  2.288209e-08   2.288209e-08             0              1\n",
      "\n",
      "📁 Saved to:\n",
      "  ./interpretability/interpretability_PDA_testmixed50/fold_0_topk/topk_low_gradient.csv\n",
      "  ./interpretability/interpretability_PDA_testmixed50/fold_0_topk/topk_high_gradient.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from data.multimodal_features_surv import MultimodalCTWSIDatasetSurv\n",
    "from models.dpe.main_model_nobackbone_surv_new_gcs import MADPENetNoBackbonesSurv\n",
    "\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "\n",
    "class CoxLoss(_WeightedLoss):\n",
    "    def forward(self, hazard_pred: torch.Tensor, survtime: torch.Tensor, censor: torch.Tensor):\n",
    "        censor = censor.float()\n",
    "        n = len(survtime)\n",
    "        R_mat = survtime.reshape((1, n)) >= survtime.reshape((n, 1))\n",
    "        theta = hazard_pred.reshape(-1)\n",
    "        exp_theta = torch.exp(theta)\n",
    "        loss = -torch.mean((theta - torch.log(torch.sum(exp_theta * R_mat, dim=1))) * censor)\n",
    "        return loss\n",
    "\n",
    "\n",
    "cox_loss_fn = CoxLoss()\n",
    "\n",
    "def compute_grad_and_curvature(fused_features, hazard, survtime, censor):\n",
    "    hazard = hazard.view(-1)\n",
    "    loss = cox_loss_fn(hazard, survtime, censor)\n",
    "    grad_f = torch.autograd.grad(loss, fused_features, create_graph=True)[0]\n",
    "    grad_norm = grad_f.flatten(1).norm(p=2, dim=1)\n",
    "\n",
    "    random_vec = grad_f.detach().clone().sign()\n",
    "    grad_dot_random = torch.sum(grad_f * random_vec)\n",
    "    hvp = torch.autograd.grad(grad_dot_random, fused_features, retain_graph=False)[0]\n",
    "    curvature = torch.sum(hvp * random_vec, dim=list(range(1, hvp.ndim)))\n",
    "    \n",
    "    #print(\"grad_f_att:\", grad_f)\n",
    "    #print(\"grad_norm:\", grad_f.flatten(1).norm(p=2, dim=1))\n",
    "\n",
    "    \n",
    "    return grad_norm.detach().cpu().numpy(), curvature.detach().cpu().numpy()   \n",
    "\n",
    "def evaluate_and_log():\n",
    "    folds_dir = \"./models/ckpts/CPTACPDA_trainmixed50_multival_Titan_MedImSight\"\n",
    "    test_path = \"./data/processed/processed_CPTAC_PDA_test/k=all.tsv\"\n",
    "    ct_path = \"../MedImageInsights/embeddings_output_cptacpda_93\"\n",
    "    wsi_path = \"../../TitanCPTACPDA/20x_512px_0px_overlap/slide_features_titan\"\n",
    "    output_dir = \"./interpretability/interpretability_PDA_testmixed50\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for fold in range(1):\n",
    "        print(f\"\\n=== Fold {fold} ===\")\n",
    "        model = MADPENetNoBackbonesSurv(\n",
    "            rad_input_dim=1024, histo_input_dim=768,\n",
    "            inter_dim=256, token_dim=256, dim_hider=256\n",
    "        )\n",
    "        model.eval()\n",
    "\n",
    "        # Load checkpoint\n",
    "        fold_dir = os.path.join(folds_dir, f\"fold_{fold}\")\n",
    "        model_subdir = next(d for d in os.listdir(fold_dir) if os.path.isdir(os.path.join(fold_dir, d)))\n",
    "        model_file = next(f for f in os.listdir(os.path.join(fold_dir, model_subdir)) if f.endswith(\"mixed_missing.pth\"))\n",
    "        checkpoint = torch.load(os.path.join(fold_dir, model_subdir, model_file), map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        dataset = MultimodalCTWSIDatasetSurv(\n",
    "            fold=fold,\n",
    "            split=\"train\",\n",
    "            ct_path=ct_path,\n",
    "            wsi_path=wsi_path,\n",
    "            labels_splits_path=test_path,\n",
    "            missing_modality_prob=0.0,\n",
    "            require_both_modalities=False,\n",
    "            pairing_mode=\"one_to_one\",\n",
    "            allow_repeats=True,\n",
    "            pairs_per_patient=None,\n",
    "            missing_modality=\"wsi\"\n",
    "        )\n",
    "\n",
    "        loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "        # CSV Logger\n",
    "        log_file = os.path.join(output_dir, f\"fold_{fold}_log.csv\")\n",
    "        fold_tensor_dir = os.path.join(output_dir, f\"fold_{fold}\")\n",
    "        os.makedirs(fold_tensor_dir, exist_ok=True)\n",
    "\n",
    "        with open(log_file, \"w\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                \"patient_id\", \"ct_path\", \"wsi_path\",\n",
    "                \"ct_available\", \"wsi_available\",\n",
    "                \"hazard_score\", \"survtime\", \"censor\",\n",
    "                \"grad_norm\", \"curvature\",\n",
    "                \"fused_path\", \"pe_path\", \"hazard_tensor_path\"\n",
    "            ])\n",
    "\n",
    "            for i, batch in enumerate(loader):\n",
    "                for k in batch:\n",
    "                    if isinstance(batch[k], torch.Tensor):\n",
    "                        batch[k] = batch[k].float()\n",
    "\n",
    "                with torch.enable_grad():\n",
    "                    for k in batch:\n",
    "                        if isinstance(batch[k], torch.Tensor):\n",
    "                            batch[k].requires_grad = True\n",
    "\n",
    "                    outputs = model(\n",
    "                        batch[\"ct_feature\"],\n",
    "                        batch[\"wsi_feature\"],\n",
    "                        modality_flag=batch[\"modality_mask\"],\n",
    "                        output_layers=[\"hazard\", \"fused_features\", \"positional_embeddings\"]\n",
    "                    )\n",
    "                    fused_features = outputs[\"fused_features\"]\n",
    "                    fused_features.requires_grad_(True)\n",
    "\n",
    "                    # Recompute hazard *outside* the model to ensure it is explicitly tied to fused_features\n",
    "                    hazard = model.hazard_net(fused_features)\n",
    "\n",
    "                    #fused_features = outputs[\"fused_features\"]\n",
    "                    pos_emb = outputs[\"positional_embeddings\"].squeeze()\n",
    "                    #fused_features.requires_grad_(True)\n",
    "                    grad_norm, curvature = compute_grad_and_curvature(\n",
    "                        fused_features, hazard, batch[\"survtime\"], batch[\"censor\"]\n",
    "                    )\n",
    "\n",
    "                # Save tensor files\n",
    "                sample_id = f\"sample_{i:04d}\"\n",
    "                fused_path = os.path.join(fold_tensor_dir, f\"{sample_id}_fused.npy\")\n",
    "                pe_path = os.path.join(fold_tensor_dir, f\"{sample_id}_pe.npy\")\n",
    "                hazard_path = os.path.join(fold_tensor_dir, f\"{sample_id}_hazard.npy\")\n",
    "\n",
    "                np.save(fused_path, fused_features.detach().cpu().numpy())\n",
    "                np.save(pe_path, pos_emb.detach().cpu().numpy())\n",
    "                np.save(hazard_path, hazard.detach().cpu().numpy())\n",
    "\n",
    "                ct_flag = int(batch[\"modality_mask\"][0, 0].item())\n",
    "                wsi_flag = int(batch[\"modality_mask\"][0, 1].item())\n",
    "\n",
    "                writer.writerow([\n",
    "                    batch[\"patient_id\"][0],\n",
    "                    dataset.samples[i][\"ct_path\"],\n",
    "                    dataset.samples[i][\"wsi_feature\"],\n",
    "                    ct_flag, wsi_flag,\n",
    "                    hazard.item(),\n",
    "                    batch[\"survtime\"].item(),\n",
    "                    batch[\"censor\"].item(),\n",
    "                    grad_norm[0],\n",
    "                    curvature[0],\n",
    "                    fused_path,\n",
    "                    pe_path,\n",
    "                    hazard_path\n",
    "                ])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_and_log()\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "def find_topk_gradient_extremes(\n",
    "    csv_path: str,\n",
    "    k: int = 10,\n",
    "    output_dir: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds top-k highest and lowest |gradient norm| samples from a CSV log.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to fold_X_log.csv generated during eval.\n",
    "        k (int): Number of top and bottom samples to retrieve.\n",
    "        output_dir (str): Optional folder to save CSVs. If None, only prints results.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if \"grad_norm\" not in df.columns:\n",
    "        raise ValueError(\"The CSV does not contain 'grad_norm' column.\")\n",
    "\n",
    "    # Compute absolute gradient norms\n",
    "    df[\"abs_grad_norm\"] = df[\"grad_norm\"].abs()\n",
    "\n",
    "    # Sort by absolute gradient norm\n",
    "    df_sorted = df.sort_values(by=\"abs_grad_norm\", ascending=True)\n",
    "\n",
    "    topk_low = df_sorted.head(k).copy()\n",
    "    topk_high = df_sorted.tail(k).copy()[::-1]  # High to low\n",
    "\n",
    "    print(\"\\n🔹 Top-k LOW |Gradient Norm| Samples:\")\n",
    "    print(topk_low[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    print(\"\\n🔸 Top-k HIGH |Gradient Norm| Samples:\")\n",
    "    print(topk_high[[\"patient_id\", \"grad_norm\", \"abs_grad_norm\", \"ct_available\", \"wsi_available\"]])\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        low_path = os.path.join(output_dir, \"topk_low_gradient.csv\")\n",
    "        high_path = os.path.join(output_dir, \"topk_high_gradient.csv\")\n",
    "        topk_low.to_csv(low_path, index=False)\n",
    "        topk_high.to_csv(high_path, index=False)\n",
    "        print(f\"\\n📁 Saved to:\\n  {low_path}\\n  {high_path}\")\n",
    "\n",
    "    return topk_low, topk_high\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    for i in range(1):\n",
    "        csv_input = f\"./interpretability/interpretability_PDA_testmixed50/fold_{i}_log.csv\"\n",
    "        find_topk_gradient_extremes(\n",
    "            csv_path=csv_input,\n",
    "            k=15,\n",
    "            output_dir=f\"./interpretability/interpretability_PDA_testmixed50/fold_{i}_topk\"\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_biocv_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
