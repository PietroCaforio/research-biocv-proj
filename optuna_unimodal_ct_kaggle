{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9516694,"sourceType":"datasetVersion","datasetId":5758221},{"sourceId":9547833,"sourceType":"datasetVersion","datasetId":5664308}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":17965.45818,"end_time":"2024-10-02T20:17:42.170002","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-02T15:18:16.711822","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"047333d144e24bf8b319e49a1f6e9cdf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_513d3d81ce9e41a4a325aa7e02b5c124","placeholder":"​","style":"IPY_MODEL_8458072955dc4dd7ba706733845b2ef8","value":" 69.5k/69.5k [00:00&lt;00:00, 5.09MB/s]"}},"0b361808f6884662b933b542fc34feb7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"108059115399479dad375dc16e62def8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b361808f6884662b933b542fc34feb7","max":87287364,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c47149da128e46de9bcb9e7bd88153bd","value":87287364}},"27846f24692b4133a13fb18f8a3f75b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be17eb4383b24bd8b66ada9ca7d2a135","placeholder":"​","style":"IPY_MODEL_2f4980d8c3d04312ab308b8fe4115c13","value":"model.safetensors: 100%"}},"27e7b5c91ca646f3bd1210031e04770c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2edff0ca6fe049609701cf391489f7eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f4980d8c3d04312ab308b8fe4115c13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44147eb3885d4e21ae35e1b50ccf0e18":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4543b3c26029434ba3e6e24300789050":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec4459ef641d4d2ca65b3c5f5e671c2f","placeholder":"​","style":"IPY_MODEL_e70455dd1ccd4cfabfb02af6152c67d5","value":"config.json: 100%"}},"513d3d81ce9e41a4a325aa7e02b5c124":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58ce7cc177a54d6386dd16fbc8889500":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62e7a84cafa6467482dadec05f65d895":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8458072955dc4dd7ba706733845b2ef8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84deff5e5c6e4188a6656bc8570d626c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_58ce7cc177a54d6386dd16fbc8889500","max":69548,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d2eca2b80d0443c584777b4a43c768bd","value":69548}},"be17eb4383b24bd8b66ada9ca7d2a135":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c47149da128e46de9bcb9e7bd88153bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2eca2b80d0443c584777b4a43c768bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"df8f0bb1722c457596555fb9f0d9bdd7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4543b3c26029434ba3e6e24300789050","IPY_MODEL_84deff5e5c6e4188a6656bc8570d626c","IPY_MODEL_047333d144e24bf8b319e49a1f6e9cdf"],"layout":"IPY_MODEL_2edff0ca6fe049609701cf391489f7eb"}},"df949194982a432e98545ab6b92e8d3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27e7b5c91ca646f3bd1210031e04770c","placeholder":"​","style":"IPY_MODEL_62e7a84cafa6467482dadec05f65d895","value":" 87.3M/87.3M [00:02&lt;00:00, 63.0MB/s]"}},"e70455dd1ccd4cfabfb02af6152c67d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec4459ef641d4d2ca65b3c5f5e671c2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f449314493af40d493b59a6cf1f558ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27846f24692b4133a13fb18f8a3f75b2","IPY_MODEL_108059115399479dad375dc16e62def8","IPY_MODEL_df949194982a432e98545ab6b92e8d3a"],"layout":"IPY_MODEL_44147eb3885d4e21ae35e1b50ccf0e18"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/pietrocaforio/unimodal-ct-training-kaggle?scriptVersionId=199214851\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"markdown","source":"# Train unimodal CT","metadata":{"papermill":{"duration":0.009881,"end_time":"2024-10-02T15:18:19.408617","exception":false,"start_time":"2024-10-02T15:18:19.398736","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!git clone https://github.com/PietroCaforio/research-biocv-proj\n!cd research-biocv-proj && git switch dev","metadata":{"papermill":{"duration":2.933532,"end_time":"2024-10-02T15:18:22.351626","exception":false,"start_time":"2024-10-02T15:18:19.418094","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:41:06.193273Z","iopub.execute_input":"2024-10-05T15:41:06.193607Z","iopub.status.idle":"2024-10-05T15:41:09.936689Z","shell.execute_reply.started":"2024-10-05T15:41:06.193569Z","shell.execute_reply":"2024-10-05T15:41:09.935652Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'research-biocv-proj'...\nremote: Enumerating objects: 440, done.\u001b[K\nremote: Counting objects: 100% (232/232), done.\u001b[K\nremote: Compressing objects: 100% (164/164), done.\u001b[K\nremote: Total 440 (delta 128), reused 137 (delta 46), pack-reused 208 (from 1)\u001b[K\nReceiving objects: 100% (440/440), 5.26 MiB | 21.98 MiB/s, done.\nResolving deltas: 100% (250/250), done.\nBranch 'dev' set up to track remote branch 'dev' from 'origin'.\nSwitched to a new branch 'dev'\n","output_type":"stream"}]},{"cell_type":"code","source":"!cd research-biocv-proj && git pull","metadata":{"papermill":{"duration":1.299069,"end_time":"2024-10-02T15:18:23.659966","exception":false,"start_time":"2024-10-02T15:18:22.360897","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:41:09.938872Z","iopub.execute_input":"2024-10-05T15:41:09.939183Z","iopub.status.idle":"2024-10-05T15:41:11.453548Z","shell.execute_reply.started":"2024-10-05T15:41:09.939148Z","shell.execute_reply":"2024-10-05T15:41:11.452430Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Already up to date.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-10-07T16:43:33.988771Z","iopub.execute_input":"2024-10-07T16:43:33.989050Z","iopub.status.idle":"2024-10-07T16:43:34.996391Z","shell.execute_reply.started":"2024-10-07T16:43:33.989018Z","shell.execute_reply":"2024-10-07T16:43:34.995221Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Architecture:             x86_64\n  CPU op-mode(s):         32-bit, 64-bit\n  Address sizes:          46 bits physical, 48 bits virtual\n  Byte Order:             Little Endian\nCPU(s):                   4\n  On-line CPU(s) list:    0-3\nVendor ID:                GenuineIntel\n  Model name:             Intel(R) Xeon(R) CPU @ 2.00GHz\n    CPU family:           6\n    Model:                85\n    Thread(s) per core:   2\n    Core(s) per socket:   2\n    Socket(s):            1\n    Stepping:             3\n    BogoMIPS:             4000.27\n    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge m\n                          ca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht sysc\n                          all nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xt\n                          opology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq\n                           ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt\n                           aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dno\n                          wprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgs\n                          base tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid r\n                          tm mpx avx512f avx512dq rdseed adx smap clflushopt clw\n                          b avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 x\n                          saves arat md_clear arch_capabilities\nVirtualization features:  \n  Hypervisor vendor:      KVM\n  Virtualization type:    full\nCaches (sum of all):      \n  L1d:                    64 KiB (2 instances)\n  L1i:                    64 KiB (2 instances)\n  L2:                     2 MiB (2 instances)\n  L3:                     38.5 MiB (1 instance)\nNUMA:                     \n  NUMA node(s):           1\n  NUMA node0 CPU(s):      0-3\nVulnerabilities:          \n  Gather data sampling:   Not affected\n  Itlb multihit:          Not affected\n  L1tf:                   Mitigation; PTE Inversion\n  Mds:                    Mitigation; Clear CPU buffers; SMT Host state unknown\n  Meltdown:               Mitigation; PTI\n  Mmio stale data:        Vulnerable: Clear CPU buffers attempted, no microcode;\n                           SMT Host state unknown\n  Reg file data sampling: Not affected\n  Retbleed:               Mitigation; IBRS\n  Spec rstack overflow:   Not affected\n  Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prct\n                          l and seccomp\n  Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointe\n                          r sanitization\n  Spectre v2:             Mitigation; IBRS; IBPB conditional; STIBP conditional;\n                           RSB filling; PBRSB-eIBRS Not affected; BHI SW loop, K\n                          VM SW loop\n  Srbds:                  Not affected\n  Tsx async abort:        Mitigation; Clear CPU buffers; SMT Host state unknown\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb","metadata":{"papermill":{"duration":13.863737,"end_time":"2024-10-02T15:18:37.532831","exception":false,"start_time":"2024-10-02T15:18:23.669094","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:41:11.455027Z","iopub.execute_input":"2024-10-05T15:41:11.455414Z","iopub.status.idle":"2024-10-05T15:41:24.866563Z","shell.execute_reply.started":"2024-10-05T15:41:11.455374Z","shell.execute_reply":"2024-10-05T15:41:24.865230Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.3)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.15.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb_api_key\")","metadata":{"papermill":{"duration":0.192061,"end_time":"2024-10-02T15:18:37.734334","exception":false,"start_time":"2024-10-02T15:18:37.542273","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:41:24.869588Z","iopub.execute_input":"2024-10-05T15:41:24.869997Z","iopub.status.idle":"2024-10-05T15:41:25.256250Z","shell.execute_reply.started":"2024-10-05T15:41:24.869951Z","shell.execute_reply":"2024-10-05T15:41:25.255393Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login(key=secret_value_0)","metadata":{"papermill":{"duration":2.112979,"end_time":"2024-10-02T15:18:39.856514","exception":false,"start_time":"2024-10-02T15:18:37.743535","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:41:25.257608Z","iopub.execute_input":"2024-10-05T15:41:25.257876Z","iopub.status.idle":"2024-10-05T15:41:28.807320Z","shell.execute_reply.started":"2024-10-05T15:41:25.257847Z","shell.execute_reply":"2024-10-05T15:41:28.806252Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import sys\nfrom pathlib import Path\n\n# Add the 'data' directory to sys.path\nsys.path.append(str(Path('research-biocv-proj').resolve()))\nfrom data.unimodal import *\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom sklearn.utils.class_weight import compute_class_weight","metadata":{"papermill":{"duration":4.405419,"end_time":"2024-10-02T15:18:44.271628","exception":false,"start_time":"2024-10-02T15:18:39.866209","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:41:28.808712Z","iopub.execute_input":"2024-10-05T15:41:28.809269Z","iopub.status.idle":"2024-10-05T15:41:34.244014Z","shell.execute_reply.started":"2024-10-05T15:41:28.809225Z","shell.execute_reply":"2024-10-05T15:41:34.242863Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Train ResNet model","metadata":{"papermill":{"duration":0.0093,"end_time":"2024-10-02T15:18:44.290983","exception":false,"start_time":"2024-10-02T15:18:44.281683","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#https://github.com/mathiaszinnen/focal_loss_torch/tree/main\n!pip install focal_loss_torch","metadata":{"papermill":{"duration":12.924966,"end_time":"2024-10-02T15:18:57.225527","exception":false,"start_time":"2024-10-02T15:18:44.300561","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:41:34.245484Z","iopub.execute_input":"2024-10-05T15:41:34.246154Z","iopub.status.idle":"2024-10-05T15:41:46.474382Z","shell.execute_reply.started":"2024-10-05T15:41:34.246093Z","shell.execute_reply":"2024-10-05T15:41:46.473186Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting focal_loss_torch\n  Downloading focal_loss_torch-0.1.2-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from focal_loss_torch) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from focal_loss_torch) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->focal_loss_torch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->focal_loss_torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->focal_loss_torch) (1.3.0)\nDownloading focal_loss_torch-0.1.2-py3-none-any.whl (4.5 kB)\nInstalling collected packages: focal_loss_torch\nSuccessfully installed focal_loss_torch-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"class EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print            \n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"execution":{"iopub.status.busy":"2024-10-05T15:41:46.476013Z","iopub.execute_input":"2024-10-05T15:41:46.476376Z","iopub.status.idle":"2024-10-05T15:41:46.488663Z","shell.execute_reply.started":"2024-10-05T15:41:46.476328Z","shell.execute_reply":"2024-10-05T15:41:46.487807Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from focal_loss.focal_loss import FocalLoss\n\ndef train(model, train_loader,val_loader, config, run_name=None):\n    \n    #wandb.init(\n        # set the wandb project where this run will be logged\n     #   project=\"unimodal_ct_training\",\n     #   name = run_name,\n        # track hyperparameters and run metadata\n      #  config=config\n    #)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if config[\"class_weights\"] is not None: \n        config[\"class_weights\"] = torch.tensor(config[\"class_weights\"], dtype=torch.float).to(device) \n    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n    if config[\"focal_loss\"] is not None:\n        \n        criterion = FocalLoss(gamma = config[\"focal_loss\"])\n    else:\n        criterion = nn.CrossEntropyLoss(weight = config[\"class_weights\"])\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = config[\"reduce_lr_factor\"], patience = config[\"patience\"])\n    \n    # initialize the early_stopping object\n    early_stopping = EarlyStopping(patience=config[\"early_stop_patience\"], verbose=True)\n    \n    # Training loop\n    num_epochs = config[\"epochs\"]\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        train_correct = 0\n        total = 0\n        correct_per_class = [0, 0, 0]  # For G1, G2, G3\n        total_per_class = [0, 0, 0]  # For G1, G2, G3\n        for batch in train_loader:\n            frames = batch['frame'].float().to(device)\n            labels = batch['label'].long().to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(frames)\n            if config[\"focal_loss\"]:\n                softmax = torch.nn.Softmax(dim=-1)\n                loss = criterion(softmax(outputs.logits), labels)\n            else:\n                loss = criterion(outputs.logits, labels)\n\n            \n            _, predicted = torch.max(outputs.logits, 1)\n            train_correct += (predicted == labels).sum().item()\n            loss.backward()\n            optimizer.step()\n            total += labels.size(0)\n            running_loss += loss.item()\n            \n            # Calculate accuracy per class\n            for i in range(3):  # We have 3 classes: G1 (0), G2 (1), G3 (2)\n                correct_per_class[i] += ((predicted == i) & (labels == i)).sum().item()\n                total_per_class[i] += (labels == i).sum().item()\n\n        train_accuracy = 100 * train_correct / total\n        class_accuracy = [(100 * correct_per_class[i] / total_per_class[i]) if total_per_class[i] > 0 else 0 for i in range(3)]\n        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n        #wandb.log({\"Train Accuracy\": train_accuracy, \"Train loss\": running_loss/len(train_loader), \"G1_TrainAcc\":class_accuracy[0], \"G2_TrainAcc\":class_accuracy[1], \"G3_TrainAcc\":class_accuracy[2]})\n\n        # Validation loop\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        # Initialize counters for each class (G1, G2, G3)\n        correct_per_class = [0, 0, 0]  # For G1, G2, G3\n        total_per_class = [0, 0, 0]  # For G1, G2, G3\n        val_labels = []\n        val_predictions = []\n\n\n        with torch.no_grad():\n            for batch in val_loader:\n                frames = batch['frame'].float().to(device)\n                labels = batch['label'].long().to(device)\n\n                outputs = model(frames)\n                \n                if config[\"focal_loss\"]:\n                    softmax = torch.nn.Softmax(dim=-1)\n                    loss = criterion(softmax(outputs.logits), labels)\n                else:\n                    loss = criterion(outputs.logits, labels)\n\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs.logits, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n                # Calculate accuracy per class\n                for i in range(3):  # We have 3 classes: G1 (0), G2 (1), G3 (2)\n                    correct_per_class[i] += ((predicted == i) & (labels == i)).sum().item()\n                    total_per_class[i] += (labels == i).sum().item()\n                \n                val_labels.extend(labels.cpu().tolist())\n                val_predictions.extend(predicted.cpu().tolist())\n        \n        scheduler.step(val_loss)\n        \n        # Compute total accuracy and per-class accuracy\n       \n        total_accuracy = 100 * correct / total\n        class_accuracy = [(100 * correct_per_class[i] / total_per_class[i]) if total_per_class[i] > 0 else 0 for i in range(3)]\n        print(f\"Validation Loss: {val_loss/len(val_loader)}, Total Accuracy: {total_accuracy:.2f}%\")\n        print(f\"Accuracy per class - G1: {class_accuracy[0]:.2f}%, G2: {class_accuracy[1]:.2f}%, G3: {class_accuracy[2]:.2f}%\")\n        # log metrics to wandb\n        #wandb.log({\n        #    \"Total Accuracy\": total_accuracy, \n        #    \"Validation Loss\": val_loss/len(val_loader), \n        #    \"G1_Acc\":class_accuracy[0], \n        #    \"G2_Acc\":class_accuracy[1], \n        #    \"G3_Acc\":class_accuracy[2]\n        #})\n        \n        \n        early_stopping(val_loss, model)\n        \n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n    #wandb.finish()\n    return ","metadata":{"papermill":{"duration":0.042486,"end_time":"2024-10-02T15:18:57.278475","exception":false,"start_time":"2024-10-02T15:18:57.235989","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:41:46.490324Z","iopub.execute_input":"2024-10-05T15:41:46.490673Z","iopub.status.idle":"2024-10-05T15:41:46.520904Z","shell.execute_reply.started":"2024-10-05T15:41:46.490629Z","shell.execute_reply":"2024-10-05T15:41:46.520045Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nfrom transformers import ResNetForImageClassification\nfrom sklearn.model_selection import StratifiedGroupKFold #For crossvalidation","metadata":{"papermill":{"duration":1.881492,"end_time":"2024-10-02T15:18:59.170019","exception":false,"start_time":"2024-10-02T15:18:57.288527","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:41:46.524920Z","iopub.execute_input":"2024-10-05T15:41:46.525243Z","iopub.status.idle":"2024-10-05T15:41:49.697575Z","shell.execute_reply.started":"2024-10-05T15:41:46.525211Z","shell.execute_reply":"2024-10-05T15:41:49.696720Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_dataset = UnimodalCTDataset(split='all',dataset_path = \"/kaggle/input/preprocessed57patientscptacpda/processed/\" )","metadata":{"papermill":{"duration":10.372006,"end_time":"2024-10-02T15:19:09.552995","exception":false,"start_time":"2024-10-02T15:18:59.180989","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:41:49.698795Z","iopub.execute_input":"2024-10-05T15:41:49.699416Z","iopub.status.idle":"2024-10-05T15:42:06.337417Z","shell.execute_reply.started":"2024-10-05T15:41:49.699368Z","shell.execute_reply":"2024-10-05T15:42:06.336612Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#print(f\"Training set stats:{train_dataset.stats()}\")\n#print(f\"Validation set stats:{val_dataset.stats()}\")","metadata":{"papermill":{"duration":0.018465,"end_time":"2024-10-02T15:19:09.581885","exception":false,"start_time":"2024-10-02T15:19:09.56342","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:42:06.338782Z","iopub.execute_input":"2024-10-05T15:42:06.339559Z","iopub.status.idle":"2024-10-05T15:42:06.343196Z","shell.execute_reply.started":"2024-10-05T15:42:06.339512Z","shell.execute_reply":"2024-10-05T15:42:06.342381Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nlabels = []\nfor sample in train_dataset:\n    labels.append(sample[\"label\"])\nlabels = np.array(labels)\nclass_weights = compute_class_weight(\"balanced\", classes=np.unique(labels), y=labels)\n\"\"\"","metadata":{"papermill":{"duration":7.62732,"end_time":"2024-10-02T15:19:17.219578","exception":false,"start_time":"2024-10-02T15:19:09.592258","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:42:06.344511Z","iopub.execute_input":"2024-10-05T15:42:06.345109Z","iopub.status.idle":"2024-10-05T15:42:06.354848Z","shell.execute_reply.started":"2024-10-05T15:42:06.345066Z","shell.execute_reply":"2024-10-05T15:42:06.353966Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'\\nlabels = []\\nfor sample in train_dataset:\\n    labels.append(sample[\"label\"])\\nlabels = np.array(labels)\\nclass_weights = compute_class_weight(\"balanced\", classes=np.unique(labels), y=labels)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"#print(class_weights)","metadata":{"papermill":{"duration":0.019437,"end_time":"2024-10-02T15:19:17.249529","exception":false,"start_time":"2024-10-02T15:19:17.230092","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:42:06.355749Z","iopub.execute_input":"2024-10-05T15:42:06.356056Z","iopub.status.idle":"2024-10-05T15:42:06.360726Z","shell.execute_reply.started":"2024-10-05T15:42:06.356011Z","shell.execute_reply":"2024-10-05T15:42:06.359863Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#train_dataset = UnimodalCTDataset(split='train',dataset_path = \"/kaggle/input/oversampling57patientscptacpda/processed_oversampling/\" )\n#val_dataset = UnimodalCTDataset(split='val',dataset_path = \"/kaggle/input/oversampling57patientscptacpda/processed_oversampling/\")\n\n#train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n#val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)","metadata":{"papermill":{"duration":0.016646,"end_time":"2024-10-02T15:19:17.276853","exception":false,"start_time":"2024-10-02T15:19:17.260207","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:42:06.361955Z","iopub.execute_input":"2024-10-05T15:42:06.362230Z","iopub.status.idle":"2024-10-05T15:42:06.369582Z","shell.execute_reply.started":"2024-10-05T15:42:06.362199Z","shell.execute_reply":"2024-10-05T15:42:06.368800Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#print(f\"Training set stats:{train_dataset.stats()}\")\n#print(f\"Validation set stats:{val_dataset.stats()}\")","metadata":{"papermill":{"duration":0.017716,"end_time":"2024-10-02T15:19:17.304683","exception":false,"start_time":"2024-10-02T15:19:17.286967","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:42:06.370748Z","iopub.execute_input":"2024-10-05T15:42:06.371517Z","iopub.status.idle":"2024-10-05T15:42:06.377815Z","shell.execute_reply.started":"2024-10-05T15:42:06.371471Z","shell.execute_reply":"2024-10-05T15:42:06.376937Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#total = train_dataset.stats()[\"length\"]\n#most_frequent = max(train_dataset.stats()[\"class_frequency\"].values())\n#freq_dict = train_dataset.stats()[\"class_frequency\"]\n#target_volume_depth= {}\n#for index in freq_dict.keys():\n#    target_volume_depth[index] = int((total/3 ) * most_frequent / freq_dict[index])\n#print(target_volume_depth)","metadata":{"papermill":{"duration":0.016864,"end_time":"2024-10-02T15:19:17.331677","exception":false,"start_time":"2024-10-02T15:19:17.314813","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:42:06.378904Z","iopub.execute_input":"2024-10-05T15:42:06.379191Z","iopub.status.idle":"2024-10-05T15:42:06.386735Z","shell.execute_reply.started":"2024-10-05T15:42:06.379161Z","shell.execute_reply":"2024-10-05T15:42:06.385912Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Resnet-50","metadata":{"papermill":{"duration":0.009875,"end_time":"2024-10-02T15:19:17.351964","exception":false,"start_time":"2024-10-02T15:19:17.342089","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#model = ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\n#model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, UnimodalCTDataset.num_classes) #Adjusting the final layer to the unimodal number of classes","metadata":{"papermill":{"duration":0.016715,"end_time":"2024-10-02T15:19:17.378929","exception":false,"start_time":"2024-10-02T15:19:17.362214","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:42:06.387756Z","iopub.execute_input":"2024-10-05T15:42:06.388027Z","iopub.status.idle":"2024-10-05T15:42:06.394105Z","shell.execute_reply.started":"2024-10-05T15:42:06.387996Z","shell.execute_reply":"2024-10-05T15:42:06.393378Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#model.to(device)","metadata":{"papermill":{"duration":0.016813,"end_time":"2024-10-02T15:19:17.405696","exception":false,"start_time":"2024-10-02T15:19:17.388883","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:42:06.395138Z","iopub.execute_input":"2024-10-05T15:42:06.395490Z","iopub.status.idle":"2024-10-05T15:42:06.405067Z","shell.execute_reply.started":"2024-10-05T15:42:06.395440Z","shell.execute_reply":"2024-10-05T15:42:06.404204Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nconfig={\n    \"learning_rate\": 1e-4,\n    \"architecture\": \"microsoft/resnet-50 new1\",\n    \"epochs\": 100,\n    \"weight_decay\": 1e-4,\n    \"reduce_lr_factor\": 0.2,\n    \"patience\": 10,\n    \"class_weights\": class_weights\n    }\ntrain(model, config, run_name = config[\"architecture\"])\n\"\"\"","metadata":{"papermill":{"duration":0.018966,"end_time":"2024-10-02T15:19:17.43459","exception":false,"start_time":"2024-10-02T15:19:17.415624","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:42:06.406093Z","iopub.execute_input":"2024-10-05T15:42:06.406379Z","iopub.status.idle":"2024-10-05T15:42:06.416735Z","shell.execute_reply.started":"2024-10-05T15:42:06.406348Z","shell.execute_reply":"2024-10-05T15:42:06.415868Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'\\nconfig={\\n    \"learning_rate\": 1e-4,\\n    \"architecture\": \"microsoft/resnet-50 new1\",\\n    \"epochs\": 100,\\n    \"weight_decay\": 1e-4,\\n    \"reduce_lr_factor\": 0.2,\\n    \"patience\": 10,\\n    \"class_weights\": class_weights\\n    }\\ntrain(model, config, run_name = config[\"architecture\"])\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Resnet-18","metadata":{"papermill":{"duration":0.010326,"end_time":"2024-10-02T15:19:17.455915","exception":false,"start_time":"2024-10-02T15:19:17.445589","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\"\"\"\nconfig={\n    \"learning_rate\": 1e-7,\n    \"architecture\": \"microsoft/resnet-18\",\n    \"run_name\": \"microsoft/resnet-18 NOOVERSAMPLING FOCALLOSS\",\n    \"epochs\": 800,\n    \"weight_decay\": 1e-6,\n    \"reduce_lr_factor\": 0.25,\n    \"patience\": 20,\n    \"early_stop_patience\": 40,\n    \"class_weights\": None,\n    \"focal_loss\": 2\n    }\n\n\"\"\"\n\n\n","metadata":{"papermill":{"duration":0.019094,"end_time":"2024-10-02T15:19:17.541025","exception":false,"start_time":"2024-10-02T15:19:17.521931","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:42:06.417739Z","iopub.execute_input":"2024-10-05T15:42:06.418014Z","iopub.status.idle":"2024-10-05T15:42:06.429175Z","shell.execute_reply.started":"2024-10-05T15:42:06.417985Z","shell.execute_reply":"2024-10-05T15:42:06.428283Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'\\nconfig={\\n    \"learning_rate\": 1e-7,\\n    \"architecture\": \"microsoft/resnet-18\",\\n    \"run_name\": \"microsoft/resnet-18 NOOVERSAMPLING FOCALLOSS\",\\n    \"epochs\": 800,\\n    \"weight_decay\": 1e-6,\\n    \"reduce_lr_factor\": 0.25,\\n    \"patience\": 20,\\n    \"early_stop_patience\": 40,\\n    \"class_weights\": None,\\n    \"focal_loss\": 2\\n    }\\n\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Resnet-34","metadata":{"papermill":{"duration":0.010245,"end_time":"2024-10-02T15:19:17.561644","exception":false,"start_time":"2024-10-02T15:19:17.551399","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#model = ResNetForImageClassification.from_pretrained('microsoft/resnet-34')\n#model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, UnimodalCTDataset.num_classes) #Adjusting the final layer to the unimodal number of classes","metadata":{"papermill":{"duration":2.836385,"end_time":"2024-10-02T15:19:20.408504","exception":false,"start_time":"2024-10-02T15:19:17.572119","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:42:06.430252Z","iopub.execute_input":"2024-10-05T15:42:06.430598Z","iopub.status.idle":"2024-10-05T15:42:06.437421Z","shell.execute_reply.started":"2024-10-05T15:42:06.430563Z","shell.execute_reply":"2024-10-05T15:42:06.436622Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#model.to(device)","metadata":{"papermill":{"duration":0.266223,"end_time":"2024-10-02T15:19:20.685917","exception":false,"start_time":"2024-10-02T15:19:20.419694","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-05T15:42:06.438297Z","iopub.execute_input":"2024-10-05T15:42:06.438581Z","iopub.status.idle":"2024-10-05T15:42:06.445453Z","shell.execute_reply.started":"2024-10-05T15:42:06.438552Z","shell.execute_reply":"2024-10-05T15:42:06.444677Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nconfig={\n    \"learning_rate\": 5e-8,\n    \"architecture\": \"microsoft/resnet-34\",\n    \"run_name\": \"microsoft/resnet-34 NOOVERSAMPLING FOCALLOSS fold: \",\n    \"epochs\": 2,\n    \"weight_decay\": 1e-6,\n    \"reduce_lr_factor\": 0.25,\n    \"patience\": 20,\n    \"early_stop_patience\": 40,\n    \"class_weights\": None,\n    \"focal_loss\": 5\n    }\n\n\"\"\"\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T15:48:29.471634Z","iopub.execute_input":"2024-10-05T15:48:29.472553Z","iopub.status.idle":"2024-10-05T15:48:29.477562Z","shell.execute_reply.started":"2024-10-05T15:48:29.472512Z","shell.execute_reply":"2024-10-05T15:48:29.476542Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfrom torch.utils.data import DataLoader, Subset\nfrom collections import Counter \n\nk_folds = 3\nbatch_size = 32\n# Initialize the k-fold cross validation\n#group fold in order to group indices by patient_id so that we don't introduct frames of the same patient in the train set and test set\ngkf = StratifiedGroupKFold(n_splits=k_folds) \npatient_ids = [info.split(\"/\")[0].split(\"_\")[0] for info in train_dataset.items]\nlabels = [train_dataset.labels[patient_id] for patient_id in patient_ids]\nindices = list(range(len(patient_ids)))\n# Loop through each fold\nfor fold, (train_idx, test_idx) in enumerate(gkf.split(indices, labels, groups=patient_ids)):\n    print(f\"Fold {fold + 1}\")\n    print(\"-------\")\n    # Define the data loaders for the current fold\n    train_subset = Subset(train_dataset, train_idx)\n    val_subset = Subset(train_dataset, test_idx)\n    train_loader = DataLoader(\n        dataset=train_subset,\n        batch_size=batch_size,\n        shuffle = True\n    )\n    val_loader = DataLoader(\n        dataset=val_subset,\n        batch_size=batch_size,\n        shuffle = False,\n    )\n    train_ids = []\n    train_labels = []\n    for frame in train_loader:\n        train_ids.extend(frame[\"patient_id\"])\n        train_labels.extend(frame[\"label\"].tolist())\n    print(f\"TRAIN LABELS IN FOLD {fold + 1 }: {Counter(train_labels)}\")\n    val_ids = []\n    val_labels = []\n    for frame in val_loader:\n        val_ids.extend(frame[\"patient_id\"])\n        val_labels.extend(frame[\"label\"].tolist())\n    print(f\"VAL LABELS IN FOLD {fold + 1}: {Counter(val_labels)}\")\n    print(len(set(train_idx) - set(val_ids)))\n    print(len(set(train_ids)))\n    assert len(set(train_ids) - set(val_ids)) == len(set(train_ids))\n    run_name = list(config[\"run_name\"])\n    run_name[-1] = fold + 1\n    config[\"run_name\"] = run_name\n    #Prepare model\n    model = ResNetForImageClassification.from_pretrained(config[\"architecture\"])\n    model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, UnimodalCTDataset.num_classes) #Adjusting the final layer to the unimodal number of classes\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    #Train model\n    train(model, config, run_name = config[\"run_name\"])\n\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T15:48:55.959540Z","iopub.execute_input":"2024-10-05T15:48:55.959951Z","iopub.status.idle":"2024-10-05T15:49:49.430189Z","shell.execute_reply.started":"2024-10-05T15:48:55.959914Z","shell.execute_reply":"2024-10-05T15:49:49.429182Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Fold 1\n-------\nTRAIN LABELS IN FOLD 1: Counter({1: 1383, 2: 625, 0: 234})\nVAL LABELS IN FOLD 1: Counter({1: 693, 2: 306, 0: 53})\n2242\n38\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8508bb04ef0f416db81dd0fe2234fcae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaed2eac06a447baa700de30c3d61880"}},"metadata":{}},{"name":"stdout","text":"Fold 2\n-------\nTRAIN LABELS IN FOLD 2: Counter({1: 1384, 2: 617, 0: 213})\nVAL LABELS IN FOLD 2: Counter({1: 692, 2: 314, 0: 74})\n2214\n35\nFold 3\n-------\nTRAIN LABELS IN FOLD 3: Counter({1: 1385, 2: 620, 0: 127})\nVAL LABELS IN FOLD 3: Counter({1: 691, 2: 311, 0: 160})\n2132\n37\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset.stats()","metadata":{"execution":{"iopub.status.busy":"2024-10-05T15:49:57.779985Z","iopub.execute_input":"2024-10-05T15:49:57.780428Z","iopub.status.idle":"2024-10-05T15:49:57.786837Z","shell.execute_reply.started":"2024-10-05T15:49:57.780389Z","shell.execute_reply":"2024-10-05T15:49:57.785929Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'length': 3294, 'class_frequency': {'G1': 287, 'G2': 2076, 'G3': 931}}"},"metadata":{}}]},{"cell_type":"code","source":"!pip install optuna-integration","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom optuna.trial import TrialState\nfrom torch.utils.data import DataLoader, Subset\nfrom collections import Counter \nfrom sklearn.metrics import f1_score\nfrom optuna.integration.wandb import WeightsAndBiasesCallback\n\n# Define the objective function for Optuna\ndef objective(trial):\n    # Suggest hyperparameters to be tuned by Optuna\n    config = {\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-9, 1e-6),\n        \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-3),\n        \"reduce_lr_factor\": trial.suggest_float(\"reduce_lr_factor\", 0.1, 0.5),\n        \"patience\": trial.suggest_int(\"patience\", 5, 30),\n        \"early_stop_patience\": trial.suggest_int(\"early_stop_patience\", 10, 50),\n        \"focal_loss\": trial.suggest_float(\"focal_loss\", 0.0, 5.0),  # Focal loss gamma can vary from 0 (CrossEntropy) to 5\n        \"epochs\": 2,  # Fixing the number of epochs to avoid very long runs\n        \"class_weights\": None,\n        \"run_name\": f\"optuna_run_{trial.number} microsoft/resnet-18 NOOVERSAMPLING FOCALLOSS fold: \",\n        \"architecture\": \"microsoft/resnet-18\"\n    }\n\n    # Use k-fold cross-validation\n    k_folds = 3\n    batch_size = 32\n\n    # Initialize the k-fold cross-validation\n    gkf = StratifiedGroupKFold(n_splits=k_folds)\n    patient_ids = [info.split(\"/\")[0].split(\"_\")[0] for info in train_dataset.items]\n    labels = [train_dataset.labels[patient_id] for patient_id in patient_ids]\n    indices = list(range(len(patient_ids)))\n    \n    val_scores = []\n\n    for fold, (train_idx, test_idx) in enumerate(gkf.split(indices, labels, groups=patient_ids)):\n        print(f\"Fold {fold + 1}\")\n        train_subset = Subset(train_dataset, train_idx)\n        val_subset = Subset(train_dataset, test_idx)\n        train_loader = DataLoader(dataset=train_subset, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(dataset=val_subset, batch_size=batch_size, shuffle=False)\n\n        # Prepare the model for each fold\n        model = ResNetForImageClassification.from_pretrained(config[\"architecture\"])\n        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, UnimodalCTDataset.num_classes)\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        model.to(device)\n        \n        run_name = list(config[\"run_name\"])\n        run_name[-1] = str(fold + 1)\n        config[\"run_name\"] = ''.join(run_name)\n\n        # Train the model for the current fold\n        train(model, train_loader, val_loader, config, run_name=config[\"run_name\"])\n\n        # After training, evaluate on the validation set\n        val_loss = evaluate(model, val_loader, device)\n        val_scores.append(val_loss)\n\n    # Return the mean validation loss or accuracy across all folds for Optuna to optimize\n    return sum(val_scores) / len(val_scores)\n\n# Define the evaluation function\ndef evaluate(model, val_loader, device):\n    model.eval()\n    val_labels = []\n    val_predictions = []\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            frames = batch['frame'].float().to(device)\n            labels = batch['label'].long().to(device)\n            outputs = model(frames)\n            \n            _, predicted = torch.max(outputs.logits, 1)\n            val_labels.extend(labels.cpu().tolist())\n            val_predictions.extend(predicted.cpu().tolist())\n\n    # Calculate macro-averaged F1-score across all classes\n    f1 = f1_score(val_labels, val_predictions, average='macro')\n    return f1\n\nwandb_kwargs = {\"project\": \"my-project\"}\nwandbc = WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs, as_multirun=True)\n\n# Create an Optuna study and optimize\nstudy = optuna.create_study(direction=\"maximize\")  # Maximizing f1-score\nstudy.optimize(objective, n_trials=20, n_jobs=4, callbacks=[wandbc])  \n\n# Print the best hyperparameters found by Optuna\nprint(\"Best hyperparameters: \", study.best_params)\n","metadata":{},"execution_count":null,"outputs":[]}]}